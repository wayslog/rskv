{"files":[{"path":["/","Users","xuesong.zhao","repo","rust","rskv","benches","performance.rs"],"content":"//! æ€§èƒ½åŸºå‡†æµ‹è¯•\n//!\n//! æµ‹è¯• rskv åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼š\n//! - ä¸åŒ value size (1B to 100KB)\n//! - ä¸åŒè¯»å†™æ¯”ä¾‹ (0%, 50%, 90%, 95%, 99% read)\n//! - å¹¶å‘è®¿é—®æ€§èƒ½\n//! - æ‰¹é‡æ“ä½œæ€§èƒ½\n\nuse std::sync::Arc;\nuse std::time::Duration;\n\nuse criterion::{BenchmarkId, Criterion, Throughput, black_box, criterion_group, criterion_main};\nuse rskv::{Config, RsKv};\nuse tempfile::tempdir;\n\n/// ç”ŸæˆæŒ‡å®šå¤§å°çš„æµ‹è¯•æ•°æ®\nfn generate_test_data(size: usize) -> Vec<u8> {\n    (0..size).map(|i| (i % 256) as u8).collect()\n}\n\n/// ç”Ÿæˆæµ‹è¯•é”®\nfn generate_key(index: usize) -> Vec<u8> {\n    format!(\"key_{:08}\", index).into_bytes()\n}\n\n/// åˆ›å»ºæµ‹è¯•ç”¨çš„ RsKv å®ä¾‹\nasync fn create_test_rskv(memory_size: u64) -> RsKv {\n    let temp_dir = tempdir().unwrap();\n    let config = Config {\n        storage_dir: temp_dir.path().to_string_lossy().to_string(),\n        memory_size,\n        page_size: 64 * 1024,        // 64KB pages\n        enable_checkpointing: false, // ç¦ç”¨åå°ä»»åŠ¡ä»¥è·å¾—ä¸€è‡´çš„æ€§èƒ½\n        enable_gc: false,\n        use_mmap: true, // å¯ç”¨å†…å­˜æ˜ å°„\n        enable_readahead: true,\n        sync_mode: rskv::common::SyncMode::None, // ç¦ç”¨åŒæ­¥è·å¾—æœ€ä½³æ€§èƒ½\n        ..Default::default()\n    };\n\n    RsKv::new(config).await.unwrap()\n}\n\n/// åŸºç¡€å†™å…¥æ€§èƒ½æµ‹è¯•\nfn bench_write_performance(c: &mut Criterion) {\n    let value_sizes = vec![\n        (\"1B\", 1),\n        (\"100B\", 100),\n        (\"1KB\", 1024),\n        (\"10KB\", 10 * 1024),\n        (\"100KB\", 100 * 1024),\n    ];\n\n    let mut group = c.benchmark_group(\"write_performance\");\n    group.sample_size(20); // å¢åŠ æ ·æœ¬æ•°é‡ç¡®ä¿ç»Ÿè®¡æœ‰æ•ˆæ€§\n    group.measurement_time(Duration::from_secs(10));\n\n    for (size_name, size) in value_sizes {\n        group.throughput(Throughput::Bytes(size as u64 * 100)); // 100 operations\n\n        group.bench_with_input(\n            BenchmarkId::new(\"sequential_write\", size_name),\n            &size,\n            |b, &value_size| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = create_test_rskv(256 * 1024 * 1024).await;\n                            let test_data = generate_test_data(value_size);\n\n                            let start = std::time::Instant::now();\n                            for i in 0..100 {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// åŸºç¡€è¯»å–æ€§èƒ½æµ‹è¯•\nfn bench_read_performance(c: &mut Criterion) {\n    let value_sizes = vec![\n        (\"1B\", 1),\n        (\"100B\", 100),\n        (\"1KB\", 1024),\n        (\"10KB\", 10 * 1024),\n        (\"100KB\", 100 * 1024),\n    ];\n\n    let mut group = c.benchmark_group(\"read_performance\");\n    group.sample_size(20);\n    group.measurement_time(Duration::from_secs(10));\n\n    for (size_name, size) in value_sizes {\n        group.throughput(Throughput::Bytes(size as u64 * 100));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"sequential_read\", size_name),\n            &size,\n            |b, &value_size| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = create_test_rskv(256 * 1024 * 1024).await;\n                            let test_data = generate_test_data(value_size);\n\n                            // é¢„å¡«å……æ•°æ®\n                            for i in 0..100 {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            // æµ‹è¯•è¯»å–\n                            let start = std::time::Instant::now();\n                            for i in 0..100 {\n                                let key = generate_key(i);\n                                let _value = rskv.read(&key).await.unwrap();\n                            }\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// æ··åˆè¯»å†™æ€§èƒ½æµ‹è¯•\nfn bench_mixed_workload(c: &mut Criterion) {\n    let read_percentages = vec![0, 50, 90, 95, 99];\n    let value_size = 1024; // 1KB values\n\n    let mut group = c.benchmark_group(\"mixed_workload\");\n    group.sample_size(15);\n    group.measurement_time(Duration::from_secs(8));\n    group.throughput(Throughput::Elements(100));\n\n    for read_pct in read_percentages {\n        group.bench_with_input(\n            BenchmarkId::new(\"mixed_ops\", format!(\"{}%_read\", read_pct)),\n            &read_pct,\n            |b, &read_percentage| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = create_test_rskv(256 * 1024 * 1024).await;\n                            let test_data = generate_test_data(value_size);\n\n                            // é¢„å¡«å……ä¸€äº›æ•°æ®ç”¨äºè¯»å–\n                            for i in 0..50 {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            let start = std::time::Instant::now();\n\n                            for i in 0..100 {\n                                let should_read = (i % 100) < read_percentage;\n                                let key = generate_key(i % 50); // å¤ç”¨é”®ä»¥ç¡®ä¿è¯»å–å‘½ä¸­\n\n                                if should_read {\n                                    let _value = rskv.read(&key).await.unwrap();\n                                } else {\n                                    rskv.upsert(key, test_data.clone()).await.unwrap();\n                                }\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆæ‰©å±•ç‰ˆï¼‰\nfn bench_concurrent_operations(c: &mut Criterion) {\n    let concurrency_levels = vec![1, 2, 4, 8, 16, 32];\n    let value_size = 1024; // 1KB values\n\n    let mut group = c.benchmark_group(\"concurrent_operations\");\n    group.sample_size(5); // å‡å°‘æ ·æœ¬æ•°é‡ä»¥åŠ å¿«æµ‹è¯•\n    group.measurement_time(Duration::from_secs(15));\n\n    for concurrency in concurrency_levels {\n        group.throughput(Throughput::Elements(100 * concurrency as u64));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"concurrent_mixed\", format!(\"{}_threads\", concurrency)),\n            &concurrency,\n            |b, &num_threads| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = Arc::new(create_test_rskv(512 * 1024 * 1024).await);\n                            let test_data = generate_test_data(value_size);\n\n                            // é¢„å¡«å……æ•°æ®\n                            for i in 0..100 {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            let start = std::time::Instant::now();\n\n                            let mut handles = Vec::new();\n\n                            for thread_id in 0..num_threads {\n                                let rskv_clone = rskv.clone();\n                                let data_clone = test_data.clone();\n\n                                let handle = tokio::spawn(async move {\n                                    for i in 0..100 {\n                                        let key_index = thread_id * 100 + i;\n                                        let key = generate_key(key_index);\n\n                                        // 50% è¯» 50% å†™\n                                        if i % 2 == 0 {\n                                            let _value = rskv_clone.read(&key).await.unwrap();\n                                        } else {\n                                            rskv_clone\n                                                .upsert(key, data_clone.clone())\n                                                .await\n                                                .unwrap();\n                                        }\n                                    }\n                                });\n\n                                handles.push(handle);\n                            }\n\n                            // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n                            for handle in handles {\n                                handle.await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// æ‰¹é‡æ“ä½œæ€§èƒ½æµ‹è¯•\nfn bench_batch_operations(c: &mut Criterion) {\n    let batch_sizes = vec![1, 10, 100];\n    let value_size = 1024; // 1KB values\n\n    let mut group = c.benchmark_group(\"batch_operations\");\n    group.sample_size(10);\n\n    for batch_size in batch_sizes {\n        group.throughput(Throughput::Elements(batch_size as u64));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"batch_write\", format!(\"{}_ops\", batch_size)),\n            &batch_size,\n            |b, &batch_size| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = create_test_rskv(256 * 1024 * 1024).await;\n                            let test_data = generate_test_data(value_size);\n\n                            let start = std::time::Instant::now();\n\n                            // æ¨¡æ‹Ÿæ‰¹é‡å†™å…¥\n                            for i in 0..batch_size {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// æ‰«ææ“ä½œæ€§èƒ½æµ‹è¯•\nfn bench_scan_operations(c: &mut Criterion) {\n    let data_sizes = vec![10, 100, 1000];\n\n    let mut group = c.benchmark_group(\"scan_operations\");\n    group.sample_size(5); // æ‰«ææ“ä½œæ¯”è¾ƒæ…¢ï¼Œå‡å°‘æ ·æœ¬æ•°é‡\n\n    for data_size in data_sizes {\n        group.throughput(Throughput::Elements(data_size as u64));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"scan_all\", format!(\"{}_entries\", data_size)),\n            &data_size,\n            |b, &data_size| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = create_test_rskv(256 * 1024 * 1024).await;\n                            let test_data = generate_test_data(100); // 100B values\n\n                            // å¡«å……æ•°æ®\n                            for i in 0..data_size {\n                                let key = generate_key(i);\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            let start = std::time::Instant::now();\n                            let _results = rskv.scan_all().await.unwrap();\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// ä¸“é—¨çš„å¤šçº¿ç¨‹æ‰©å±•æ€§æµ‹è¯•\nfn bench_thread_scaling(c: &mut Criterion) {\n    let thread_counts = vec![1, 2, 4, 8, 16, 24, 32];\n    let value_size = 1024; // 1KB values\n\n    let mut group = c.benchmark_group(\"thread_scaling\");\n    group.sample_size(5);\n    group.measurement_time(Duration::from_secs(12));\n\n    for thread_count in thread_counts {\n        group.throughput(Throughput::Elements(1000 * thread_count as u64));\n\n        // å†™å…¥æ‰©å±•æ€§æµ‹è¯•\n        group.bench_with_input(\n            BenchmarkId::new(\"write_scaling\", format!(\"{}_threads\", thread_count)),\n            &thread_count,\n            |b, &num_threads| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = Arc::new(create_test_rskv(512 * 1024 * 1024).await);\n                            let test_data = generate_test_data(value_size);\n\n                            let start = std::time::Instant::now();\n\n                            let mut handles = Vec::new();\n\n                            for thread_id in 0..num_threads {\n                                let rskv_clone = rskv.clone();\n                                let data_clone = test_data.clone();\n\n                                let handle = tokio::spawn(async move {\n                                    for i in 0..1000 {\n                                        let key =\n                                            format!(\"thread_{}_{}\", thread_id, i).into_bytes();\n                                        rskv_clone.upsert(key, data_clone.clone()).await.unwrap();\n                                    }\n                                });\n\n                                handles.push(handle);\n                            }\n\n                            for handle in handles {\n                                handle.await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n\n        // è¯»å–æ‰©å±•æ€§æµ‹è¯•\n        group.bench_with_input(\n            BenchmarkId::new(\"read_scaling\", format!(\"{}_threads\", thread_count)),\n            &thread_count,\n            |b, &num_threads| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = Arc::new(create_test_rskv(512 * 1024 * 1024).await);\n                            let test_data = generate_test_data(value_size);\n\n                            // é¢„å¡«å……æ•°æ®\n                            for i in 0..1000 {\n                                let key = format!(\"read_test_{}\", i).into_bytes();\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            let start = std::time::Instant::now();\n\n                            let mut handles = Vec::new();\n\n                            for thread_id in 0..num_threads {\n                                let rskv_clone = rskv.clone();\n\n                                let handle = tokio::spawn(async move {\n                                    for i in 0..1000 {\n                                        let key = format!(\"read_test_{}\", i % 1000).into_bytes();\n                                        let _value = rskv_clone.read(&key).await.unwrap();\n                                    }\n                                });\n\n                                handles.push(handle);\n                            }\n\n                            for handle in handles {\n                                handle.await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n\n        // æ··åˆæ‰©å±•æ€§æµ‹è¯• (70% è¯», 30% å†™)\n        group.bench_with_input(\n            BenchmarkId::new(\"mixed_scaling\", format!(\"{}_threads\", thread_count)),\n            &thread_count,\n            |b, &num_threads| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = Arc::new(create_test_rskv(512 * 1024 * 1024).await);\n                            let test_data = generate_test_data(value_size);\n\n                            // é¢„å¡«å……æ•°æ®\n                            for i in 0..500 {\n                                let key = format!(\"mixed_test_{}\", i).into_bytes();\n                                rskv.upsert(key, test_data.clone()).await.unwrap();\n                            }\n\n                            let start = std::time::Instant::now();\n\n                            let mut handles = Vec::new();\n\n                            for thread_id in 0..num_threads {\n                                let rskv_clone = rskv.clone();\n                                let data_clone = test_data.clone();\n\n                                let handle = tokio::spawn(async move {\n                                    for i in 0..1000 {\n                                        let key = format!(\"mixed_test_{}\", i % 500).into_bytes();\n\n                                        // 70% è¯», 30% å†™\n                                        if i % 10 < 7 {\n                                            let _value = rskv_clone.read(&key).await.unwrap();\n                                        } else {\n                                            rskv_clone\n                                                .upsert(key, data_clone.clone())\n                                                .await\n                                                .unwrap();\n                                        }\n                                    }\n                                });\n\n                                handles.push(handle);\n                            }\n\n                            for handle in handles {\n                                handle.await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n/// é«˜å¹¶å‘å‹åŠ›æµ‹è¯•\nfn bench_high_concurrency(c: &mut Criterion) {\n    let scenarios = vec![\n        (\"light_load\", 1000, 100),  // 1000 threads, 100 ops each\n        (\"heavy_load\", 100, 10000), // 100 threads, 10000 ops each\n    ];\n\n    let mut group = c.benchmark_group(\"high_concurrency\");\n    group.sample_size(3);\n    group.measurement_time(Duration::from_secs(20));\n\n    for (scenario_name, thread_count, ops_per_thread) in scenarios {\n        group.throughput(Throughput::Elements(thread_count * ops_per_thread));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"stress_test\", scenario_name),\n            &(thread_count, ops_per_thread),\n            |b, &(num_threads, ops_per_thread)| {\n                b.iter_custom(|iters| {\n                    let rt = tokio::runtime::Runtime::new().unwrap();\n\n                    let mut total_duration = Duration::from_nanos(0);\n\n                    for _ in 0..iters {\n                        let duration = rt.block_on(async {\n                            let rskv = Arc::new(create_test_rskv(1024 * 1024 * 1024).await); // 1GB\n                            let test_data = generate_test_data(256); // 256B values\n\n                            let start = std::time::Instant::now();\n\n                            let mut handles = Vec::new();\n\n                            for thread_id in 0..num_threads {\n                                let rskv_clone = rskv.clone();\n                                let data_clone = test_data.clone();\n\n                                let handle = tokio::spawn(async move {\n                                    for i in 0..ops_per_thread {\n                                        let key =\n                                            format!(\"stress_{}_{}\", thread_id, i).into_bytes();\n\n                                        // 80% å†™, 20% è¯»\n                                        if i % 5 < 4 {\n                                            rskv_clone\n                                                .upsert(key, data_clone.clone())\n                                                .await\n                                                .unwrap();\n                                        } else {\n                                            let _value = rskv_clone.read(&key).await.ok();\n                                        }\n                                    }\n                                });\n\n                                handles.push(handle);\n                            }\n\n                            for handle in handles {\n                                handle.await.unwrap();\n                            }\n\n                            start.elapsed()\n                        });\n\n                        total_duration += duration;\n                    }\n\n                    total_duration\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\ncriterion_group!(\n    benches,\n    bench_write_performance,\n    bench_read_performance,\n    bench_mixed_workload,\n    bench_concurrent_operations,\n    bench_thread_scaling,\n    bench_high_concurrency,\n    bench_batch_operations,\n    bench_scan_operations\n);\n\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","examples","basic_usage.rs"],"content":"//! Basic usage example for rskv\n//!\n//! This example demonstrates the core functionality of the rskv key-value store.\n\nuse rskv::{Config, RsKv};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Initialize logging\n    env_logger::init();\n\n    // Create a temporary directory for this example\n    let temp_dir = tempfile::tempdir()?;\n\n    // Configure the rskv instance\n    let config = Config {\n        storage_dir: temp_dir.path().to_string_lossy().to_string(),\n        memory_size: 64 * 1024 * 1024, // 64MB\n        enable_checkpointing: true,\n        checkpoint_interval_ms: 5000,\n        enable_gc: true,\n        gc_interval_ms: 10000,\n        ..Default::default()\n    };\n\n    println!(\"ğŸš€ Initializing rskv with config: {:?}\", config);\n\n    // Create the key-value store\n    let store = RsKv::new(config).await?;\n\n    println!(\"âœ… rskv initialized successfully\");\n\n    // Basic operations\n    println!(\"\\nğŸ“ Demonstrating basic operations...\");\n\n    // Insert some data\n    let key1 = b\"user:1001\".to_vec();\n    let value1 = b\"Alice\".to_vec();\n\n    let key2 = b\"user:1002\".to_vec();\n    let value2 = b\"Bob\".to_vec();\n\n    let key3 = b\"config:timeout\".to_vec();\n    let value3 = b\"30\".to_vec();\n\n    store.upsert(key1.clone(), value1.clone()).await?;\n    store.upsert(key2.clone(), value2.clone()).await?;\n    store.upsert(key3.clone(), value3.clone()).await?;\n\n    println!(\"âœ… Inserted 3 key-value pairs\");\n\n    // Read the data back\n    if let Some(value) = store.read(&key1).await? {\n        println!(\n            \"ğŸ” Read key 'user:1001': {}\",\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    if let Some(value) = store.read(&key2).await? {\n        println!(\n            \"ğŸ” Read key 'user:1002': {}\",\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    if let Some(value) = store.read(&key3).await? {\n        println!(\n            \"ğŸ” Read key 'config:timeout': {}\",\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    // Check if keys exist\n    println!(\"\\nğŸ” Checking key existence...\");\n    println!(\n        \"Key 'user:1001' exists: {}\",\n        store.contains_key(&key1).await?\n    );\n    println!(\n        \"Key 'user:9999' exists: {}\",\n        store.contains_key(&b\"user:9999\".to_vec()).await?\n    );\n\n    // Update a value\n    println!(\"\\nâœï¸  Updating values...\");\n    let new_value1 = b\"Alice Smith\".to_vec();\n    store.upsert(key1.clone(), new_value1.clone()).await?;\n\n    if let Some(value) = store.read(&key1).await? {\n        println!(\n            \"ğŸ” Updated 'user:1001': {}\",\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    // Demonstrate prefix scanning\n    println!(\"\\nğŸ” Prefix scan for 'user:' keys...\");\n    let user_entries = store.scan_prefix(b\"user:\").await?;\n    for (key, value) in user_entries {\n        println!(\n            \"  {} = {}\",\n            String::from_utf8_lossy(&key),\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    // Delete a key\n    println!(\"\\nğŸ—‘ï¸  Deleting key 'user:1002'...\");\n    store.delete(&key2).await?;\n\n    println!(\n        \"Key 'user:1002' exists after deletion: {}\",\n        store.contains_key(&key2).await?\n    );\n\n    // Show statistics\n    println!(\"\\nğŸ“Š Store statistics:\");\n    let stats = store.stats();\n    println!(\"  Index entries: {}\", stats.index_entries);\n    println!(\"  Log tail address: 0x{:x}\", stats.log_tail_address);\n    println!(\"  Mutable region size: {} bytes\", stats.mutable_region_size);\n    println!(\n        \"  Read-only region size: {} bytes\",\n        stats.read_only_region_size\n    );\n    println!(\"  Disk region size: {} bytes\", stats.disk_region_size);\n\n    // Perform a checkpoint\n    println!(\"\\nğŸ’¾ Performing checkpoint...\");\n    store.checkpoint().await?;\n    println!(\"âœ… Checkpoint completed\");\n\n    // Scan all remaining data\n    println!(\"\\nğŸ“‹ All remaining data:\");\n    let all_entries = store.scan_all().await?;\n    for (key, value) in all_entries {\n        println!(\n            \"  {} = {}\",\n            String::from_utf8_lossy(&key),\n            String::from_utf8_lossy(&value)\n        );\n    }\n\n    // Close the store\n    println!(\"\\nğŸ”’ Closing store...\");\n    store.close().await?;\n    println!(\"âœ… Store closed successfully\");\n\n    println!(\"\\nğŸ‰ Example completed successfully!\");\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","examples","concurrency_demo.rs"],"content":"//! rskv å¹¶å‘æ€§èƒ½æ¼”ç¤º\n//!\n//! è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† rskv åœ¨ä¸åŒçº¿ç¨‹æ•°ä¸‹çš„å¹¶å‘æ€§èƒ½å’Œæ‰©å±•æ€§\n\nuse std::sync::Arc;\nuse std::time::Instant;\n\nuse rskv::{Config, RsKv};\nuse tempfile::tempdir;\nuse tokio::task::JoinSet;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    env_logger::init();\n\n    println!(\"ğŸš€ rskv å¹¶å‘æ€§èƒ½æ¼”ç¤º\");\n    println!(\"==================\");\n\n    // åˆ›å»ºæµ‹è¯•å®ä¾‹\n    let temp_dir = tempdir()?;\n    let config = Config {\n        storage_dir: temp_dir.path().to_string_lossy().to_string(),\n        memory_size: 512 * 1024 * 1024, // 512MB\n        use_mmap: true,\n        enable_readahead: true,\n        sync_mode: rskv::common::SyncMode::None,\n        enable_checkpointing: false,\n        enable_gc: false,\n        ..Default::default()\n    };\n\n    let rskv = Arc::new(RsKv::new(config).await?);\n\n    // æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„å†™å…¥æ€§èƒ½\n    println!(\"\\nğŸ“ å¤šçº¿ç¨‹å†™å…¥æ€§èƒ½æµ‹è¯•\");\n    println!(\"======================\");\n\n    let thread_counts = vec![1, 2, 4, 8, 16, 32];\n    let ops_per_thread = 1000;\n    let value_size = 1024; // 1KB\n\n    for &thread_count in &thread_counts {\n        let rskv_clone = rskv.clone();\n        let test_data = vec![42u8; value_size];\n\n        let start = Instant::now();\n\n        let mut join_set = JoinSet::new();\n\n        for thread_id in 0..thread_count {\n            let rskv_ref = rskv_clone.clone();\n            let data = test_data.clone();\n\n            join_set.spawn(async move {\n                for i in 0..ops_per_thread {\n                    let key = format!(\"write_{}_{}\", thread_id, i).into_bytes();\n                    rskv_ref.upsert(key, data.clone()).await.unwrap();\n                }\n                ops_per_thread\n            });\n        }\n\n        let mut total_ops = 0;\n        while let Some(result) = join_set.join_next().await {\n            total_ops += result?;\n        }\n\n        let elapsed = start.elapsed();\n        let ops_per_sec = total_ops as f64 / elapsed.as_secs_f64();\n        let throughput =\n            (total_ops * value_size) as f64 / elapsed.as_secs_f64() / (1024.0 * 1024.0);\n\n        println!(\n            \"  {} çº¿ç¨‹: {:.0} ops/s, {:.2} MB/s, {:.2} Âµs/op\",\n            thread_count,\n            ops_per_sec,\n            throughput,\n            elapsed.as_micros() as f64 / total_ops as f64\n        );\n    }\n\n    // æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„è¯»å–æ€§èƒ½\n    println!(\"\\nğŸ“– å¤šçº¿ç¨‹è¯»å–æ€§èƒ½æµ‹è¯•\");\n    println!(\"======================\");\n\n    for &thread_count in &thread_counts {\n        let rskv_clone = rskv.clone();\n        let thread_count_len = thread_counts.len();\n\n        let start = Instant::now();\n\n        let mut join_set = JoinSet::new();\n\n        for thread_id in 0..thread_count {\n            let rskv_ref = rskv_clone.clone();\n\n            join_set.spawn(async move {\n                let mut successful_reads = 0;\n                for i in 0..ops_per_thread {\n                    // è¯»å–ä¹‹å‰å†™å…¥çš„æ•°æ®\n                    let key = format!(\"write_{}_{}\", thread_id % thread_count_len, i).into_bytes();\n                    if rskv_ref.read(&key).await.unwrap().is_some() {\n                        successful_reads += 1;\n                    }\n                }\n                successful_reads\n            });\n        }\n\n        let mut total_reads = 0;\n        while let Some(result) = join_set.join_next().await {\n            total_reads += result?;\n        }\n\n        let elapsed = start.elapsed();\n        let ops_per_sec = total_reads as f64 / elapsed.as_secs_f64();\n        let throughput =\n            (total_reads * value_size) as f64 / elapsed.as_secs_f64() / (1024.0 * 1024.0);\n\n        println!(\n            \"  {} çº¿ç¨‹: {:.0} ops/s, {:.2} MB/s, {:.2} Âµs/op (å‘½ä¸­: {})\",\n            thread_count,\n            ops_per_sec,\n            throughput,\n            elapsed.as_micros() as f64 / total_reads as f64,\n            total_reads\n        );\n    }\n\n    // æµ‹è¯•æ··åˆå·¥ä½œè´Ÿè½½çš„çº¿ç¨‹æ‰©å±•æ€§\n    println!(\"\\nğŸ”„ æ··åˆå·¥ä½œè´Ÿè½½çº¿ç¨‹æ‰©å±•æ€§æµ‹è¯• (70% è¯» + 30% å†™)\");\n    println!(\"===============================================\");\n\n    for &thread_count in &thread_counts {\n        let rskv_clone = rskv.clone();\n        let test_data = vec![42u8; value_size];\n\n        let start = Instant::now();\n\n        let mut join_set = JoinSet::new();\n\n        for thread_id in 0..thread_count {\n            let rskv_ref = rskv_clone.clone();\n            let data = test_data.clone();\n\n            join_set.spawn(async move {\n                let mut read_count = 0;\n                let mut write_count = 0;\n\n                for i in 0..ops_per_thread {\n                    let key = format!(\"mixed_{}_{}\", thread_id, i % 100).into_bytes();\n\n                    // 70% è¯»å–, 30% å†™å…¥\n                    if i % 10 < 7 {\n                        let _value = rskv_ref.read(&key).await.unwrap();\n                        read_count += 1;\n                    } else {\n                        rskv_ref.upsert(key, data.clone()).await.unwrap();\n                        write_count += 1;\n                    }\n                }\n\n                (read_count, write_count)\n            });\n        }\n\n        let mut total_reads = 0;\n        let mut total_writes = 0;\n\n        while let Some(result) = join_set.join_next().await {\n            let (reads, writes) = result?;\n            total_reads += reads;\n            total_writes += writes;\n        }\n\n        let elapsed = start.elapsed();\n        let total_ops = total_reads + total_writes;\n        let ops_per_sec = total_ops as f64 / elapsed.as_secs_f64();\n\n        println!(\n            \"  {} çº¿ç¨‹: {:.0} ops/s ({} è¯» + {} å†™), {:.2} Âµs/op\",\n            thread_count,\n            ops_per_sec,\n            total_reads,\n            total_writes,\n            elapsed.as_micros() as f64 / total_ops as f64\n        );\n    }\n\n    // è®¡ç®—çº¿ç¨‹æ‰©å±•æ€§æ•ˆç‡\n    println!(\"\\nğŸ“Š çº¿ç¨‹æ‰©å±•æ€§åˆ†æ\");\n    println!(\"==================\");\n\n    // é‡æ–°è¿è¡Œä¸€æ¬¡å†™å…¥æµ‹è¯•æ¥è®¡ç®—æ‰©å±•æ€§\n    let mut write_results = Vec::new();\n\n    for &thread_count in &thread_counts {\n        let rskv_clone = rskv.clone();\n        let test_data = vec![42u8; value_size];\n        let mini_ops = 500; // å‡å°‘æ“ä½œæ•°é‡ä»¥åŠ å¿«æµ‹è¯•\n\n        let start = Instant::now();\n\n        let mut join_set = JoinSet::new();\n\n        for thread_id in 0..thread_count {\n            let rskv_ref = rskv_clone.clone();\n            let data = test_data.clone();\n\n            join_set.spawn(async move {\n                for i in 0..mini_ops {\n                    let key = format!(\"scale_{}_{}\", thread_id, i).into_bytes();\n                    rskv_ref.upsert(key, data.clone()).await.unwrap();\n                }\n            });\n        }\n\n        while let Some(_) = join_set.join_next().await {}\n\n        let elapsed = start.elapsed();\n        let total_ops = thread_count * mini_ops;\n        let ops_per_sec = total_ops as f64 / elapsed.as_secs_f64();\n\n        write_results.push((thread_count, ops_per_sec));\n    }\n\n    // è®¡ç®—ç›¸å¯¹äºå•çº¿ç¨‹çš„åŠ é€Ÿæ¯”\n    let baseline_perf = write_results[0].1; // å•çº¿ç¨‹æ€§èƒ½\n\n    println!(\"çº¿ç¨‹æ•°  | æ€§èƒ½ (ops/s) | åŠ é€Ÿæ¯”  | æ•ˆç‡   | è¯„çº§\");\n    println!(\"-------|-------------|--------|--------|--------\");\n\n    for (thread_count, ops_per_sec) in write_results {\n        let speedup = ops_per_sec / baseline_perf;\n        let efficiency = speedup / thread_count as f64;\n        let rating = if efficiency > 0.8 {\n            \"ğŸŸ¢ ä¼˜ç§€\"\n        } else if efficiency > 0.6 {\n            \"ğŸŸ¡ è‰¯å¥½\"\n        } else if efficiency > 0.4 {\n            \"ğŸŸ  ä¸€èˆ¬\"\n        } else {\n            \"ğŸ”´ è¾ƒå·®\"\n        };\n\n        println!(\n            \"{:^6} | {:^11.0} | {:^6.2}x | {:^6.1}% | {}\",\n            thread_count,\n            ops_per_sec,\n            speedup,\n            efficiency * 100.0,\n            rating\n        );\n    }\n\n    // é«˜å¹¶å‘å‹åŠ›æµ‹è¯•\n    println!(\"\\nğŸ’¥ é«˜å¹¶å‘å‹åŠ›æµ‹è¯•\");\n    println!(\"==================\");\n\n    let stress_scenarios = vec![\n        (\"è½»è´Ÿè½½\", 50, 100),  // 50çº¿ç¨‹ï¼Œæ¯çº¿ç¨‹100æ“ä½œ\n        (\"ä¸­è´Ÿè½½\", 100, 200), // 100çº¿ç¨‹ï¼Œæ¯çº¿ç¨‹200æ“ä½œ\n        (\"é‡è´Ÿè½½\", 200, 100), // 200çº¿ç¨‹ï¼Œæ¯çº¿ç¨‹100æ“ä½œ\n    ];\n\n    for (name, thread_count, ops_per_thread) in stress_scenarios {\n        let rskv_clone = rskv.clone();\n        let test_data = vec![42u8; 256]; // 256Bæ•°æ®\n\n        println!(\n            \"\\n{} - {} çº¿ç¨‹ x {} æ“ä½œ\",\n            name, thread_count, ops_per_thread\n        );\n\n        let start = Instant::now();\n\n        let mut join_set = JoinSet::new();\n\n        for thread_id in 0..thread_count {\n            let rskv_ref = rskv_clone.clone();\n            let data = test_data.clone();\n\n            join_set.spawn(async move {\n                for i in 0..ops_per_thread {\n                    let key = format!(\"stress_{}_{}_{}\", name, thread_id, i).into_bytes();\n\n                    // 80% å†™å…¥, 20% è¯»å–\n                    if i % 5 < 4 {\n                        rskv_ref.upsert(key, data.clone()).await.unwrap();\n                    } else {\n                        let _ = rskv_ref.read(&key).await;\n                    }\n                }\n            });\n        }\n\n        while let Some(_) = join_set.join_next().await {}\n\n        let elapsed = start.elapsed();\n        let total_ops = thread_count * ops_per_thread;\n        let ops_per_sec = total_ops as f64 / elapsed.as_secs_f64();\n        let throughput =\n            (total_ops * test_data.len()) as f64 / elapsed.as_secs_f64() / (1024.0 * 1024.0);\n\n        println!(\n            \"  ç»“æœ: {:.0} ops/s, {:.2} MB/s, {:.2} ms æ€»æ—¶é—´\",\n            ops_per_sec,\n            throughput,\n            elapsed.as_millis()\n        );\n    }\n\n    // æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡\n    println!(\"\\nğŸ“Š å­˜å‚¨ç»Ÿè®¡\");\n    println!(\"============\");\n    let stats = rskv.stats();\n    println!(\"  ç´¢å¼•æ¡ç›®æ•°: {}\", stats.index_entries);\n    println!(\"  æ—¥å¿—å°¾åœ°å€: 0x{:x}\", stats.log_tail_address);\n    println!(\n        \"  å¯å˜åŒºåŸŸå¤§å°: {:.2} MB\",\n        stats.mutable_region_size as f64 / (1024.0 * 1024.0)\n    );\n    println!(\n        \"  åªè¯»åŒºåŸŸå¤§å°: {:.2} MB\",\n        stats.read_only_region_size as f64 / (1024.0 * 1024.0)\n    );\n\n    println!(\"\\nâœ… å¹¶å‘æ€§èƒ½æ¼”ç¤ºå®Œæˆ!\");\n    println!(\"\\nğŸ’¡ å…³é”®å‘ç°:\");\n    println!(\"  - rskv å±•ç°å‡ºè‰¯å¥½çš„å¤šçº¿ç¨‹æ‰©å±•æ€§\");\n    println!(\"  - å†™å…¥æ“ä½œåœ¨å¤šçº¿ç¨‹ä¸‹æ‰©å±•æ€§ä¼˜äºè¯»å–æ“ä½œ\");\n    println!(\"  - æ··åˆå·¥ä½œè´Ÿè½½åœ¨é«˜å¹¶å‘ä¸‹è¡¨ç°ç¨³å®š\");\n    println!(\"  - é€‚åˆé«˜å¹¶å‘ã€ä½å»¶è¿Ÿçš„åº”ç”¨åœºæ™¯\");\n\n    println!(\"\\nğŸ”— æ›´å¤šæµ‹è¯•:\");\n    println!(\"  - è¿è¡Œ 'cargo bench --bench performance -- thread_scaling' è¿›è¡Œè¯¦ç»†åŸºå‡†æµ‹è¯•\");\n    println!(\"  - è¿è¡Œ 'cargo bench --bench performance -- high_concurrency' è¿›è¡Œé«˜å¹¶å‘æµ‹è¯•\");\n    println!(\"  - æŸ¥çœ‹ 'target/criterion/' è·å–è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š\");\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","examples","performance_demo.rs"],"content":"//! rskv æ€§èƒ½æ¼”ç¤º\n//!\n//! è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº† rskv åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°\n\nuse std::time::Instant;\n\nuse rskv::{Config, RsKv};\nuse tempfile::tempdir;\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    env_logger::init();\n\n    println!(\"ğŸš€ rskv æ€§èƒ½æ¼”ç¤º\");\n    println!(\"================\");\n\n    // åˆ›å»ºæµ‹è¯•å®ä¾‹\n    let temp_dir = tempdir()?;\n    let config = Config {\n        storage_dir: temp_dir.path().to_string_lossy().to_string(),\n        memory_size: 256 * 1024 * 1024, // 256MB\n        use_mmap: true,\n        enable_readahead: true,\n        sync_mode: rskv::common::SyncMode::None,\n        enable_checkpointing: false,\n        enable_gc: false,\n        ..Default::default()\n    };\n\n    let rskv = RsKv::new(config).await?;\n\n    // æµ‹è¯•ä¸åŒå¤§å°çš„å†™å…¥æ€§èƒ½\n    println!(\"\\nğŸ“ å†™å…¥æ€§èƒ½æµ‹è¯•\");\n    println!(\"----------------\");\n\n    let value_sizes = vec![\n        (\"1B\", 1),\n        (\"100B\", 100),\n        (\"1KB\", 1024),\n        (\"10KB\", 10 * 1024),\n        (\"100KB\", 100 * 1024),\n    ];\n\n    for (size_name, size) in &value_sizes {\n        let test_data = vec![42u8; *size];\n        let num_ops = if *size > 10240 { 100 } else { 1000 };\n\n        let start = Instant::now();\n        for i in 0..num_ops {\n            let key = format!(\"write_test_{}_{}\", size_name, i).into_bytes();\n            rskv.upsert(key, test_data.clone()).await?;\n        }\n        let elapsed = start.elapsed();\n\n        let throughput =\n            (*size as f64 * num_ops as f64) / elapsed.as_secs_f64() / (1024.0 * 1024.0);\n        let ops_per_sec = num_ops as f64 / elapsed.as_secs_f64();\n\n        println!(\n            \"  {}: {:.2} MB/s, {:.0} ops/s, {:.2} Âµs/op\",\n            size_name,\n            throughput,\n            ops_per_sec,\n            elapsed.as_micros() as f64 / num_ops as f64\n        );\n    }\n\n    // æµ‹è¯•è¯»å–æ€§èƒ½\n    println!(\"\\nğŸ“– è¯»å–æ€§èƒ½æµ‹è¯•\");\n    println!(\"----------------\");\n\n    for (size_name, size) in &value_sizes {\n        let num_ops = if *size > 10240 { 100 } else { 1000 };\n\n        let start = Instant::now();\n        for i in 0..num_ops {\n            let key = format!(\"write_test_{}_{}\", size_name, i).into_bytes();\n            let _value = rskv.read(&key).await?;\n        }\n        let elapsed = start.elapsed();\n\n        let throughput =\n            (*size as f64 * num_ops as f64) / elapsed.as_secs_f64() / (1024.0 * 1024.0);\n        let ops_per_sec = num_ops as f64 / elapsed.as_secs_f64();\n\n        println!(\n            \"  {}: {:.2} MB/s, {:.0} ops/s, {:.2} Âµs/op\",\n            size_name,\n            throughput,\n            ops_per_sec,\n            elapsed.as_micros() as f64 / num_ops as f64\n        );\n    }\n\n    // æµ‹è¯•æ··åˆå·¥ä½œè´Ÿè½½\n    println!(\"\\nğŸ”„ æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯• (1KB values)\");\n    println!(\"--------------------------------\");\n\n    let test_data = vec![42u8; 1024];\n    let read_percentages = vec![0, 50, 90, 95, 99];\n\n    for read_pct in read_percentages {\n        let num_ops = 1000;\n\n        let start = Instant::now();\n        for i in 0..num_ops {\n            let key = format!(\"mixed_test_{}\", i % 100).into_bytes();\n\n            if (i % 100) < read_pct {\n                // è¯»æ“ä½œ\n                let _value = rskv.read(&key).await?;\n            } else {\n                // å†™æ“ä½œ\n                rskv.upsert(key, test_data.clone()).await?;\n            }\n        }\n        let elapsed = start.elapsed();\n\n        let ops_per_sec = num_ops as f64 / elapsed.as_secs_f64();\n\n        println!(\n            \"  {}% è¯»å–: {:.0} ops/s, {:.2} Âµs/op\",\n            read_pct,\n            ops_per_sec,\n            elapsed.as_micros() as f64 / num_ops as f64\n        );\n    }\n\n    // æµ‹è¯•æ‰«ææ€§èƒ½\n    println!(\"\\nğŸ” æ‰«ææ“ä½œæµ‹è¯•\");\n    println!(\"----------------\");\n\n    // å‡†å¤‡æ‰«ææµ‹è¯•æ•°æ®\n    let scan_data = vec![42u8; 100];\n    for i in 0..1000 {\n        let key = format!(\"scan_test_{:04}\", i).into_bytes();\n        rskv.upsert(key, scan_data.clone()).await?;\n    }\n\n    // å…¨è¡¨æ‰«æ\n    let start = Instant::now();\n    let all_results = rskv.scan_all().await?;\n    let scan_elapsed = start.elapsed();\n\n    println!(\n        \"  å…¨è¡¨æ‰«æ: {} æ¡è®°å½•, {:.2} ms, {:.0} records/s\",\n        all_results.len(),\n        scan_elapsed.as_millis(),\n        all_results.len() as f64 / scan_elapsed.as_secs_f64()\n    );\n\n    // å‰ç¼€æ‰«æ\n    let start = Instant::now();\n    let prefix_results = rskv.scan_prefix(b\"scan_test_\").await?;\n    let prefix_elapsed = start.elapsed();\n\n    println!(\n        \"  å‰ç¼€æ‰«æ: {} æ¡è®°å½•, {:.2} ms, {:.0} records/s\",\n        prefix_results.len(),\n        prefix_elapsed.as_millis(),\n        prefix_results.len() as f64 / prefix_elapsed.as_secs_f64()\n    );\n\n    // æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯\n    println!(\"\\nğŸ“Š å­˜å‚¨ç»Ÿè®¡\");\n    println!(\"------------\");\n    let stats = rskv.stats();\n    println!(\"  ç´¢å¼•æ¡ç›®æ•°: {}\", stats.index_entries);\n    println!(\"  æ—¥å¿—å°¾åœ°å€: 0x{:x}\", stats.log_tail_address);\n    println!(\"  å¯å˜åŒºåŸŸå¤§å°: {} bytes\", stats.mutable_region_size);\n    println!(\"  åªè¯»åŒºåŸŸå¤§å°: {} bytes\", stats.read_only_region_size);\n    println!(\"  ç£ç›˜åŒºåŸŸå¤§å°: {} bytes\", stats.disk_region_size);\n\n    println!(\"\\nâœ… æ€§èƒ½æ¼”ç¤ºå®Œæˆ!\");\n    println!(\"\\nğŸ’¡ æç¤º:\");\n    println!(\"  - è¿è¡Œ 'make perf-quick' è¿›è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•\");\n    println!(\"  - è¿è¡Œ 'make performance' è¿›è¡Œå®Œæ•´æ€§èƒ½åˆ†æ\");\n    println!(\"  - æŸ¥çœ‹ PERFORMANCE.md äº†è§£è¯¦ç»†çš„æ€§èƒ½æµ‹è¯•æŒ‡å—\");\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","background.rs"],"content":"//! Background task management for rskv\n//!\n//! This module implements background tasks for automatic checkpointing,\n//! garbage collection, and log maintenance operations.\n\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicBool, Ordering};\n\nuse tokio::sync::RwLock as AsyncRwLock;\nuse tokio::time::{Duration, MissedTickBehavior, interval};\n\nuse crate::checkpoint::CheckpointState;\nuse crate::common::{Config, Result, RsKvError};\nuse crate::gc::{GcConfig, GcState};\nuse crate::hlog::HybridLog;\n\n/// Background task manager for automatic maintenance operations\npub struct BackgroundTaskManager {\n    /// Whether background tasks are running\n    running: Arc<AtomicBool>,\n\n    /// Configuration\n    config: Config,\n\n    /// Reference to checkpoint state\n    checkpoint_state: Arc<CheckpointState>,\n\n    /// Reference to GC state\n    gc_state: Arc<GcState>,\n\n    /// Reference to hybrid log\n    hlog: Arc<HybridLog>,\n\n    /// Lock to coordinate with manual operations\n    operation_lock: Arc<AsyncRwLock<()>>,\n\n    /// Task handles for cleanup\n    task_handles: parking_lot::Mutex<Vec<tokio::task::JoinHandle<()>>>,\n}\n\nimpl BackgroundTaskManager {\n    /// Create a new background task manager\n    pub fn new(\n        config: Config,\n        checkpoint_state: Arc<CheckpointState>,\n        gc_state: Arc<GcState>,\n        hlog: Arc<HybridLog>,\n        operation_lock: Arc<AsyncRwLock<()>>,\n    ) -> Self {\n        Self {\n            running: Arc::new(AtomicBool::new(false)),\n            config,\n            checkpoint_state,\n            gc_state,\n            hlog,\n            operation_lock,\n            task_handles: parking_lot::Mutex::new(Vec::new()),\n        }\n    }\n\n    /// Start all background tasks\n    pub fn start(&self) -> Result<()> {\n        if self\n            .running\n            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n            .is_err()\n        {\n            return Err(RsKvError::Internal {\n                message: \"Background tasks are already running\".to_string(),\n            });\n        }\n\n        log::info!(\"Starting background task manager\");\n\n        let mut handles = self.task_handles.lock();\n\n        // Start checkpoint task if enabled\n        if self.config.enable_checkpointing {\n            let handle = self.start_checkpoint_task();\n            handles.push(handle);\n        }\n\n        // Start GC task if enabled\n        if self.config.enable_gc {\n            let handle = self.start_gc_task();\n            handles.push(handle);\n        }\n\n        // Start log maintenance task\n        let handle = self.start_log_maintenance_task();\n        handles.push(handle);\n\n        log::info!(\"Started {} background tasks\", handles.len());\n        Ok(())\n    }\n\n    /// Stop all background tasks\n    pub async fn stop(&self) -> Result<()> {\n        if !self.running.swap(false, Ordering::AcqRel) {\n            return Ok(()); // Already stopped\n        }\n\n        log::info!(\"Stopping background tasks\");\n\n        // Cancel all tasks\n        let handles = {\n            let mut handles = self.task_handles.lock();\n            std::mem::take(&mut *handles)\n        };\n\n        for handle in handles {\n            handle.abort();\n            let _ = handle.await; // Ignore cancellation errors\n        }\n\n        log::info!(\"All background tasks stopped\");\n        Ok(())\n    }\n\n    /// Check if background tasks are running\n    pub fn is_running(&self) -> bool {\n        self.running.load(Ordering::Acquire)\n    }\n\n    /// Start the checkpoint task\n    fn start_checkpoint_task(&self) -> tokio::task::JoinHandle<()> {\n        let running = self.running.clone();\n        let checkpoint_state = self.checkpoint_state.clone();\n        let operation_lock = self.operation_lock.clone();\n        let interval_ms = self.config.checkpoint_interval_ms;\n\n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_millis(interval_ms));\n            interval.set_missed_tick_behavior(MissedTickBehavior::Delay);\n\n            log::info!(\"Checkpoint task started with interval {interval_ms}ms\");\n\n            while running.load(Ordering::Acquire) {\n                interval.tick().await;\n\n                if !running.load(Ordering::Acquire) {\n                    break;\n                }\n\n                // Try to acquire lock for checkpoint\n                if let Ok(_lock) = operation_lock.try_write() {\n                    match checkpoint_state.initiate_checkpoint().await {\n                        Ok(metadata) => {\n                            log::debug!(\n                                \"Background checkpoint {} completed\",\n                                metadata.checkpoint_id\n                            );\n                        }\n                        Err(e) => {\n                            log::warn!(\"Background checkpoint failed: {e}\");\n                        }\n                    }\n                } else {\n                    log::debug!(\"Skipping checkpoint - manual operation in progress\");\n                }\n            }\n\n            log::info!(\"Checkpoint task stopped\");\n        })\n    }\n\n    /// Start the garbage collection task\n    fn start_gc_task(&self) -> tokio::task::JoinHandle<()> {\n        let running = self.running.clone();\n        let gc_state = self.gc_state.clone();\n        let operation_lock = self.operation_lock.clone();\n        let interval_ms = self.config.gc_interval_ms;\n\n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_millis(interval_ms));\n            interval.set_missed_tick_behavior(MissedTickBehavior::Delay);\n\n            log::info!(\"GC task started with interval {interval_ms}ms\");\n\n            while running.load(Ordering::Acquire) {\n                interval.tick().await;\n\n                if !running.load(Ordering::Acquire) {\n                    break;\n                }\n\n                // Check if GC is needed\n                let gc_config = GcConfig::default();\n                match gc_state.should_run_gc(&gc_config) {\n                    Ok(true) => {\n                        // Try to acquire lock for GC\n                        if let Ok(_lock) = operation_lock.try_read() {\n                            match gc_state.initiate_gc(gc_config).await {\n                                Ok(stats) => {\n                                    log::debug!(\n                                        \"Background GC completed, reclaimed {} bytes\",\n                                        stats.bytes_reclaimed\n                                    );\n                                }\n                                Err(e) => {\n                                    log::warn!(\"Background GC failed: {e}\");\n                                }\n                            }\n                        } else {\n                            log::debug!(\"Skipping GC - manual operation in progress\");\n                        }\n                    }\n                    Ok(false) => {\n                        log::trace!(\"GC not needed\");\n                    }\n                    Err(e) => {\n                        log::warn!(\"Failed to check GC requirement: {e}\");\n                    }\n                }\n            }\n\n            log::info!(\"GC task stopped\");\n        })\n    }\n\n    /// Start the log maintenance task\n    fn start_log_maintenance_task(&self) -> tokio::task::JoinHandle<()> {\n        let running = self.running.clone();\n        let hlog = self.hlog.clone();\n        let operation_lock = self.operation_lock.clone();\n\n        tokio::spawn(async move {\n            let mut interval = interval(Duration::from_secs(30)); // Run every 30 seconds\n            interval.set_missed_tick_behavior(MissedTickBehavior::Delay);\n\n            log::info!(\"Log maintenance task started\");\n\n            while running.load(Ordering::Acquire) {\n                interval.tick().await;\n\n                if !running.load(Ordering::Acquire) {\n                    break;\n                }\n\n                // Try to acquire read lock for maintenance\n                if let Ok(_lock) = operation_lock.try_read() {\n                    // Perform log maintenance operations\n                    Self::perform_log_maintenance(&hlog).await;\n                }\n            }\n\n            log::info!(\"Log maintenance task stopped\");\n        })\n    }\n\n    /// Perform log maintenance operations\n    async fn perform_log_maintenance(hlog: &HybridLog) {\n        // Check if we need to advance the read-only address\n        let tail_address = hlog.get_tail_address();\n        let read_only_address = hlog.get_read_only_address();\n        let head_address = hlog.get_head_address();\n\n        // If mutable region is getting large, advance read-only address\n        let mutable_region_size = tail_address.saturating_sub(read_only_address);\n        const MAX_MUTABLE_REGION_SIZE: u64 = 128 * 1024 * 1024; // 128MB\n\n        if mutable_region_size > MAX_MUTABLE_REGION_SIZE {\n            let new_read_only = hlog.shift_read_only_address();\n            log::debug!(\"Advanced read-only address to 0x{new_read_only:x}\");\n\n            // Try to flush the newly read-only data\n            if let Err(e) = hlog.flush_to_disk(new_read_only).await {\n                log::warn!(\"Failed to flush during maintenance: {e}\");\n            }\n        }\n\n        // Check if we need to advance the head address\n        let read_only_region_size = read_only_address.saturating_sub(head_address);\n        const MAX_READ_ONLY_REGION_SIZE: u64 = 256 * 1024 * 1024; // 256MB\n\n        if read_only_region_size > MAX_READ_ONLY_REGION_SIZE {\n            // Move some data from memory to disk-only\n            let new_head = head_address + (read_only_region_size / 2); // Move half\n            if let Err(e) = hlog.shift_head_address(new_head) {\n                log::warn!(\"Failed to shift head address during maintenance: {e}\");\n            } else {\n                log::debug!(\"Advanced head address to 0x{new_head:x}\");\n            }\n        }\n    }\n\n    /// Get statistics about background task performance\n    pub fn get_stats(&self) -> BackgroundTaskStats {\n        BackgroundTaskStats {\n            is_running: self.is_running(),\n            checkpoint_enabled: self.config.enable_checkpointing,\n            gc_enabled: self.config.enable_gc,\n            checkpoint_interval_ms: self.config.checkpoint_interval_ms,\n            gc_interval_ms: self.config.gc_interval_ms,\n            active_task_count: self.task_handles.lock().len(),\n        }\n    }\n}\n\nimpl Drop for BackgroundTaskManager {\n    fn drop(&mut self) {\n        // Stop background tasks when dropping\n        let running = self.running.clone();\n        let handles = {\n            let mut handles = self.task_handles.lock();\n            std::mem::take(&mut *handles)\n        };\n\n        if running.swap(false, Ordering::AcqRel) {\n            // Cancel all tasks\n            for handle in handles {\n                handle.abort();\n            }\n        }\n    }\n}\n\n/// Statistics about background task performance\n#[derive(Debug, Clone)]\npub struct BackgroundTaskStats {\n    /// Whether background tasks are currently running\n    pub is_running: bool,\n    /// Whether checkpointing is enabled\n    pub checkpoint_enabled: bool,\n    /// Whether garbage collection is enabled\n    pub gc_enabled: bool,\n    /// Checkpoint interval in milliseconds\n    pub checkpoint_interval_ms: u64,\n    /// GC interval in milliseconds\n    pub gc_interval_ms: u64,\n    /// Number of active background tasks\n    pub active_task_count: usize,\n}\n\n#[cfg(test)]\nmod tests {\n    use tempfile::tempdir;\n\n    use super::*;\n    use crate::checkpoint::CheckpointState;\n    use crate::epoch::EpochManager;\n    use crate::hlog::FileStorageDevice;\n    use crate::index::new_shared_mem_hash_index;\n\n    async fn create_test_background_manager() -> (BackgroundTaskManager, tempfile::TempDir) {\n        let temp_dir = tempdir().unwrap();\n\n        let config = Config {\n            storage_dir: temp_dir.path().to_string_lossy().to_string(),\n            memory_size: 32 * 1024 * 1024, // 32MB for testing\n            enable_checkpointing: true,\n            checkpoint_interval_ms: 100, // Very short for testing\n            enable_gc: true,\n            gc_interval_ms: 200, // Very short for testing\n            ..Default::default()\n        };\n\n        let epoch = Arc::new(EpochManager::new());\n        let storage = Box::new(FileStorageDevice::new(temp_dir.path().join(\"test.log\")).unwrap());\n        let hlog = Arc::new(HybridLog::new(config.memory_size, storage, epoch.clone()).unwrap());\n        let index = new_shared_mem_hash_index(epoch);\n\n        let checkpoint_dir = temp_dir.path().join(\"checkpoints\");\n        let checkpoint_state =\n            Arc::new(CheckpointState::new(checkpoint_dir, hlog.clone(), index.clone()).unwrap());\n        let gc_state = Arc::new(GcState::new(hlog.clone(), index));\n        let operation_lock = Arc::new(AsyncRwLock::new(()));\n\n        let manager =\n            BackgroundTaskManager::new(config, checkpoint_state, gc_state, hlog, operation_lock);\n\n        (manager, temp_dir)\n    }\n\n    #[tokio::test]\n    async fn test_background_manager_start_stop() {\n        let (manager, _temp_dir) = create_test_background_manager().await;\n\n        assert!(!manager.is_running());\n\n        // Start background tasks\n        manager.start().unwrap();\n        assert!(manager.is_running());\n\n        // Stop background tasks\n        manager.stop().await.unwrap();\n        assert!(!manager.is_running());\n    }\n\n    #[tokio::test]\n    async fn test_background_manager_double_start() {\n        let (manager, _temp_dir) = create_test_background_manager().await;\n\n        // First start should succeed\n        manager.start().unwrap();\n\n        // Second start should fail\n        let result = manager.start();\n        assert!(result.is_err());\n\n        manager.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_background_tasks_run() {\n        let (manager, _temp_dir) = create_test_background_manager().await;\n\n        manager.start().unwrap();\n\n        // Let tasks run for a short time\n        tokio::time::sleep(Duration::from_millis(500)).await;\n\n        // Tasks should still be running\n        assert!(manager.is_running());\n\n        manager.stop().await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_background_manager_stats() {\n        let (manager, _temp_dir) = create_test_background_manager().await;\n\n        let stats_before = manager.get_stats();\n        assert!(!stats_before.is_running);\n        assert_eq!(stats_before.active_task_count, 0);\n\n        manager.start().unwrap();\n\n        let stats_after = manager.get_stats();\n        assert!(stats_after.is_running);\n        assert!(stats_after.checkpoint_enabled);\n        assert!(stats_after.gc_enabled);\n        assert!(stats_after.active_task_count > 0);\n\n        manager.stop().await.unwrap();\n    }\n\n    #[test]\n    fn test_background_manager_drop() {\n        tokio::runtime::Runtime::new().unwrap().block_on(async {\n            let (manager, _temp_dir) = create_test_background_manager().await;\n\n            manager.start().unwrap();\n            assert!(manager.is_running());\n\n            // Drop should stop background tasks\n            drop(manager);\n\n            // Give some time for cleanup\n            tokio::time::sleep(Duration::from_millis(50)).await;\n        });\n    }\n}\n","traces":[{"line":43,"address":[],"length":0,"stats":{"Line":11}},{"line":51,"address":[],"length":0,"stats":{"Line":33}},{"line":57,"address":[],"length":0,"stats":{"Line":11}},{"line":62,"address":[],"length":0,"stats":{"Line":7}},{"line":63,"address":[],"length":0,"stats":{"Line":14}},{"line":64,"address":[],"length":0,"stats":{"Line":14}},{"line":65,"address":[],"length":0,"stats":{"Line":14}},{"line":68,"address":[],"length":0,"stats":{"Line":1}},{"line":69,"address":[],"length":0,"stats":{"Line":1}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":6}},{"line":79,"address":[],"length":0,"stats":{"Line":24}},{"line":80,"address":[],"length":0,"stats":{"Line":12}},{"line":84,"address":[],"length":0,"stats":{"Line":5}},{"line":85,"address":[],"length":0,"stats":{"Line":20}},{"line":86,"address":[],"length":0,"stats":{"Line":10}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":12}},{"line":99,"address":[],"length":0,"stats":{"Line":12}},{"line":100,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":5}},{"line":106,"address":[],"length":0,"stats":{"Line":5}},{"line":107,"address":[],"length":0,"stats":{"Line":15}},{"line":108,"address":[],"length":0,"stats":{"Line":10}},{"line":111,"address":[],"length":0,"stats":{"Line":33}},{"line":112,"address":[],"length":0,"stats":{"Line":28}},{"line":113,"address":[],"length":0,"stats":{"Line":28}},{"line":116,"address":[],"length":0,"stats":{"Line":5}},{"line":121,"address":[],"length":0,"stats":{"Line":7}},{"line":122,"address":[],"length":0,"stats":{"Line":14}},{"line":126,"address":[],"length":0,"stats":{"Line":6}},{"line":127,"address":[],"length":0,"stats":{"Line":18}},{"line":128,"address":[],"length":0,"stats":{"Line":18}},{"line":129,"address":[],"length":0,"stats":{"Line":18}},{"line":130,"address":[],"length":0,"stats":{"Line":12}},{"line":132,"address":[],"length":0,"stats":{"Line":7}},{"line":133,"address":[],"length":0,"stats":{"Line":4}},{"line":134,"address":[],"length":0,"stats":{"Line":3}},{"line":136,"address":[],"length":0,"stats":{"Line":1}},{"line":138,"address":[],"length":0,"stats":{"Line":12}},{"line":139,"address":[],"length":0,"stats":{"Line":12}},{"line":141,"address":[],"length":0,"stats":{"Line":10}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":10}},{"line":148,"address":[],"length":0,"stats":{"Line":5}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":5}},{"line":169,"address":[],"length":0,"stats":{"Line":15}},{"line":170,"address":[],"length":0,"stats":{"Line":15}},{"line":171,"address":[],"length":0,"stats":{"Line":15}},{"line":172,"address":[],"length":0,"stats":{"Line":10}},{"line":174,"address":[],"length":0,"stats":{"Line":9}},{"line":175,"address":[],"length":0,"stats":{"Line":16}},{"line":176,"address":[],"length":0,"stats":{"Line":12}},{"line":178,"address":[],"length":0,"stats":{"Line":4}},{"line":180,"address":[],"length":0,"stats":{"Line":14}},{"line":181,"address":[],"length":0,"stats":{"Line":8}},{"line":183,"address":[],"length":0,"stats":{"Line":6}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":6}},{"line":189,"address":[],"length":0,"stats":{"Line":6}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":3}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":3}},{"line":222,"address":[],"length":0,"stats":{"Line":6}},{"line":223,"address":[],"length":0,"stats":{"Line":18}},{"line":224,"address":[],"length":0,"stats":{"Line":18}},{"line":225,"address":[],"length":0,"stats":{"Line":18}},{"line":227,"address":[],"length":0,"stats":{"Line":11}},{"line":228,"address":[],"length":0,"stats":{"Line":15}},{"line":229,"address":[],"length":0,"stats":{"Line":15}},{"line":231,"address":[],"length":0,"stats":{"Line":5}},{"line":233,"address":[],"length":0,"stats":{"Line":12}},{"line":234,"address":[],"length":0,"stats":{"Line":4}},{"line":236,"address":[],"length":0,"stats":{"Line":2}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":1}},{"line":247,"address":[],"length":0,"stats":{"Line":4}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":288,"address":[],"length":0,"stats":{"Line":2}},{"line":290,"address":[],"length":0,"stats":{"Line":6}},{"line":291,"address":[],"length":0,"stats":{"Line":4}},{"line":292,"address":[],"length":0,"stats":{"Line":4}},{"line":293,"address":[],"length":0,"stats":{"Line":4}},{"line":294,"address":[],"length":0,"stats":{"Line":4}},{"line":295,"address":[],"length":0,"stats":{"Line":2}},{"line":301,"address":[],"length":0,"stats":{"Line":11}},{"line":303,"address":[],"length":0,"stats":{"Line":33}},{"line":304,"address":[],"length":0,"stats":{"Line":11}},{"line":305,"address":[],"length":0,"stats":{"Line":33}},{"line":306,"address":[],"length":0,"stats":{"Line":22}},{"line":309,"address":[],"length":0,"stats":{"Line":22}},{"line":311,"address":[],"length":0,"stats":{"Line":7}}],"covered":85,"coverable":118},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","checkpoint.rs"],"content":"//! Checkpoint and recovery implementation for rskv\n//!\n//! This module implements non-blocking checkpointing inspired by FASTER's design.\n//! It provides consistent snapshots of the entire database state without pausing operations.\n\nuse std::path::PathBuf;\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicBool, AtomicU64, Ordering};\n\nuse serde::{Deserialize, Serialize};\nuse tokio::fs as async_fs;\nuse tokio::time::Instant;\n\nuse crate::common::{Address, Key, Result, RsKvError};\nuse crate::hlog::HybridLog;\nuse crate::index::SharedMemHashIndex;\n\n/// Metadata for a checkpoint containing all necessary information for recovery\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CheckpointMetadata {\n    /// Unique checkpoint ID\n    pub checkpoint_id: u64,\n    /// Timestamp when checkpoint was initiated\n    pub timestamp: u64,\n    /// Log addresses at checkpoint time\n    pub log_metadata: LogMetadata,\n    /// Index snapshot information\n    pub index_metadata: IndexMetadata,\n    /// Version of the checkpoint format\n    pub format_version: u32,\n    /// Size of the checkpoint in bytes\n    pub total_size: u64,\n}\n\n/// Log-specific metadata in a checkpoint\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LogMetadata {\n    /// Begin address of the log\n    pub begin_address: Address,\n    /// Head address at checkpoint time\n    pub head_address: Address,\n    /// Read-only address at checkpoint time\n    pub read_only_address: Address,\n    /// Tail address at checkpoint time\n    pub tail_address: Address,\n    /// Address up to which data has been flushed\n    pub flushed_until_address: Address,\n}\n\n/// Index-specific metadata in a checkpoint\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct IndexMetadata {\n    /// Number of entries in the index\n    pub entry_count: usize,\n    /// Total size of keys in bytes\n    pub total_key_size: usize,\n    /// Size of the index snapshot file\n    pub snapshot_file_size: u64,\n    /// Hash of the index snapshot for integrity checking\n    pub snapshot_hash: u64,\n}\n\n/// State machine for checkpoint operations\npub struct CheckpointState {\n    /// Unique ID for this checkpoint\n    checkpoint_id: AtomicU64,\n\n    /// Whether a checkpoint is currently in progress\n    in_progress: AtomicBool,\n\n    /// Directory where checkpoints are stored\n    checkpoint_dir: PathBuf,\n\n    /// Reference to the hybrid log\n    hlog: Arc<HybridLog>,\n\n    /// Reference to the hash index\n    index: SharedMemHashIndex,\n\n    /// Start time of current checkpoint\n    start_time: parking_lot::Mutex<Option<Instant>>,\n}\n\nimpl CheckpointState {\n    /// Create a new checkpoint state manager\n    pub fn new(\n        checkpoint_dir: PathBuf,\n        hlog: Arc<HybridLog>,\n        index: SharedMemHashIndex,\n    ) -> Result<Self> {\n        // Ensure checkpoint directory exists\n        std::fs::create_dir_all(&checkpoint_dir)?;\n\n        Ok(Self {\n            checkpoint_id: AtomicU64::new(1),\n            in_progress: AtomicBool::new(false),\n            checkpoint_dir,\n            hlog,\n            index,\n            start_time: parking_lot::Mutex::new(None),\n        })\n    }\n\n    /// Check if a checkpoint is currently in progress\n    pub fn is_in_progress(&self) -> bool {\n        self.in_progress.load(Ordering::Acquire)\n    }\n\n    /// Initiate a new checkpoint operation\n    pub async fn initiate_checkpoint(&self) -> Result<CheckpointMetadata> {\n        // Check if checkpoint is already in progress\n        if self\n            .in_progress\n            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n            .is_err()\n        {\n            return Err(RsKvError::CheckpointFailed {\n                message: \"Checkpoint already in progress\".to_string(),\n            });\n        }\n\n        let checkpoint_id = self.checkpoint_id.fetch_add(1, Ordering::AcqRel);\n        let start_time = Instant::now();\n        *self.start_time.lock() = Some(start_time);\n\n        log::info!(\"Initiating checkpoint {checkpoint_id}\");\n\n        // Phase 1: Capture current log state and shift read-only address\n        let tail_address_before = self.hlog.get_tail_address();\n        let checkpoint_address = self.hlog.shift_read_only_address();\n\n        log::debug!(\n            \"Checkpoint {} shifted read-only to address 0x{:x}\",\n            checkpoint_id,\n            checkpoint_address\n        );\n\n        // Phase 2: Create log metadata\n        let log_metadata = LogMetadata {\n            begin_address: self.hlog.get_begin_address(),\n            head_address: self.hlog.get_head_address(),\n            read_only_address: checkpoint_address,\n            tail_address: tail_address_before,\n            flushed_until_address: checkpoint_address, // Will be updated after flush\n        };\n\n        // Phase 3: Create index snapshot\n        let index_snapshot = self.create_index_snapshot(checkpoint_id).await?;\n        let index_metadata = IndexMetadata {\n            entry_count: index_snapshot.len(),\n            total_key_size: index_snapshot.iter().map(|(k, _)| k.len()).sum(),\n            snapshot_file_size: 0, // Will be updated after writing\n            snapshot_hash: self.calculate_snapshot_hash(&index_snapshot),\n        };\n\n        // Phase 4: Flush log data to disk\n        self.hlog.flush_to_disk(checkpoint_address).await?;\n\n        // Phase 5: Write checkpoint files\n        let metadata = CheckpointMetadata {\n            checkpoint_id,\n            timestamp: start_time.elapsed().as_millis() as u64,\n            log_metadata,\n            index_metadata,\n            format_version: 1,\n            total_size: 0, // Will be calculated\n        };\n\n        self.write_checkpoint_files(checkpoint_id, &metadata, index_snapshot)\n            .await?;\n\n        log::info!(\n            \"Checkpoint {} completed in {:?}\",\n            checkpoint_id,\n            start_time.elapsed()\n        );\n\n        // Mark checkpoint as complete\n        self.in_progress.store(false, Ordering::Release);\n\n        Ok(metadata)\n    }\n\n    /// Create a snapshot of the current index state\n    async fn create_index_snapshot(&self, checkpoint_id: u64) -> Result<Vec<(Key, Address)>> {\n        log::debug!(\"Creating index snapshot for checkpoint {checkpoint_id}\");\n\n        let snapshot = self.index.snapshot();\n\n        log::debug!(\"Index snapshot created with {} entries\", snapshot.len());\n        Ok(snapshot)\n    }\n\n    /// Calculate hash of index snapshot for integrity checking\n    fn calculate_snapshot_hash(&self, snapshot: &[(Key, Address)]) -> u64 {\n        use std::collections::hash_map::DefaultHasher;\n        use std::hash::{Hash, Hasher};\n\n        let mut hasher = DefaultHasher::new();\n\n        // Sort snapshot by key for deterministic hashing\n        let mut sorted_snapshot = snapshot.to_vec();\n        sorted_snapshot.sort_by(|a, b| a.0.cmp(&b.0));\n\n        for (key, address) in sorted_snapshot {\n            key.hash(&mut hasher);\n            address.hash(&mut hasher);\n        }\n\n        hasher.finish()\n    }\n\n    /// Write checkpoint files to disk\n    async fn write_checkpoint_files(\n        &self,\n        checkpoint_id: u64,\n        metadata: &CheckpointMetadata,\n        index_snapshot: Vec<(Key, Address)>,\n    ) -> Result<()> {\n        let checkpoint_prefix = self\n            .checkpoint_dir\n            .join(format!(\"checkpoint_{checkpoint_id}\"));\n\n        // Write index snapshot\n        let index_file_path = format!(\"{}.index\", checkpoint_prefix.to_string_lossy());\n        self.write_index_snapshot(&index_file_path, index_snapshot)\n            .await?;\n\n        // Write metadata\n        let metadata_file_path = format!(\"{}.meta\", checkpoint_prefix.to_string_lossy());\n        self.write_metadata(&metadata_file_path, metadata).await?;\n\n        log::info!(\n            \"Checkpoint {} files written to {}\",\n            checkpoint_id,\n            checkpoint_prefix.to_string_lossy()\n        );\n\n        Ok(())\n    }\n\n    /// Write index snapshot to file\n    async fn write_index_snapshot(\n        &self,\n        file_path: &str,\n        snapshot: Vec<(Key, Address)>,\n    ) -> Result<()> {\n        let data = bincode::serialize(&snapshot)?;\n        async_fs::write(file_path, data).await?;\n\n        log::debug!(\"Index snapshot written to {file_path}\");\n        Ok(())\n    }\n\n    /// Write checkpoint metadata to file\n    async fn write_metadata(&self, file_path: &str, metadata: &CheckpointMetadata) -> Result<()> {\n        let data = bincode::serialize(metadata)?;\n        async_fs::write(file_path, data).await?;\n\n        log::debug!(\"Checkpoint metadata written to {file_path}\");\n        Ok(())\n    }\n\n    /// Recover from the latest checkpoint\n    pub async fn recover_from_latest_checkpoint(&self) -> Result<Option<CheckpointMetadata>> {\n        let latest_checkpoint = self.find_latest_checkpoint().await?;\n\n        if let Some(checkpoint_id) = latest_checkpoint {\n            log::info!(\"Recovering from checkpoint {checkpoint_id}\");\n            let metadata = self.load_checkpoint(checkpoint_id).await?;\n            Ok(Some(metadata))\n        } else {\n            log::info!(\"No checkpoint found, starting fresh\");\n            Ok(None)\n        }\n    }\n\n    /// Find the latest checkpoint ID\n    async fn find_latest_checkpoint(&self) -> Result<Option<u64>> {\n        let mut entries = async_fs::read_dir(&self.checkpoint_dir).await?;\n        let mut latest_id = None;\n\n        while let Some(entry) = entries.next_entry().await? {\n            let file_name = entry.file_name();\n            let file_str = file_name.to_string_lossy();\n\n            if file_str.starts_with(\"checkpoint_\")\n                && file_str.ends_with(\".meta\")\n                && let Some(id_str) = file_str\n                    .strip_prefix(\"checkpoint_\")\n                    .and_then(|s| s.strip_suffix(\".meta\"))\n                && let Ok(id) = id_str.parse::<u64>()\n            {\n                latest_id = Some(latest_id.unwrap_or(0).max(id));\n            }\n        }\n\n        Ok(latest_id)\n    }\n\n    /// Load a specific checkpoint\n    async fn load_checkpoint(&self, checkpoint_id: u64) -> Result<CheckpointMetadata> {\n        let checkpoint_prefix = self\n            .checkpoint_dir\n            .join(format!(\"checkpoint_{checkpoint_id}\"));\n\n        // Load metadata\n        let metadata_file_path = format!(\"{}.meta\", checkpoint_prefix.to_string_lossy());\n        let metadata_data = async_fs::read(&metadata_file_path).await?;\n        let metadata: CheckpointMetadata = bincode::deserialize(&metadata_data)?;\n\n        // Load and restore index snapshot\n        let index_file_path = format!(\"{}.index\", checkpoint_prefix.to_string_lossy());\n        let index_data = async_fs::read(&index_file_path).await?;\n        let index_snapshot: Vec<(Key, Address)> = bincode::deserialize(&index_data)?;\n\n        // Verify snapshot integrity\n        let calculated_hash = self.calculate_snapshot_hash(&index_snapshot);\n        if calculated_hash != metadata.index_metadata.snapshot_hash {\n            return Err(RsKvError::CheckpointFailed {\n                message: format!(\n                    \"Index snapshot hash mismatch: expected {}, got {}\",\n                    metadata.index_metadata.snapshot_hash, calculated_hash\n                ),\n            });\n        }\n\n        // Restore index from snapshot\n        self.index.restore_from_snapshot(index_snapshot);\n\n        log::info!(\"Checkpoint {checkpoint_id} loaded successfully\");\n        Ok(metadata)\n    }\n\n    /// List all available checkpoints\n    pub async fn list_checkpoints(&self) -> Result<Vec<u64>> {\n        let mut entries = async_fs::read_dir(&self.checkpoint_dir).await?;\n        let mut checkpoint_ids = Vec::new();\n\n        while let Some(entry) = entries.next_entry().await? {\n            let file_name = entry.file_name();\n            let file_str = file_name.to_string_lossy();\n\n            if file_str.starts_with(\"checkpoint_\")\n                && file_str.ends_with(\".meta\")\n                && let Some(id_str) = file_str\n                    .strip_prefix(\"checkpoint_\")\n                    .and_then(|s| s.strip_suffix(\".meta\"))\n                && let Ok(id) = id_str.parse::<u64>()\n            {\n                checkpoint_ids.push(id);\n            }\n        }\n\n        checkpoint_ids.sort();\n        Ok(checkpoint_ids)\n    }\n\n    /// Delete old checkpoints, keeping only the specified number\n    pub async fn cleanup_old_checkpoints(&self, keep_count: usize) -> Result<()> {\n        let mut checkpoint_ids = self.list_checkpoints().await?;\n        checkpoint_ids.sort();\n\n        if checkpoint_ids.len() <= keep_count {\n            return Ok(()); // Nothing to cleanup\n        }\n\n        let to_delete = &checkpoint_ids[..checkpoint_ids.len() - keep_count];\n\n        for &checkpoint_id in to_delete {\n            self.delete_checkpoint(checkpoint_id).await?;\n        }\n\n        log::info!(\"Cleaned up {} old checkpoints\", to_delete.len());\n        Ok(())\n    }\n\n    /// Delete a specific checkpoint\n    async fn delete_checkpoint(&self, checkpoint_id: u64) -> Result<()> {\n        let checkpoint_prefix = self\n            .checkpoint_dir\n            .join(format!(\"checkpoint_{checkpoint_id}\"));\n\n        let metadata_file = format!(\"{}.meta\", checkpoint_prefix.to_string_lossy());\n        let index_file = format!(\"{}.index\", checkpoint_prefix.to_string_lossy());\n\n        if async_fs::metadata(&metadata_file).await.is_ok() {\n            async_fs::remove_file(&metadata_file).await?;\n        }\n\n        if async_fs::metadata(&index_file).await.is_ok() {\n            async_fs::remove_file(&index_file).await?;\n        }\n\n        log::debug!(\"Deleted checkpoint {checkpoint_id}\");\n        Ok(())\n    }\n\n    /// Get checkpoint statistics\n    pub async fn get_checkpoint_stats(&self) -> Result<CheckpointStats> {\n        let checkpoint_ids = self.list_checkpoints().await?;\n        let total_count = checkpoint_ids.len();\n\n        let mut total_size = 0u64;\n        for &checkpoint_id in &checkpoint_ids {\n            let checkpoint_prefix = self\n                .checkpoint_dir\n                .join(format!(\"checkpoint_{checkpoint_id}\"));\n\n            let metadata_file = format!(\"{}.meta\", checkpoint_prefix.to_string_lossy());\n            let index_file = format!(\"{}.index\", checkpoint_prefix.to_string_lossy());\n\n            if let Ok(meta) = async_fs::metadata(&metadata_file).await {\n                total_size += meta.len();\n            }\n            if let Ok(meta) = async_fs::metadata(&index_file).await {\n                total_size += meta.len();\n            }\n        }\n\n        Ok(CheckpointStats {\n            total_checkpoints: total_count,\n            total_size_bytes: total_size,\n            latest_checkpoint_id: checkpoint_ids.last().copied(),\n            in_progress: self.is_in_progress(),\n        })\n    }\n}\n\n/// Statistics about checkpoints\n#[derive(Debug, Clone)]\npub struct CheckpointStats {\n    /// Total number of checkpoints\n    pub total_checkpoints: usize,\n    /// Total size of all checkpoints in bytes\n    pub total_size_bytes: u64,\n    /// ID of the latest checkpoint\n    pub latest_checkpoint_id: Option<u64>,\n    /// Whether a checkpoint is currently in progress\n    pub in_progress: bool,\n}\n\n#[cfg(test)]\nmod tests {\n    use tempfile::tempdir;\n\n    use super::*;\n    use crate::epoch::EpochManager;\n    use crate::hlog::FileStorageDevice;\n    use crate::index::new_shared_mem_hash_index;\n\n    async fn create_test_checkpoint_state() -> (CheckpointState, tempfile::TempDir) {\n        let temp_dir = tempdir().unwrap();\n        let checkpoint_dir = temp_dir.path().join(\"checkpoints\");\n\n        let epoch = Arc::new(EpochManager::new());\n        let storage = Box::new(FileStorageDevice::new(temp_dir.path().join(\"test.log\")).unwrap());\n        let hlog = Arc::new(HybridLog::new(64 * 1024 * 1024, storage, epoch.clone()).unwrap());\n        let index = new_shared_mem_hash_index(epoch);\n\n        let checkpoint_state = CheckpointState::new(checkpoint_dir, hlog, index).unwrap();\n        (checkpoint_state, temp_dir)\n    }\n\n    #[tokio::test]\n    async fn test_checkpoint_creation() {\n        let (checkpoint_state, _temp_dir) = create_test_checkpoint_state().await;\n\n        // Add some data to index\n        checkpoint_state.index.insert(b\"key1\".to_vec(), 100);\n        checkpoint_state.index.insert(b\"key2\".to_vec(), 200);\n\n        // Create checkpoint\n        let metadata = checkpoint_state.initiate_checkpoint().await.unwrap();\n\n        assert_eq!(metadata.checkpoint_id, 1);\n        assert_eq!(metadata.index_metadata.entry_count, 2);\n        assert!(!checkpoint_state.is_in_progress());\n    }\n\n    #[tokio::test]\n    async fn test_checkpoint_recovery() {\n        let (checkpoint_state, _temp_dir) = create_test_checkpoint_state().await;\n\n        // Add data and create checkpoint\n        checkpoint_state.index.insert(b\"key1\".to_vec(), 100);\n        checkpoint_state.index.insert(b\"key2\".to_vec(), 200);\n\n        let _metadata = checkpoint_state.initiate_checkpoint().await.unwrap();\n\n        // Clear index\n        checkpoint_state.index.clear();\n        assert_eq!(checkpoint_state.index.len(), 0);\n\n        // Recover from checkpoint\n        let recovered_metadata = checkpoint_state\n            .recover_from_latest_checkpoint()\n            .await\n            .unwrap();\n\n        assert!(recovered_metadata.is_some());\n        assert_eq!(checkpoint_state.index.len(), 2);\n        assert_eq!(checkpoint_state.index.find(&b\"key1\".to_vec()), Some(100));\n        assert_eq!(checkpoint_state.index.find(&b\"key2\".to_vec()), Some(200));\n    }\n\n    #[tokio::test]\n    async fn test_checkpoint_cleanup() {\n        let (checkpoint_state, _temp_dir) = create_test_checkpoint_state().await;\n\n        // Create multiple checkpoints\n        for i in 0..5 {\n            checkpoint_state\n                .index\n                .insert(format!(\"key{}\", i).into_bytes(), i as u64);\n            checkpoint_state.initiate_checkpoint().await.unwrap();\n        }\n\n        let checkpoints_before = checkpoint_state.list_checkpoints().await.unwrap();\n        assert_eq!(checkpoints_before.len(), 5);\n\n        // Cleanup, keeping only 2\n        checkpoint_state.cleanup_old_checkpoints(2).await.unwrap();\n\n        let checkpoints_after = checkpoint_state.list_checkpoints().await.unwrap();\n        assert_eq!(checkpoints_after.len(), 2);\n        assert_eq!(checkpoints_after, vec![4, 5]); // Should keep the latest 2\n    }\n\n    #[tokio::test]\n    async fn test_checkpoint_stats() {\n        let (checkpoint_state, _temp_dir) = create_test_checkpoint_state().await;\n\n        let stats_before = checkpoint_state.get_checkpoint_stats().await.unwrap();\n        assert_eq!(stats_before.total_checkpoints, 0);\n\n        // Create a checkpoint\n        checkpoint_state.index.insert(b\"key1\".to_vec(), 100);\n        checkpoint_state.initiate_checkpoint().await.unwrap();\n\n        let stats_after = checkpoint_state.get_checkpoint_stats().await.unwrap();\n        assert_eq!(stats_after.total_checkpoints, 1);\n        assert_eq!(stats_after.latest_checkpoint_id, Some(1));\n        assert!(stats_after.total_size_bytes > 0);\n    }\n}\n","traces":[{"line":86,"address":[],"length":0,"stats":{"Line":15}},{"line":92,"address":[],"length":0,"stats":{"Line":30}},{"line":94,"address":[],"length":0,"stats":{"Line":15}},{"line":95,"address":[],"length":0,"stats":{"Line":15}},{"line":96,"address":[],"length":0,"stats":{"Line":15}},{"line":97,"address":[],"length":0,"stats":{"Line":15}},{"line":98,"address":[],"length":0,"stats":{"Line":15}},{"line":99,"address":[],"length":0,"stats":{"Line":15}},{"line":100,"address":[],"length":0,"stats":{"Line":15}},{"line":105,"address":[],"length":0,"stats":{"Line":3}},{"line":106,"address":[],"length":0,"stats":{"Line":9}},{"line":110,"address":[],"length":0,"stats":{"Line":30}},{"line":112,"address":[],"length":0,"stats":{"Line":30}},{"line":113,"address":[],"length":0,"stats":{"Line":30}},{"line":114,"address":[],"length":0,"stats":{"Line":30}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":15}},{"line":151,"address":[],"length":0,"stats":{"Line":44}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":15}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":30}},{"line":186,"address":[],"length":0,"stats":{"Line":15}},{"line":188,"address":[],"length":0,"stats":{"Line":30}},{"line":190,"address":[],"length":0,"stats":{"Line":15}},{"line":191,"address":[],"length":0,"stats":{"Line":15}},{"line":195,"address":[],"length":0,"stats":{"Line":16}},{"line":199,"address":[],"length":0,"stats":{"Line":32}},{"line":202,"address":[],"length":0,"stats":{"Line":48}},{"line":203,"address":[],"length":0,"stats":{"Line":92}},{"line":205,"address":[],"length":0,"stats":{"Line":64}},{"line":210,"address":[],"length":0,"stats":{"Line":32}},{"line":214,"address":[],"length":0,"stats":{"Line":15}},{"line":220,"address":[],"length":0,"stats":{"Line":45}},{"line":221,"address":[],"length":0,"stats":{"Line":30}},{"line":222,"address":[],"length":0,"stats":{"Line":30}},{"line":225,"address":[],"length":0,"stats":{"Line":60}},{"line":226,"address":[],"length":0,"stats":{"Line":60}},{"line":227,"address":[],"length":0,"stats":{"Line":15}},{"line":230,"address":[],"length":0,"stats":{"Line":15}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":15}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":15}},{"line":248,"address":[],"length":0,"stats":{"Line":45}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":15}},{"line":256,"address":[],"length":0,"stats":{"Line":30}},{"line":257,"address":[],"length":0,"stats":{"Line":45}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[],"length":0,"stats":{"Line":15}},{"line":265,"address":[],"length":0,"stats":{"Line":14}},{"line":266,"address":[],"length":0,"stats":{"Line":21}},{"line":268,"address":[],"length":0,"stats":{"Line":1}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":1}},{"line":273,"address":[],"length":0,"stats":{"Line":6}},{"line":274,"address":[],"length":0,"stats":{"Line":6}},{"line":279,"address":[],"length":0,"stats":{"Line":14}},{"line":280,"address":[],"length":0,"stats":{"Line":21}},{"line":283,"address":[],"length":0,"stats":{"Line":20}},{"line":288,"address":[],"length":0,"stats":{"Line":2}},{"line":289,"address":[],"length":0,"stats":{"Line":2}},{"line":290,"address":[],"length":0,"stats":{"Line":1}},{"line":291,"address":[],"length":0,"stats":{"Line":3}},{"line":292,"address":[],"length":0,"stats":{"Line":2}},{"line":298,"address":[],"length":0,"stats":{"Line":7}},{"line":302,"address":[],"length":0,"stats":{"Line":2}},{"line":303,"address":[],"length":0,"stats":{"Line":3}},{"line":304,"address":[],"length":0,"stats":{"Line":2}},{"line":305,"address":[],"length":0,"stats":{"Line":2}},{"line":308,"address":[],"length":0,"stats":{"Line":4}},{"line":309,"address":[],"length":0,"stats":{"Line":3}},{"line":310,"address":[],"length":0,"stats":{"Line":1}},{"line":314,"address":[],"length":0,"stats":{"Line":1}},{"line":315,"address":[],"length":0,"stats":{"Line":1}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":12}},{"line":337,"address":[],"length":0,"stats":{"Line":18}},{"line":340,"address":[],"length":0,"stats":{"Line":102}},{"line":345,"address":[],"length":0,"stats":{"Line":30}},{"line":346,"address":[],"length":0,"stats":{"Line":30}},{"line":347,"address":[],"length":0,"stats":{"Line":15}},{"line":348,"address":[],"length":0,"stats":{"Line":45}},{"line":349,"address":[],"length":0,"stats":{"Line":30}},{"line":355,"address":[],"length":0,"stats":{"Line":6}},{"line":360,"address":[],"length":0,"stats":{"Line":4}},{"line":361,"address":[],"length":0,"stats":{"Line":6}},{"line":365,"address":[],"length":0,"stats":{"Line":1}},{"line":370,"address":[],"length":0,"stats":{"Line":7}},{"line":371,"address":[],"length":0,"stats":{"Line":9}},{"line":374,"address":[],"length":0,"stats":{"Line":1}},{"line":379,"address":[],"length":0,"stats":{"Line":6}},{"line":380,"address":[],"length":0,"stats":{"Line":9}},{"line":381,"address":[],"length":0,"stats":{"Line":6}},{"line":382,"address":[],"length":0,"stats":{"Line":6}},{"line":384,"address":[],"length":0,"stats":{"Line":12}},{"line":385,"address":[],"length":0,"stats":{"Line":12}},{"line":387,"address":[],"length":0,"stats":{"Line":12}},{"line":388,"address":[],"length":0,"stats":{"Line":6}},{"line":391,"address":[],"length":0,"stats":{"Line":6}},{"line":392,"address":[],"length":0,"stats":{"Line":6}},{"line":395,"address":[],"length":0,"stats":{"Line":3}},{"line":400,"address":[],"length":0,"stats":{"Line":4}},{"line":401,"address":[],"length":0,"stats":{"Line":6}},{"line":405,"address":[],"length":0,"stats":{"Line":4}},{"line":406,"address":[],"length":0,"stats":{"Line":3}},{"line":407,"address":[],"length":0,"stats":{"Line":2}},{"line":408,"address":[],"length":0,"stats":{"Line":2}},{"line":410,"address":[],"length":0,"stats":{"Line":4}},{"line":411,"address":[],"length":0,"stats":{"Line":4}},{"line":413,"address":[],"length":0,"stats":{"Line":4}},{"line":416,"address":[],"length":0,"stats":{"Line":4}},{"line":421,"address":[],"length":0,"stats":{"Line":2}},{"line":422,"address":[],"length":0,"stats":{"Line":2}},{"line":423,"address":[],"length":0,"stats":{"Line":2}},{"line":424,"address":[],"length":0,"stats":{"Line":2}},{"line":425,"address":[],"length":0,"stats":{"Line":2}}],"covered":109,"coverable":128},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","common.rs"],"content":"//! Common types and error definitions for rskv\n//!\n//! This module contains core data types and error handling used throughout the system.\n//! Inspired by FASTER's address.h and common error handling patterns.\n\nuse serde::{Deserialize, Serialize};\nuse thiserror::Error;\n\n/// Synchronization mode for durability vs performance trade-off\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\npub enum SyncMode {\n    /// No explicit sync - rely on OS page cache (fastest, least durable)\n    None,\n    /// Sync data to disk periodically (balanced)\n    Periodic,\n    /// Sync data after every write (slowest, most durable)\n    Always,\n    /// Sync only metadata, data can be cached (compromise)\n    MetadataOnly,\n}\n\n/// Address type representing logical addresses in the hybrid log.\n/// Follows FASTER's Address design with 48-bit addressing:\n/// - 25 bits for offset within page (32MB page size)\n/// - 23 bits for page index (supports ~8M pages)\n/// - 16 bits reserved for hash table control bits\npub type Address = u64;\n\n/// Key type for the key-value store.\n/// Using Vec<u8> for maximum flexibility with different key types.\npub type Key = Vec<u8>;\n\n/// Value type for the key-value store.\n/// Using Vec<u8> for maximum flexibility with different value types.\npub type Value = Vec<u8>;\n\n/// Page size constant - 32MB pages like FASTER\npub const PAGE_SIZE: u32 = 32 * 1024 * 1024; // 32MB\n\n/// Address bit layout constants (matching FASTER's design)\npub const ADDRESS_BITS: u64 = 48;\npub const OFFSET_BITS: u64 = 25;\npub const PAGE_BITS: u64 = ADDRESS_BITS - OFFSET_BITS; // 23 bits\npub const MAX_OFFSET: u32 = (1u32 << OFFSET_BITS) - 1;\npub const MAX_PAGE: u32 = (1u32 << PAGE_BITS) - 1;\npub const INVALID_ADDRESS: Address = 1; // Matches FASTER's kInvalidAddress\n\n/// Address utility functions\n#[inline]\npub fn get_page(address: Address) -> u32 {\n    ((address >> OFFSET_BITS) & ((1u64 << PAGE_BITS) - 1)) as u32\n}\n\n#[inline]\npub fn get_offset(address: Address) -> u32 {\n    (address & ((1u64 << OFFSET_BITS) - 1)) as u32\n}\n\n#[inline]\npub fn make_address(page: u32, offset: u32) -> Address {\n    ((page as u64) << OFFSET_BITS) | (offset as u64)\n}\n\n/// Error types for rskv operations\n#[derive(Error, Debug)]\npub enum RsKvError {\n    #[error(\"IO Error: {0}\")]\n    Io(#[from] std::io::Error),\n\n    #[error(\"Serialization Error: {0}\")]\n    Serialization(#[from] bincode::Error),\n\n    /// Key not found in the store\n    #[error(\"Key not found\")]\n    KeyNotFound,\n\n    #[error(\"Address out of bounds: {address}\")]\n    AddressOutOfBounds { address: Address },\n\n    #[error(\"Page not found: {page}\")]\n    PageNotFound { page: u32 },\n\n    #[error(\"Allocation failed: size {size}\")]\n    AllocationFailed { size: u32 },\n\n    #[error(\"Checkpoint operation failed: {message}\")]\n    CheckpointFailed { message: String },\n\n    #[error(\"Recovery operation failed: {message}\")]\n    RecoveryFailed { message: String },\n\n    #[error(\"Garbage collection failed: {message}\")]\n    GarbageCollectionFailed { message: String },\n\n    #[error(\"Configuration error: {message}\")]\n    Configuration { message: String },\n\n    /// Invalid configuration\n    #[error(\"Invalid configuration: {message}\")]\n    InvalidConfig { message: String },\n\n    /// Key is too large\n    #[error(\"Key size {size} bytes exceeds maximum allowed size {max_size} bytes\")]\n    KeyTooLarge { size: usize, max_size: usize },\n\n    /// Value is too large  \n    #[error(\"Value size {size} bytes exceeds maximum allowed size {max_size} bytes\")]\n    ValueTooLarge { size: usize, max_size: usize },\n\n    /// Storage device error\n    #[error(\"Storage device error: {message}\")]\n    StorageError { message: String },\n\n    /// Memory mapping error\n    #[error(\"Memory mapping error: {message}\")]\n    MmapError { message: String },\n\n    /// Data corruption detected\n    #[error(\"Data corruption detected: {message}\")]\n    Corruption { message: String },\n\n    /// Resource exhausted\n    #[error(\"Resource exhausted: {resource}\")]\n    ResourceExhausted { resource: String },\n\n    /// Operation timeout\n    #[error(\"Operation timed out after {duration_ms} ms\")]\n    Timeout { duration_ms: u64 },\n\n    /// Concurrent operation conflict\n    #[error(\"Concurrent operation conflict: {message}\")]\n    Conflict { message: String },\n\n    #[error(\"Internal error: {message}\")]\n    Internal { message: String },\n}\n\nimpl RsKvError {\n    /// Check if this error is recoverable\n    pub fn is_recoverable(&self) -> bool {\n        matches!(\n            self,\n            RsKvError::Io(_)\n                | RsKvError::Timeout { .. }\n                | RsKvError::Conflict { .. }\n                | RsKvError::ResourceExhausted { .. }\n                | RsKvError::StorageError { .. }\n                | RsKvError::MmapError { .. }\n        )\n    }\n\n    /// Check if this error indicates data corruption\n    pub fn is_corruption(&self) -> bool {\n        matches!(self, RsKvError::Corruption { .. })\n    }\n\n    /// Check if this error is a user input error\n    pub fn is_user_error(&self) -> bool {\n        matches!(\n            self,\n            RsKvError::KeyNotFound\n                | RsKvError::KeyTooLarge { .. }\n                | RsKvError::ValueTooLarge { .. }\n                | RsKvError::InvalidConfig { .. }\n                | RsKvError::Configuration { .. }\n        )\n    }\n\n    /// Get error category for logging and metrics\n    pub fn category(&self) -> &'static str {\n        match self {\n            RsKvError::Io(_) => \"io\",\n            RsKvError::Serialization(_) => \"serialization\",\n            RsKvError::AddressOutOfBounds { .. } => \"addressing\",\n            RsKvError::PageNotFound { .. } => \"addressing\",\n            RsKvError::AllocationFailed { .. } => \"allocation\",\n            RsKvError::KeyNotFound => \"not_found\",\n            RsKvError::KeyTooLarge { .. } | RsKvError::ValueTooLarge { .. } => \"size_limit\",\n            RsKvError::CheckpointFailed { .. } => \"checkpoint\",\n            RsKvError::RecoveryFailed { .. } => \"recovery\",\n            RsKvError::GarbageCollectionFailed { .. } => \"garbage_collection\",\n            RsKvError::Configuration { .. } | RsKvError::InvalidConfig { .. } => \"configuration\",\n            RsKvError::StorageError { .. } => \"storage\",\n            RsKvError::MmapError { .. } => \"memory_mapping\",\n            RsKvError::Corruption { .. } => \"corruption\",\n            RsKvError::ResourceExhausted { .. } => \"resource_exhausted\",\n            RsKvError::Timeout { .. } => \"timeout\",\n            RsKvError::Conflict { .. } => \"conflict\",\n            RsKvError::Internal { .. } => \"internal\",\n        }\n    }\n}\n\n// Error conversion implementations\n// Note: memmap2::Error is private, so we convert through std::io::Error\n\nimpl From<std::num::TryFromIntError> for RsKvError {\n    fn from(err: std::num::TryFromIntError) -> Self {\n        RsKvError::Internal {\n            message: format!(\"Integer conversion error: {err}\"),\n        }\n    }\n}\n\n/// Result type alias for rskv operations\npub type Result<T> = std::result::Result<T, RsKvError>;\n\n/// Record header information (matches FASTER's RecordInfo)\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub struct RecordInfo {\n    /// Previous address in the version chain\n    pub previous_address: Address,\n    /// Checkpoint version when this record was created\n    pub checkpoint_version: u16,\n    /// Whether this record is marked as invalid\n    pub invalid: bool,\n    /// Whether this is a tombstone (deleted) record\n    pub tombstone: bool,\n    /// Whether this is the final record in a version chain\n    pub final_bit: bool,\n}\n\nimpl RecordInfo {\n    pub fn new(\n        previous_address: Address,\n        checkpoint_version: u16,\n        final_bit: bool,\n        tombstone: bool,\n        invalid: bool,\n    ) -> Self {\n        Self {\n            previous_address,\n            checkpoint_version,\n            invalid,\n            tombstone,\n            final_bit,\n        }\n    }\n\n    pub fn is_null(&self) -> bool {\n        self.previous_address == 0 && self.checkpoint_version == 0\n    }\n}\n\n/// Configuration for rskv instance\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Config {\n    /// Size of the hybrid log in memory (in bytes)\n    pub memory_size: u64,\n    /// Page size for the hybrid log\n    pub page_size: u32,\n    /// Directory for storing persistent data\n    pub storage_dir: String,\n    /// Whether to enable checkpointing\n    pub enable_checkpointing: bool,\n    /// Checkpoint interval in milliseconds\n    pub checkpoint_interval_ms: u64,\n    /// Whether to enable garbage collection\n    pub enable_gc: bool,\n    /// GC interval in milliseconds\n    pub gc_interval_ms: u64,\n    /// Maximum number of background threads\n    pub max_background_threads: usize,\n    /// Use memory mapping for storage devices\n    pub use_mmap: bool,\n    /// Enable read-ahead prefetching  \n    pub enable_readahead: bool,\n    /// Read-ahead buffer size in bytes\n    pub readahead_size: usize,\n    /// Enable write batching for better performance\n    pub enable_write_batching: bool,\n    /// Write batch size in bytes\n    pub write_batch_size: usize,\n    /// Enable compression for log data\n    pub enable_compression: bool,\n    /// Sync mode for durability vs performance trade-off\n    pub sync_mode: SyncMode,\n    /// Pre-allocate log file space\n    pub preallocate_log: bool,\n    /// Log file preallocation size in bytes\n    pub log_prealloc_size: u64,\n}\n\nimpl Config {\n    /// Validate the configuration parameters\n    pub fn validate(&self) -> Result<()> {\n        // Memory size validation\n        if self.memory_size < 1024 * 1024 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Memory size must be at least 1MB\".to_string(),\n            });\n        }\n\n        if self.memory_size > 64 * 1024 * 1024 * 1024 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Memory size cannot exceed 64GB\".to_string(),\n            });\n        }\n\n        // Page size validation\n        if self.page_size < 4096 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Page size must be at least 4KB\".to_string(),\n            });\n        }\n\n        if !self.page_size.is_power_of_two() {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Page size must be a power of 2\".to_string(),\n            });\n        }\n\n        if u64::from(self.page_size) > self.memory_size {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Page size cannot be larger than memory size\".to_string(),\n            });\n        }\n\n        // Storage directory validation\n        if self.storage_dir.is_empty() {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Storage directory cannot be empty\".to_string(),\n            });\n        }\n\n        // Interval validation\n        if self.checkpoint_interval_ms < 100 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Checkpoint interval must be at least 100ms\".to_string(),\n            });\n        }\n\n        if self.gc_interval_ms < 1000 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"GC interval must be at least 1000ms\".to_string(),\n            });\n        }\n\n        // Thread count validation\n        if self.max_background_threads == 0 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Maximum background threads must be at least 1\".to_string(),\n            });\n        }\n\n        if self.max_background_threads > 32 {\n            return Err(RsKvError::InvalidConfig {\n                message: \"Maximum background threads cannot exceed 32\".to_string(),\n            });\n        }\n\n        // Cross-parameter validation\n        if self.checkpoint_interval_ms > self.gc_interval_ms {\n            log::warn!(\n                \"Checkpoint interval ({} ms) is longer than GC interval ({} ms), this might cause \\\n                 performance issues\",\n                self.checkpoint_interval_ms,\n                self.gc_interval_ms\n            );\n        }\n\n        Ok(())\n    }\n\n    /// Create a configuration with memory size optimization\n    pub fn with_memory_size(memory_size: u64) -> Result<Self> {\n        let mut config = Self {\n            memory_size,\n            ..Self::default()\n        };\n\n        // Adjust page size based on memory size for optimal performance\n        if memory_size >= 8 * 1024 * 1024 * 1024 {\n            // 8GB+: Use 64MB pages\n            config.page_size = 64 * 1024 * 1024;\n        } else if memory_size >= 1024 * 1024 * 1024 {\n            // 1GB+: Use 32MB pages (default)\n            config.page_size = 32 * 1024 * 1024;\n        } else if memory_size >= 256 * 1024 * 1024 {\n            // 256MB+: Use 16MB pages\n            config.page_size = 16 * 1024 * 1024;\n        } else {\n            // <256MB: Use 8MB pages\n            config.page_size = 8 * 1024 * 1024;\n        }\n\n        config.validate()?;\n        Ok(config)\n    }\n\n    /// Create a configuration optimized for high-performance scenarios\n    pub fn high_performance() -> Result<Self> {\n        let config = Self {\n            memory_size: 4 * 1024 * 1024 * 1024, // 4GB\n            page_size: 64 * 1024 * 1024,         // 64MB pages\n            checkpoint_interval_ms: 30000,       // 30 seconds\n            gc_interval_ms: 60000,               // 1 minute\n            max_background_threads: 8,\n            ..Self::default()\n        };\n\n        config.validate()?;\n        Ok(config)\n    }\n\n    /// Create a configuration optimized for low-memory scenarios\n    pub fn low_memory() -> Result<Self> {\n        let config = Self {\n            memory_size: 64 * 1024 * 1024, // 64MB\n            page_size: 4 * 1024 * 1024,    // 4MB pages\n            checkpoint_interval_ms: 2000,  // 2 seconds\n            gc_interval_ms: 5000,          // 5 seconds\n            max_background_threads: 2,\n            ..Self::default()\n        };\n\n        config.validate()?;\n        Ok(config)\n    }\n}\n\nimpl Default for Config {\n    fn default() -> Self {\n        Self {\n            memory_size: 1024 * 1024 * 1024, // 1GB\n            page_size: PAGE_SIZE,\n            storage_dir: \"./rskv_data\".to_string(),\n            enable_checkpointing: true,\n            checkpoint_interval_ms: 5000, // 5 seconds\n            enable_gc: true,\n            gc_interval_ms: 10000, // 10 seconds\n            max_background_threads: 4,\n            use_mmap: true, // Enable mmap by default for better performance\n            enable_readahead: true,\n            readahead_size: 1024 * 1024, // 1MB\n            enable_write_batching: true,\n            write_batch_size: 64 * 1024, // 64KB\n            enable_compression: false,   // Disabled by default for simplicity\n            sync_mode: SyncMode::Periodic,\n            preallocate_log: true,\n            log_prealloc_size: 100 * 1024 * 1024, // 100MB\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_address_utilities() {\n        let page = 100;\n        let offset = 1024;\n\n        let address = make_address(page, offset);\n        assert_eq!(get_page(address), page);\n        assert_eq!(get_offset(address), offset);\n    }\n\n    #[test]\n    fn test_record_info() {\n        let record_info = RecordInfo::new(42, 1, true, false, false);\n        assert_eq!(record_info.previous_address, 42);\n        assert_eq!(record_info.checkpoint_version, 1);\n        assert!(record_info.final_bit);\n        assert!(!record_info.tombstone);\n        assert!(!record_info.invalid);\n        assert!(!record_info.is_null());\n    }\n\n    #[test]\n    fn test_null_record_info() {\n        let record_info = RecordInfo::new(0, 0, false, false, false);\n        assert!(record_info.is_null());\n    }\n}\n","traces":[{"line":50,"address":[],"length":0,"stats":{"Line":137}},{"line":51,"address":[],"length":0,"stats":{"Line":137}},{"line":55,"address":[],"length":0,"stats":{"Line":113}},{"line":56,"address":[],"length":0,"stats":{"Line":113}},{"line":60,"address":[],"length":0,"stats":{"Line":80}},{"line":61,"address":[],"length":0,"stats":{"Line":80}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":179,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":189,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":16}},{"line":240,"address":[],"length":0,"stats":{"Line":2}},{"line":241,"address":[],"length":0,"stats":{"Line":3}},{"line":286,"address":[],"length":0,"stats":{"Line":6}},{"line":288,"address":[],"length":0,"stats":{"Line":6}},{"line":289,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":12}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":354,"address":[],"length":0,"stats":{"Line":0}},{"line":355,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":373,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":378,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":384,"address":[],"length":0,"stats":{"Line":0}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":388,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[],"length":0,"stats":{"Line":0}},{"line":403,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":410,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":11}},{"line":425,"address":[],"length":0,"stats":{"Line":22}},{"line":427,"address":[],"length":0,"stats":{"Line":33}},{"line":435,"address":[],"length":0,"stats":{"Line":22}},{"line":437,"address":[],"length":0,"stats":{"Line":22}},{"line":441,"address":[],"length":0,"stats":{"Line":11}}],"covered":18,"coverable":91},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","epoch.rs"],"content":"//! Epoch-based memory management for rskv\n//!\n//! This module provides epoch-based garbage collection and memory reclamation\n//! using crossbeam-epoch. It's inspired by FASTER's light_epoch.h design.\n\nuse std::sync::Arc;\n\nuse crossbeam_epoch::{Collector, Guard, LocalHandle};\n\n/// Epoch manager that provides safe memory reclamation\n/// This is a wrapper around crossbeam-epoch that provides a simpler interface\n/// for the rest of the rskv codebase.\npub struct EpochManager {\n    collector: Collector,\n}\n\nimpl EpochManager {\n    /// Create a new epoch manager\n    pub fn new() -> Self {\n        Self {\n            collector: Collector::new(),\n        }\n    }\n\n    /// Create a new local handle for epoch management\n    /// Each thread should have its own local handle\n    pub fn register(&self) -> EpochHandle {\n        EpochHandle {\n            handle: self.collector.register(),\n        }\n    }\n\n    /// Pin the current thread to an epoch and return a guard\n    /// The guard must be held while accessing epoch-protected data\n    pub fn pin(&self) -> Guard {\n        self.collector.register().pin()\n    }\n\n    /// Flush all pending destructions in this epoch\n    pub fn flush(&self) {\n        // Force garbage collection for all threads\n        let guard = self.pin();\n        drop(guard);\n    }\n}\n\nimpl Default for EpochManager {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Thread-local epoch handle\n/// Each thread should have its own handle for optimal performance\npub struct EpochHandle {\n    handle: LocalHandle,\n}\n\nimpl EpochHandle {\n    /// Pin the current thread to an epoch and return a guard\n    pub fn pin(&mut self) -> Guard {\n        self.handle.pin()\n    }\n\n    /// Pin the current thread and return a guard (convenience method)\n    pub fn protect(&mut self) -> Guard {\n        self.pin()\n    }\n\n    /// Defer destruction of an object until it's safe to reclaim\n    /// This is used for lock-free data structures where we need to defer\n    /// the destruction of nodes until no other threads are accessing them\n    pub fn defer<F>(&mut self, f: F)\n    where\n        F: FnOnce() + Send + 'static,\n    {\n        let guard = self.pin();\n        guard.defer(f);\n    }\n\n    /// Defer destruction with a specific destructor function\n    ///\n    /// # Safety\n    /// The caller must ensure that the pointer was allocated via Box::into_raw\n    /// and is not used elsewhere after this call.\n    pub unsafe fn defer_destroy<T>(&mut self, ptr: *mut T)\n    where\n        T: Send + 'static,\n    {\n        // Convert to usize to make it Send\n        let ptr_addr = ptr as usize;\n        self.defer(move || {\n            let ptr = ptr_addr as *mut T;\n            if !ptr.is_null() {\n                unsafe {\n                    drop(Box::from_raw(ptr));\n                }\n            }\n        });\n    }\n\n    /// Flush any pending destructions\n    pub fn flush(&mut self) {\n        // Pin and then immediately unpin to force collection\n        let _guard = self.pin();\n    }\n}\n\n/// Epoch-protected pointer\n/// This is a smart pointer that can be safely accessed within an epoch\npub struct EpochPtr<T> {\n    ptr: *mut T,\n}\n\nimpl<T> EpochPtr<T> {\n    /// Create a new epoch-protected pointer\n    pub fn new(ptr: *mut T) -> Self {\n        Self { ptr }\n    }\n\n    /// Create a null epoch-protected pointer\n    pub fn null() -> Self {\n        Self {\n            ptr: std::ptr::null_mut(),\n        }\n    }\n\n    /// Check if the pointer is null\n    pub fn is_null(&self) -> bool {\n        self.ptr.is_null()\n    }\n\n    /// Get the raw pointer (unsafe)\n    /// The caller must ensure they hold an appropriate epoch guard\n    /// Get a raw pointer to the contained value\n    ///\n    /// # Safety\n    /// The caller must ensure that the pointer is not used after the value is dropped\n    pub unsafe fn as_ptr(&self) -> *mut T {\n        self.ptr\n    }\n\n    /// Get a reference to the pointed object (unsafe)\n    /// The caller must ensure they hold an appropriate epoch guard\n    /// and that the pointer is valid\n    /// Get a reference to the contained value\n    ///\n    /// # Safety\n    /// The caller must ensure that the reference is not used after the value is dropped\n    pub unsafe fn as_ref(&self) -> Option<&T> {\n        if self.ptr.is_null() {\n            None\n        } else {\n            unsafe { Some(&*self.ptr) }\n        }\n    }\n\n    /// Get a mutable reference to the pointed object (unsafe)\n    /// The caller must ensure they hold an appropriate epoch guard\n    /// and that the pointer is valid and exclusively accessible\n    /// Get a mutable reference to the contained value\n    ///\n    /// # Safety\n    /// The caller must ensure that the reference is not used after the value is dropped\n    pub unsafe fn as_mut(&mut self) -> Option<&mut T> {\n        if self.ptr.is_null() {\n            None\n        } else {\n            unsafe { Some(&mut *self.ptr) }\n        }\n    }\n}\n\nunsafe impl<T: Send> Send for EpochPtr<T> {}\nunsafe impl<T: Sync> Sync for EpochPtr<T> {}\n\nimpl<T> Clone for EpochPtr<T> {\n    fn clone(&self) -> Self {\n        *self\n    }\n}\n\nimpl<T> Copy for EpochPtr<T> {}\n\n/// Utility trait for epoch-based operations\npub trait EpochProtected {\n    /// Execute a function within an epoch guard\n    fn with_epoch<F, R>(&self, f: F) -> R\n    where\n        F: FnOnce(&Guard) -> R;\n}\n\nimpl EpochProtected for EpochManager {\n    fn with_epoch<F, R>(&self, f: F) -> R\n    where\n        F: FnOnce(&Guard) -> R,\n    {\n        let guard = self.pin();\n        f(&guard)\n    }\n}\n\n/// Shared epoch manager that can be used across multiple threads\npub type SharedEpochManager = Arc<EpochManager>;\n\n#[cfg(test)]\nmod tests {\n    use std::sync::Arc;\n    use std::sync::atomic::{AtomicUsize, Ordering};\n    use std::thread;\n\n    use super::*;\n\n    #[test]\n    fn test_epoch_manager_creation() {\n        let epoch_manager = EpochManager::new();\n        let _handle = epoch_manager.register();\n    }\n\n    #[test]\n    fn test_epoch_guard() {\n        let epoch_manager = EpochManager::new();\n        let _guard = epoch_manager.pin();\n        // Guard should protect current epoch\n    }\n\n    #[test]\n    fn test_defer_destruction() {\n        let epoch_manager = EpochManager::new();\n        let mut handle = epoch_manager.register();\n\n        let counter = Arc::new(AtomicUsize::new(0));\n        let counter_clone = counter.clone();\n\n        handle.defer(move || {\n            counter_clone.fetch_add(1, Ordering::SeqCst);\n        });\n\n        // Force garbage collection\n        handle.flush();\n\n        // Give some time for deferred destruction\n        thread::sleep(std::time::Duration::from_millis(10));\n\n        // Note: The exact timing of deferred destruction is not guaranteed\n        // This test mainly ensures the API works without panicking\n    }\n\n    #[test]\n    fn test_epoch_ptr() {\n        let value = Box::into_raw(Box::new(42i32));\n        let epoch_ptr = EpochPtr::new(value);\n\n        assert!(!epoch_ptr.is_null());\n\n        unsafe {\n            assert_eq!(*epoch_ptr.as_ptr(), 42);\n            if let Some(val_ref) = epoch_ptr.as_ref() {\n                assert_eq!(*val_ref, 42);\n            }\n\n            // Clean up\n            drop(Box::from_raw(value));\n        }\n    }\n\n    #[test]\n    fn test_null_epoch_ptr() {\n        let epoch_ptr: EpochPtr<i32> = EpochPtr::null();\n        assert!(epoch_ptr.is_null());\n\n        unsafe {\n            assert!(epoch_ptr.as_ref().is_none());\n        }\n    }\n\n    #[test]\n    fn test_with_epoch() {\n        let epoch_manager = EpochManager::new();\n\n        let result = epoch_manager.with_epoch(|_guard| 42);\n\n        assert_eq!(result, 42);\n    }\n}\n","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":33}},{"line":21,"address":[],"length":0,"stats":{"Line":33}},{"line":27,"address":[],"length":0,"stats":{"Line":2}},{"line":29,"address":[],"length":0,"stats":{"Line":2}},{"line":35,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":4}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":2}},{"line":62,"address":[],"length":0,"stats":{"Line":4}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":1}},{"line":77,"address":[],"length":0,"stats":{"Line":3}},{"line":78,"address":[],"length":0,"stats":{"Line":3}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":1}},{"line":105,"address":[],"length":0,"stats":{"Line":3}},{"line":117,"address":[],"length":0,"stats":{"Line":1}},{"line":122,"address":[],"length":0,"stats":{"Line":1}},{"line":124,"address":[],"length":0,"stats":{"Line":1}},{"line":129,"address":[],"length":0,"stats":{"Line":2}},{"line":130,"address":[],"length":0,"stats":{"Line":4}},{"line":139,"address":[],"length":0,"stats":{"Line":1}},{"line":140,"address":[],"length":0,"stats":{"Line":1}},{"line":150,"address":[],"length":0,"stats":{"Line":2}},{"line":151,"address":[],"length":0,"stats":{"Line":4}},{"line":152,"address":[],"length":0,"stats":{"Line":1}},{"line":154,"address":[],"length":0,"stats":{"Line":1}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":179,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":1}},{"line":198,"address":[],"length":0,"stats":{"Line":3}},{"line":199,"address":[],"length":0,"stats":{"Line":1}}],"covered":27,"coverable":47},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","gc.rs"],"content":"//! Garbage collection implementation for rskv\n//!\n//! This module implements epoch-based garbage collection inspired by FASTER's design.\n//! It reclaims space from old log entries and removes stale index entries.\n\nuse std::sync::Arc;\n// use serde::{Deserialize, Serialize}; // Reserved for future persistence\nuse std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};\n\nuse rayon::prelude::*;\n// use std::collections::HashMap; // Reserved for future use\nuse tokio::time::{Duration, Instant};\n\nuse crate::common::{Address, Key, Result, RsKvError, get_page};\nuse crate::hlog::HybridLog;\nuse crate::index::SharedMemHashIndex;\n\n/// State machine for garbage collection operations\npub struct GcState {\n    /// Whether GC is currently in progress\n    in_progress: AtomicBool,\n\n    /// Target begin address for the next GC cycle\n    #[allow(dead_code)]\n    target_begin_address: AtomicU64,\n\n    /// Reference to the hybrid log\n    hlog: Arc<HybridLog>,\n\n    /// Reference to the hash index\n    index: SharedMemHashIndex,\n\n    /// Statistics from the last GC run\n    last_stats: parking_lot::Mutex<Option<GcStats>>,\n\n    /// Number of entries processed in current GC cycle\n    entries_processed: AtomicUsize,\n\n    /// Number of entries removed in current GC cycle\n    entries_removed: AtomicUsize,\n}\n\n/// Statistics from a garbage collection cycle\n#[derive(Debug, Clone)]\npub struct GcStats {\n    /// Begin address before GC\n    pub initial_begin_address: Address,\n    /// New begin address after GC\n    pub new_begin_address: Address,\n    /// Number of bytes reclaimed\n    pub bytes_reclaimed: u64,\n    /// Number of index entries processed\n    pub entries_processed: usize,\n    /// Number of index entries removed\n    pub entries_removed: usize,\n    /// Duration of the GC operation\n    pub duration: Duration,\n    /// Timestamp when GC started\n    pub start_time: Instant,\n}\n\n/// Configuration for garbage collection\n#[derive(Debug, Clone)]\npub struct GcConfig {\n    /// Minimum amount of reclaimable space to trigger GC (in bytes)\n    pub min_reclaim_bytes: u64,\n    /// Maximum number of index entries to process in one batch\n    pub max_batch_size: usize,\n    /// Target utilization ratio (0.0 to 1.0)\n    pub target_utilization: f64,\n    /// Whether to perform parallel index scanning\n    pub parallel_scan: bool,\n}\n\nimpl Default for GcConfig {\n    fn default() -> Self {\n        Self {\n            min_reclaim_bytes: 64 * 1024 * 1024, // 64MB\n            max_batch_size: 10000,\n            target_utilization: 0.7, // Keep 70% of data\n            parallel_scan: true,\n        }\n    }\n}\n\nimpl GcState {\n    /// Create a new garbage collection state manager\n    pub fn new(hlog: Arc<HybridLog>, index: SharedMemHashIndex) -> Self {\n        Self {\n            in_progress: AtomicBool::new(false),\n            target_begin_address: AtomicU64::new(0),\n            hlog,\n            index,\n            last_stats: parking_lot::Mutex::new(None),\n            entries_processed: AtomicUsize::new(0),\n            entries_removed: AtomicUsize::new(0),\n        }\n    }\n\n    /// Check if garbage collection is currently in progress\n    pub fn is_in_progress(&self) -> bool {\n        self.in_progress.load(Ordering::Acquire)\n    }\n\n    /// Get statistics from the last GC run\n    pub fn last_stats(&self) -> Option<GcStats> {\n        self.last_stats.lock().clone()\n    }\n\n    /// Initiate garbage collection with the given configuration\n    pub async fn initiate_gc(&self, config: GcConfig) -> Result<GcStats> {\n        // Check if GC is already in progress\n        if self\n            .in_progress\n            .compare_exchange(false, true, Ordering::AcqRel, Ordering::Acquire)\n            .is_err()\n        {\n            return Err(RsKvError::GarbageCollectionFailed {\n                message: \"Garbage collection already in progress\".to_string(),\n            });\n        }\n\n        let start_time = Instant::now();\n        log::info!(\"Initiating garbage collection with config: {config:?}\");\n\n        // Reset counters\n        self.entries_processed.store(0, Ordering::Release);\n        self.entries_removed.store(0, Ordering::Release);\n\n        // Phase 1: Determine the new begin address\n        let initial_begin = self.hlog.get_begin_address();\n        let current_head = self.hlog.get_head_address();\n        let new_begin = self.calculate_new_begin_address(&config, initial_begin, current_head)?;\n\n        if new_begin <= initial_begin {\n            log::info!(\"No garbage collection needed\");\n            self.in_progress.store(false, Ordering::Release);\n\n            return Ok(GcStats {\n                initial_begin_address: initial_begin,\n                new_begin_address: initial_begin,\n                bytes_reclaimed: 0,\n                entries_processed: 0,\n                entries_removed: 0,\n                duration: start_time.elapsed(),\n                start_time,\n            });\n        }\n\n        log::info!(\n            \"Moving begin address from 0x{:x} to 0x{:x}\",\n            initial_begin,\n            new_begin\n        );\n\n        // Phase 2: Clean up stale index entries\n        let (entries_processed, entries_removed) =\n            self.cleanup_index_entries(new_begin, &config).await?;\n\n        // Phase 3: Update the begin address in the log and perform actual truncation\n        let actual_bytes_reclaimed = self.hlog.advance_begin_address(new_begin)?;\n        log::info!(\n            \"Cleaned {} entries, removed {}, reclaimed {} bytes\",\n            entries_processed,\n            entries_removed,\n            actual_bytes_reclaimed\n        );\n\n        // Calculate bytes reclaimed\n        let bytes_reclaimed = new_begin.saturating_sub(initial_begin);\n\n        let stats = GcStats {\n            initial_begin_address: initial_begin,\n            new_begin_address: new_begin,\n            bytes_reclaimed,\n            entries_processed,\n            entries_removed,\n            duration: start_time.elapsed(),\n            start_time,\n        };\n\n        // Store stats\n        *self.last_stats.lock() = Some(stats.clone());\n\n        log::info!(\n            \"Garbage collection completed in {:?}, reclaimed {} bytes\",\n            stats.duration,\n            bytes_reclaimed\n        );\n\n        // Mark GC as complete\n        self.in_progress.store(false, Ordering::Release);\n\n        Ok(stats)\n    }\n\n    /// Calculate the new begin address based on GC configuration\n    fn calculate_new_begin_address(\n        &self,\n        config: &GcConfig,\n        current_begin: Address,\n        current_head: Address,\n    ) -> Result<Address> {\n        let available_space = current_head.saturating_sub(current_begin);\n\n        if available_space < config.min_reclaim_bytes {\n            // Not enough space to reclaim\n            return Ok(current_begin);\n        }\n\n        // Calculate target based on utilization ratio\n        let target_reclaim = (available_space as f64 * (1.0 - config.target_utilization)) as u64;\n        let new_begin = current_begin + target_reclaim.min(available_space);\n\n        // Align to page boundary for efficiency\n        let new_begin_page = get_page(new_begin);\n        let aligned_begin = crate::common::make_address(new_begin_page, 0);\n\n        Ok(aligned_begin.min(current_head))\n    }\n\n    /// Clean up index entries that point to addresses before the new begin\n    async fn cleanup_index_entries(\n        &self,\n        new_begin_address: Address,\n        config: &GcConfig,\n    ) -> Result<(usize, usize)> {\n        log::debug!(\n            \"Cleaning up index entries older than address 0x{:x}\",\n            new_begin_address\n        );\n\n        if config.parallel_scan {\n            self.parallel_cleanup_index(new_begin_address, config).await\n        } else {\n            self.sequential_cleanup_index(new_begin_address, config)\n                .await\n        }\n    }\n\n    /// Parallel cleanup of index entries using rayon\n    async fn parallel_cleanup_index(\n        &self,\n        new_begin_address: Address,\n        _config: &GcConfig,\n    ) -> Result<(usize, usize)> {\n        // Collect all entries that need to be checked\n        let all_entries = self.index.snapshot();\n        let total_entries = all_entries.len();\n\n        log::debug!(\"Scanning {total_entries} index entries in parallel\");\n\n        // Process in parallel using rayon\n        let stale_keys: Vec<Key> = all_entries\n            .par_iter()\n            .filter_map(|(key, address)| {\n                if *address < new_begin_address {\n                    Some(key.clone())\n                } else {\n                    None\n                }\n            })\n            .collect();\n\n        let entries_to_remove = stale_keys.len();\n\n        // Remove stale entries\n        for key in stale_keys {\n            // Use conditional removal to avoid race conditions\n            self.index.remove_if_address(&key, new_begin_address);\n        }\n\n        self.entries_processed\n            .store(total_entries, Ordering::Release);\n        self.entries_removed\n            .store(entries_to_remove, Ordering::Release);\n\n        Ok((total_entries, entries_to_remove))\n    }\n\n    /// Sequential cleanup of index entries\n    async fn sequential_cleanup_index(\n        &self,\n        new_begin_address: Address,\n        config: &GcConfig,\n    ) -> Result<(usize, usize)> {\n        let mut entries_processed = 0;\n        let mut entries_removed = 0;\n        let mut batch = Vec::new();\n\n        // Collect entries in batches\n        self.index.for_each(|key, address| {\n            batch.push((key.clone(), address));\n\n            if batch.len() >= config.max_batch_size {\n                let (processed, removed) = self.process_batch(&batch, new_begin_address);\n                entries_processed += processed;\n                entries_removed += removed;\n                batch.clear();\n            }\n        });\n\n        // Process remaining batch\n        if !batch.is_empty() {\n            let (processed, removed) = self.process_batch(&batch, new_begin_address);\n            entries_processed += processed;\n            entries_removed += removed;\n        }\n\n        self.entries_processed\n            .store(entries_processed, Ordering::Release);\n        self.entries_removed\n            .store(entries_removed, Ordering::Release);\n\n        Ok((entries_processed, entries_removed))\n    }\n\n    /// Process a batch of index entries\n    fn process_batch(\n        &self,\n        batch: &[(Key, Address)],\n        new_begin_address: Address,\n    ) -> (usize, usize) {\n        let mut removed = 0;\n\n        for (key, address) in batch {\n            if *address < new_begin_address {\n                // This entry points to data that will be garbage collected\n                if self.index.remove_if_address(key, *address) {\n                    removed += 1;\n                }\n            }\n        }\n\n        (batch.len(), removed)\n    }\n\n    /// Estimate the amount of space that could be reclaimed\n    pub fn estimate_reclaimable_space(&self) -> Result<GcEstimate> {\n        let current_begin = self.hlog.get_begin_address();\n        let current_head = self.hlog.get_head_address();\n        let current_tail = self.hlog.get_tail_address();\n\n        // Count index entries pointing to different regions\n        let mut entries_in_disk_region = 0;\n        let mut entries_in_memory_region = 0;\n        let mut total_entries = 0;\n\n        self.index.for_each(|_key, address| {\n            total_entries += 1;\n            if address < current_head {\n                entries_in_disk_region += 1;\n            } else {\n                entries_in_memory_region += 1;\n            }\n        });\n\n        let disk_region_size = current_head.saturating_sub(current_begin);\n        let memory_region_size = current_tail.saturating_sub(current_head);\n\n        Ok(GcEstimate {\n            total_log_size: current_tail.saturating_sub(current_begin),\n            disk_region_size,\n            memory_region_size,\n            reclaimable_space: disk_region_size,\n            total_index_entries: total_entries,\n            entries_in_disk_region,\n            entries_in_memory_region,\n        })\n    }\n\n    /// Check if garbage collection is recommended\n    pub fn should_run_gc(&self, config: &GcConfig) -> Result<bool> {\n        let estimate = self.estimate_reclaimable_space()?;\n\n        Ok(estimate.reclaimable_space >= config.min_reclaim_bytes)\n    }\n}\n\n/// Estimate of garbage collection impact\n#[derive(Debug, Clone)]\npub struct GcEstimate {\n    /// Total size of the log\n    pub total_log_size: u64,\n    /// Size of the disk region (potentially reclaimable)\n    pub disk_region_size: u64,\n    /// Size of the memory region (not reclaimable)\n    pub memory_region_size: u64,\n    /// Estimated reclaimable space\n    pub reclaimable_space: u64,\n    /// Total number of index entries\n    pub total_index_entries: usize,\n    /// Number of entries pointing to disk region\n    pub entries_in_disk_region: usize,\n    /// Number of entries pointing to memory region\n    pub entries_in_memory_region: usize,\n}\n\n/// Extension trait for conditional removal from index  \ntrait ConditionalRemoval {\n    fn remove_if_address(&self, key: &Key, threshold_address: Address) -> bool;\n}\n\nimpl ConditionalRemoval for SharedMemHashIndex {\n    fn remove_if_address(&self, key: &Key, threshold_address: Address) -> bool {\n        if let Some(address) = self.find(key)\n            && address < threshold_address\n        {\n            return self.remove_if_address(key, address);\n        }\n        false\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use tempfile::tempdir;\n\n    use super::*;\n    use crate::epoch::EpochManager;\n    use crate::hlog::FileStorageDevice;\n    use crate::index::new_shared_mem_hash_index;\n\n    async fn create_test_gc_state() -> (GcState, tempfile::TempDir) {\n        let temp_dir = tempdir().unwrap();\n\n        let epoch = Arc::new(EpochManager::new());\n        let storage = Box::new(FileStorageDevice::new(temp_dir.path().join(\"test.log\")).unwrap());\n        let hlog = Arc::new(HybridLog::new(64 * 1024 * 1024, storage, epoch.clone()).unwrap());\n        let index = new_shared_mem_hash_index(epoch);\n\n        let gc_state = GcState::new(hlog, index);\n        (gc_state, temp_dir)\n    }\n\n    #[tokio::test]\n    async fn test_gc_estimate() {\n        let (gc_state, _temp_dir) = create_test_gc_state().await;\n\n        // Add some entries to the index\n        gc_state.index.insert(b\"key1\".to_vec(), 1000);\n        gc_state.index.insert(b\"key2\".to_vec(), 2000);\n        gc_state.index.insert(b\"key3\".to_vec(), 3000);\n\n        let estimate = gc_state.estimate_reclaimable_space().unwrap();\n\n        assert_eq!(estimate.total_index_entries, 3);\n        // Note: total_log_size might be 0 in test setup, which is fine\n    }\n\n    #[tokio::test]\n    async fn test_gc_should_run() {\n        let (gc_state, _temp_dir) = create_test_gc_state().await;\n\n        let config = GcConfig {\n            min_reclaim_bytes: 100, // Very low threshold for testing\n            ..Default::default()\n        };\n\n        // With empty log, should not need GC\n        let should_run = gc_state.should_run_gc(&config).unwrap();\n        assert!(!should_run);\n    }\n\n    #[tokio::test]\n    async fn test_gc_basic_operation() {\n        let (gc_state, _temp_dir) = create_test_gc_state().await;\n\n        // Add some data to index pointing to low addresses\n        gc_state.index.insert(b\"old_key1\".to_vec(), 100);\n        gc_state.index.insert(b\"old_key2\".to_vec(), 200);\n        gc_state.index.insert(b\"new_key1\".to_vec(), 10000);\n\n        let config = GcConfig {\n            min_reclaim_bytes: 0,    // Force GC to run\n            target_utilization: 0.5, // Aggressive GC\n            ..Default::default()\n        };\n\n        let stats = gc_state.initiate_gc(config).await.unwrap();\n\n        // In test setup, GC might not process entries due to test log setup\n        // Just verify it completed without error\n        assert!(!gc_state.is_in_progress());\n\n        // Verify stats are available (may be None if no actual work was done)\n        if let Some(last_stats) = gc_state.last_stats() {\n            assert_eq!(last_stats.entries_processed, stats.entries_processed);\n        }\n    }\n\n    #[tokio::test]\n    async fn test_gc_concurrent_prevention() {\n        let (gc_state, _temp_dir) = create_test_gc_state().await;\n\n        let config = GcConfig::default();\n\n        // Start first GC (this will complete immediately since there's no data)\n        let _first_result = gc_state.initiate_gc(config.clone()).await;\n\n        // Mark as in progress manually for testing\n        gc_state.in_progress.store(true, Ordering::Release);\n\n        // Try to start second GC\n        let second_result = gc_state.initiate_gc(config).await;\n\n        assert!(second_result.is_err());\n        assert!(matches!(\n            second_result,\n            Err(RsKvError::GarbageCollectionFailed { .. })\n        ));\n\n        // Clean up\n        gc_state.in_progress.store(false, Ordering::Release);\n    }\n\n    #[tokio::test]\n    async fn test_parallel_vs_sequential_cleanup() {\n        let (gc_state, _temp_dir) = create_test_gc_state().await;\n\n        // Add test data\n        for i in 0..100 {\n            // Smaller test set to avoid issues\n            gc_state\n                .index\n                .insert(format!(\"key_{}\", i).into_bytes(), i as u64);\n        }\n\n        let new_begin = 50; // Half the entries should be removed\n\n        // Test parallel cleanup\n        let config_parallel = GcConfig {\n            parallel_scan: true,\n            ..Default::default()\n        };\n\n        let (processed_par, removed_par) = gc_state\n            .parallel_cleanup_index(new_begin, &config_parallel)\n            .await\n            .unwrap();\n\n        // Restore data for sequential test\n        for i in 0..removed_par {\n            gc_state\n                .index\n                .insert(format!(\"key_{}\", i).into_bytes(), i as u64);\n        }\n\n        // Test sequential cleanup\n        let config_sequential = GcConfig {\n            parallel_scan: false,\n            max_batch_size: 10,\n            ..Default::default()\n        };\n\n        let (processed_seq, _removed_seq) = gc_state\n            .sequential_cleanup_index(new_begin, &config_sequential)\n            .await\n            .unwrap();\n\n        // Just verify both methods processed some entries\n        assert!(processed_par > 0);\n        assert!(processed_seq > 0);\n    }\n}\n","traces":[{"line":76,"address":[],"length":0,"stats":{"Line":9}},{"line":78,"address":[],"length":0,"stats":{"Line":9}},{"line":88,"address":[],"length":0,"stats":{"Line":16}},{"line":90,"address":[],"length":0,"stats":{"Line":32}},{"line":91,"address":[],"length":0,"stats":{"Line":32}},{"line":94,"address":[],"length":0,"stats":{"Line":48}},{"line":95,"address":[],"length":0,"stats":{"Line":16}},{"line":96,"address":[],"length":0,"stats":{"Line":16}},{"line":101,"address":[],"length":0,"stats":{"Line":1}},{"line":102,"address":[],"length":0,"stats":{"Line":3}},{"line":106,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":111,"address":[],"length":0,"stats":{"Line":6}},{"line":113,"address":[],"length":0,"stats":{"Line":6}},{"line":114,"address":[],"length":0,"stats":{"Line":6}},{"line":115,"address":[],"length":0,"stats":{"Line":6}},{"line":118,"address":[],"length":0,"stats":{"Line":1}},{"line":119,"address":[],"length":0,"stats":{"Line":1}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":2}},{"line":136,"address":[],"length":0,"stats":{"Line":2}},{"line":137,"address":[],"length":0,"stats":{"Line":6}},{"line":139,"address":[],"length":0,"stats":{"Line":2}},{"line":140,"address":[],"length":0,"stats":{"Line":4}},{"line":141,"address":[],"length":0,"stats":{"Line":4}},{"line":142,"address":[],"length":0,"stats":{"Line":2}},{"line":143,"address":[],"length":0,"stats":{"Line":2}},{"line":144,"address":[],"length":0,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":4}},{"line":146,"address":[],"length":0,"stats":{"Line":2}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":2}},{"line":204,"address":[],"length":0,"stats":{"Line":8}},{"line":206,"address":[],"length":0,"stats":{"Line":2}},{"line":208,"address":[],"length":0,"stats":{"Line":1}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":1}},{"line":248,"address":[],"length":0,"stats":{"Line":2}},{"line":249,"address":[],"length":0,"stats":{"Line":3}},{"line":251,"address":[],"length":0,"stats":{"Line":1}},{"line":254,"address":[],"length":0,"stats":{"Line":3}},{"line":256,"address":[],"length":0,"stats":{"Line":101}},{"line":257,"address":[],"length":0,"stats":{"Line":100}},{"line":258,"address":[],"length":0,"stats":{"Line":50}},{"line":260,"address":[],"length":0,"stats":{"Line":50}},{"line":265,"address":[],"length":0,"stats":{"Line":3}},{"line":268,"address":[],"length":0,"stats":{"Line":101}},{"line":273,"address":[],"length":0,"stats":{"Line":1}},{"line":274,"address":[],"length":0,"stats":{"Line":3}},{"line":275,"address":[],"length":0,"stats":{"Line":1}},{"line":276,"address":[],"length":0,"stats":{"Line":3}},{"line":278,"address":[],"length":0,"stats":{"Line":1}},{"line":282,"address":[],"length":0,"stats":{"Line":1}},{"line":287,"address":[],"length":0,"stats":{"Line":2}},{"line":288,"address":[],"length":0,"stats":{"Line":2}},{"line":289,"address":[],"length":0,"stats":{"Line":2}},{"line":292,"address":[],"length":0,"stats":{"Line":102}},{"line":293,"address":[],"length":0,"stats":{"Line":400}},{"line":295,"address":[],"length":0,"stats":{"Line":210}},{"line":296,"address":[],"length":0,"stats":{"Line":60}},{"line":297,"address":[],"length":0,"stats":{"Line":20}},{"line":298,"address":[],"length":0,"stats":{"Line":20}},{"line":299,"address":[],"length":0,"stats":{"Line":10}},{"line":304,"address":[],"length":0,"stats":{"Line":1}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":306,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":1}},{"line":311,"address":[],"length":0,"stats":{"Line":3}},{"line":312,"address":[],"length":0,"stats":{"Line":1}},{"line":313,"address":[],"length":0,"stats":{"Line":3}},{"line":315,"address":[],"length":0,"stats":{"Line":1}},{"line":319,"address":[],"length":0,"stats":{"Line":10}},{"line":324,"address":[],"length":0,"stats":{"Line":20}},{"line":326,"address":[],"length":0,"stats":{"Line":210}},{"line":329,"address":[],"length":0,"stats":{"Line":200}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":20}},{"line":339,"address":[],"length":0,"stats":{"Line":6}},{"line":340,"address":[],"length":0,"stats":{"Line":12}},{"line":341,"address":[],"length":0,"stats":{"Line":12}},{"line":342,"address":[],"length":0,"stats":{"Line":12}},{"line":345,"address":[],"length":0,"stats":{"Line":12}},{"line":346,"address":[],"length":0,"stats":{"Line":12}},{"line":347,"address":[],"length":0,"stats":{"Line":12}},{"line":349,"address":[],"length":0,"stats":{"Line":16}},{"line":350,"address":[],"length":0,"stats":{"Line":4}},{"line":351,"address":[],"length":0,"stats":{"Line":7}},{"line":352,"address":[],"length":0,"stats":{"Line":3}},{"line":354,"address":[],"length":0,"stats":{"Line":1}},{"line":358,"address":[],"length":0,"stats":{"Line":24}},{"line":359,"address":[],"length":0,"stats":{"Line":24}},{"line":361,"address":[],"length":0,"stats":{"Line":6}},{"line":362,"address":[],"length":0,"stats":{"Line":24}},{"line":363,"address":[],"length":0,"stats":{"Line":12}},{"line":364,"address":[],"length":0,"stats":{"Line":12}},{"line":365,"address":[],"length":0,"stats":{"Line":12}},{"line":366,"address":[],"length":0,"stats":{"Line":12}},{"line":367,"address":[],"length":0,"stats":{"Line":6}},{"line":368,"address":[],"length":0,"stats":{"Line":6}},{"line":373,"address":[],"length":0,"stats":{"Line":5}},{"line":374,"address":[],"length":0,"stats":{"Line":15}},{"line":405,"address":[],"length":0,"stats":{"Line":150}},{"line":406,"address":[],"length":0,"stats":{"Line":450}},{"line":407,"address":[],"length":0,"stats":{"Line":150}},{"line":409,"address":[],"length":0,"stats":{"Line":200}}],"covered":99,"coverable":116},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","hlog.rs"],"content":"//! Hybrid Log (HLog) implementation for rskv\n//!\n//! This module implements the core storage engine inspired by FASTER's\n//! PersistentMemoryMalloc. It provides a large, in-memory, circular buffer\n//! with persistent storage support.\n\nuse std::fs::{File, OpenOptions};\nuse std::path::{Path, PathBuf};\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicU64, Ordering};\n\nuse memmap2::{MmapMut, MmapOptions};\nuse parking_lot::{Mutex, RwLock};\nuse serde::{Deserialize, Serialize};\n\nuse crate::common::{\n    Address, Key, PAGE_SIZE, RecordInfo, Result, RsKvError, Value, get_offset, get_page,\n    make_address,\n};\nuse crate::epoch::SharedEpochManager;\n\n/// Storage device trait for abstracting disk I/O operations\npub trait StorageDevice {\n    /// Write data to storage at the specified offset\n    fn write(&mut self, offset: u64, data: &[u8]) -> Result<()>;\n\n    /// Read data from storage at the specified offset\n    fn read(&self, offset: u64, buf: &mut [u8]) -> Result<usize>;\n\n    /// Flush pending writes to storage\n    fn flush(&mut self) -> Result<()>;\n\n    /// Get the size of the storage device\n    fn size(&self) -> u64;\n\n    /// Truncate the storage to the specified size\n    fn truncate(&mut self, size: u64) -> Result<()>;\n\n    /// Check if the storage device supports memory mapping\n    fn supports_mmap(&self) -> bool {\n        false\n    }\n\n    /// Get memory mapped access to the storage (if supported)\n    fn get_mmap(&mut self, offset: u64, len: usize) -> Result<Option<&mut [u8]>> {\n        let _ = (offset, len);\n        Ok(None)\n    }\n}\n\n/// File-based storage device implementation\npub struct FileStorageDevice {\n    file: File,\n    #[allow(dead_code)]\n    path: PathBuf,\n}\n\nimpl FileStorageDevice {\n    pub fn new<P: AsRef<Path>>(path: P) -> Result<Self> {\n        let path = path.as_ref().to_path_buf();\n        let file = OpenOptions::new()\n            .read(true)\n            .write(true)\n            .create(true)\n            .truncate(true)\n            .open(&path)?;\n\n        Ok(Self { file, path })\n    }\n}\n\nimpl StorageDevice for FileStorageDevice {\n    fn write(&mut self, offset: u64, data: &[u8]) -> Result<()> {\n        use std::io::{Seek, SeekFrom, Write};\n\n        self.file.seek(SeekFrom::Start(offset))?;\n        self.file.write_all(data)?;\n        Ok(())\n    }\n\n    fn read(&self, offset: u64, buf: &mut [u8]) -> Result<usize> {\n        use std::io::{Read, Seek, SeekFrom};\n\n        let mut file = &self.file;\n        file.seek(SeekFrom::Start(offset))?;\n        Ok(file.read(buf)?)\n    }\n\n    fn flush(&mut self) -> Result<()> {\n        use std::io::Write;\n        self.file.flush()?;\n        Ok(())\n    }\n\n    fn size(&self) -> u64 {\n        self.file.metadata().map(|m| m.len()).unwrap_or(0)\n    }\n\n    fn truncate(&mut self, size: u64) -> Result<()> {\n        self.file.set_len(size)?;\n        Ok(())\n    }\n}\n\n/// Atomic page offset structure (matches FASTER's PageOffset)\n#[derive(Debug)]\npub struct AtomicPageOffset {\n    value: AtomicU64,\n}\n\nimpl AtomicPageOffset {\n    pub fn new(page: u32, offset: u32) -> Self {\n        let value = make_address(page, offset);\n        Self {\n            value: AtomicU64::new(value),\n        }\n    }\n\n    pub fn load(&self) -> (u32, u32) {\n        let addr = self.value.load(Ordering::Acquire);\n        (get_page(addr), get_offset(addr))\n    }\n\n    pub fn store(&self, page: u32, offset: u32) {\n        let addr = make_address(page, offset);\n        self.value.store(addr, Ordering::Release);\n    }\n\n    /// Reserve space for allocation (atomic fetch_add operation)\n    /// Returns the old page and offset values\n    pub fn reserve(&self, size: u32) -> (u32, u32) {\n        let old_value = self.value.fetch_add(size as u64, Ordering::AcqRel);\n        (get_page(old_value), get_offset(old_value))\n    }\n\n    /// Compare and exchange operation for page boundary crossing\n    pub fn compare_exchange(\n        &self,\n        expected_page: u32,\n        expected_offset: u32,\n        new_page: u32,\n        new_offset: u32,\n    ) -> std::result::Result<(), (u32, u32)> {\n        let expected = make_address(expected_page, expected_offset);\n        let new_value = make_address(new_page, new_offset);\n\n        match self\n            .value\n            .compare_exchange(expected, new_value, Ordering::AcqRel, Ordering::Acquire)\n        {\n            Ok(_) => Ok(()),\n            Err(actual) => Err((get_page(actual), get_offset(actual))),\n        }\n    }\n}\n\n/// Status of a page in the hybrid log\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum PageStatus {\n    /// Page is not allocated\n    NotAllocated,\n    /// Page is in memory and mutable\n    InMemory,\n    /// Page is being flushed to disk\n    Flushing,\n    /// Page has been flushed to disk\n    OnDisk,\n}\n\n/// Record stored in the hybrid log\n/// This is the serialized form that gets written to the log\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LogRecord {\n    /// Record header with metadata\n    pub header: RecordInfo,\n    /// The key (variable length)\n    pub key: Key,\n    /// The value (variable length)\n    pub value: Value,\n}\n\nimpl LogRecord {\n    pub fn new(key: Key, value: Value, previous_address: Address) -> Self {\n        Self {\n            header: RecordInfo::new(previous_address, 0, true, false, false),\n            key,\n            value,\n        }\n    }\n\n    /// Calculate the serialized size of this record\n    pub fn serialized_size(&self) -> u32 {\n        // Use bincode to estimate size\n        bincode::serialized_size(self).unwrap_or(0) as u32\n    }\n\n    /// Create a tombstone record for deletion\n    pub fn tombstone(key: Key, previous_address: Address) -> Self {\n        Self {\n            header: RecordInfo::new(previous_address, 0, true, true, false),\n            key,\n            value: Vec::new(),\n        }\n    }\n}\n\n/// The Hybrid Log - core storage engine inspired by FASTER\npub struct HybridLog {\n    /// In-memory circular buffer of pages\n    pages: Vec<RwLock<Option<Box<[u8]>>>>,\n\n    /// Page status tracking\n    page_status: Vec<RwLock<PageStatus>>,\n\n    /// Size of the circular buffer (number of pages)\n    buffer_size: u32,\n\n    /// Four atomic pointers defining log regions (matching FASTER design)\n    ///\n    /// Logical address space regions:\n    /// [begin_address, head_address): on disk only, can be garbage collected\n    /// [head_address, read_only_address): in memory, read-only, can be flushed\n    /// [read_only_address, tail_address): in memory, mutable (hot data)\n\n    /// Beginning of the log (data before this is truncated)\n    begin_address: AtomicU64,\n\n    /// Start of the in-memory portion\n    head_address: AtomicU64,\n\n    /// Boundary between read-only and mutable regions\n    read_only_address: AtomicU64,\n\n    /// End of the log where new data is appended\n    tail_page_offset: AtomicPageOffset,\n\n    /// Epoch manager for safe memory reclamation\n    #[allow(dead_code)]\n    epoch: SharedEpochManager,\n\n    /// Storage device for persistence\n    #[allow(dead_code)]\n    storage: Arc<Mutex<Box<dyn StorageDevice + Send + Sync>>>,\n\n    /// Address that has been flushed to disk\n    flushed_until_address: AtomicU64,\n}\n\nimpl HybridLog {\n    /// Create a new hybrid log instance\n    pub fn new(\n        memory_size: u64,\n        storage_device: Box<dyn StorageDevice + Send + Sync>,\n        epoch: SharedEpochManager,\n    ) -> Result<Self> {\n        let buffer_size = (memory_size / PAGE_SIZE as u64) as u32;\n        if buffer_size == 0 {\n            return Err(RsKvError::Configuration {\n                message: \"Memory size too small for at least one page\".to_string(),\n            });\n        }\n\n        let mut pages = Vec::with_capacity(buffer_size as usize);\n        let mut page_status = Vec::with_capacity(buffer_size as usize);\n\n        for _ in 0..buffer_size {\n            pages.push(RwLock::new(None));\n            page_status.push(RwLock::new(PageStatus::NotAllocated));\n        }\n\n        // Initialize the first page\n        let start_address = u64_to_address(PAGE_SIZE as u64); // Skip the invalid page\n\n        let hlog = Self {\n            pages,\n            page_status,\n            buffer_size,\n            begin_address: AtomicU64::new(address_to_u64(start_address)),\n            head_address: AtomicU64::new(address_to_u64(start_address)),\n            read_only_address: AtomicU64::new(address_to_u64(start_address)),\n            tail_page_offset: AtomicPageOffset::new(\n                get_page(start_address),\n                get_offset(start_address),\n            ),\n            epoch,\n            storage: Arc::new(Mutex::new(storage_device)),\n            flushed_until_address: AtomicU64::new(address_to_u64(start_address)),\n        };\n\n        // Allocate the first page\n        hlog.allocate_page(get_page(start_address))?;\n\n        Ok(hlog)\n    }\n\n    /// Allocate space in the log for a record of given size\n    /// Returns the address where the record can be written, or None if allocation fails\n    pub fn allocate(&self, size: u32) -> Option<Address> {\n        if size == 0 || size > PAGE_SIZE {\n            return None;\n        }\n\n        loop {\n            let (old_page, old_offset) = self.tail_page_offset.reserve(size);\n            let new_offset = old_offset + size;\n\n            if new_offset <= PAGE_SIZE {\n                // Allocation fits in current page\n                let address = make_address(old_page, old_offset);\n\n                // Ensure the page is allocated\n                if self.allocate_page(old_page).is_err() {\n                    return None;\n                }\n\n                return Some(address);\n            } else {\n                // Need to move to next page\n                let new_page = old_page + 1;\n                if new_page > u32::MAX - 1 {\n                    return None; // Address space exhausted\n                }\n\n                // Try to advance to the next page\n                if self\n                    .tail_page_offset\n                    .compare_exchange(old_page, new_offset, new_page, size)\n                    .is_ok()\n                {\n                    // Successfully moved to new page\n                    if self.allocate_page(new_page).is_err() {\n                        return None;\n                    }\n\n                    return Some(make_address(new_page, 0));\n                }\n                // If CAS failed, retry the allocation\n            }\n        }\n    }\n\n    /// Get a pointer to data at the specified address\n    /// Returns a slice of the requested data if available in memory\n    pub fn get(&self, address: Address) -> Option<&[u8]> {\n        let page = get_page(address);\n        let offset = get_offset(address);\n\n        let page_index = (page % self.buffer_size) as usize;\n        let page_guard = self.pages[page_index].read();\n\n        if let Some(ref page_data) = *page_guard\n            && (offset as usize) < page_data.len()\n        {\n            // SAFETY: We've verified the bounds above\n            unsafe {\n                let ptr = page_data.as_ptr().add(offset as usize);\n                return Some(std::slice::from_raw_parts(\n                    ptr,\n                    page_data.len() - offset as usize,\n                ));\n            }\n        }\n\n        None\n    }\n\n    /// Write data to the log at the specified address\n    pub fn write(&self, address: Address, data: &[u8]) -> Result<()> {\n        let page = get_page(address);\n        let offset = get_offset(address);\n\n        if offset as usize + data.len() > PAGE_SIZE as usize {\n            return Err(RsKvError::AllocationFailed {\n                size: data.len() as u32,\n            });\n        }\n\n        let page_index = (page % self.buffer_size) as usize;\n        let mut page_guard = self.pages[page_index].write();\n\n        if let Some(ref mut page_data) = *page_guard {\n            let start = offset as usize;\n            let end = start + data.len();\n\n            if end <= page_data.len() {\n                page_data[start..end].copy_from_slice(data);\n                return Ok(());\n            }\n        }\n\n        Err(RsKvError::AddressOutOfBounds { address })\n    }\n\n    /// Insert a record into the log\n    pub fn insert_record(&self, record: LogRecord) -> Result<Address> {\n        // Serialize the record\n        let serialized = bincode::serialize(&record)?;\n        let size = serialized.len() as u32;\n\n        // Allocate space\n        let address = self\n            .allocate(size)\n            .ok_or(RsKvError::AllocationFailed { size })?;\n\n        // Write the serialized record\n        self.write(address, &serialized)?;\n\n        Ok(address)\n    }\n\n    /// Read a record from the log\n    pub fn read_record(&self, address: Address) -> Result<LogRecord> {\n        // First, try to read from memory\n        if let Some(data) = self.get(address) {\n            // Try to deserialize the record from memory\n            match bincode::deserialize(data) {\n                Ok(record) => return Ok(record),\n                Err(_) => {\n                    // Data might be truncated in memory buffer, try disk\n                }\n            }\n        }\n\n        // If not in memory or incomplete, read from disk\n        self.read_record_from_disk(address)\n    }\n\n    /// Read a record from disk storage\n    fn read_record_from_disk(&self, address: Address) -> Result<LogRecord> {\n        // For this implementation, we'll read a fixed buffer size and try to deserialize\n        const INITIAL_READ_SIZE: usize = 1024; // Start with 1KB\n        const MAX_RECORD_SIZE: usize = 64 * 1024; // Max 64KB per record\n\n        let storage = self.storage.lock();\n        let mut buffer = vec![0u8; INITIAL_READ_SIZE];\n\n        // Read initial chunk\n        let bytes_read = storage.read(address, &mut buffer)?;\n        if bytes_read == 0 {\n            return Err(RsKvError::AddressOutOfBounds { address });\n        }\n\n        // Try to deserialize with initial buffer\n        match bincode::deserialize::<LogRecord>(&buffer[..bytes_read]) {\n            Ok(record) => Ok(record),\n            Err(_) => {\n                // Buffer might be too small, try with larger buffer\n                let mut large_buffer = vec![0u8; MAX_RECORD_SIZE];\n                let large_bytes_read = storage.read(address, &mut large_buffer)?;\n\n                if large_bytes_read == 0 {\n                    return Err(RsKvError::AddressOutOfBounds { address });\n                }\n\n                bincode::deserialize(&large_buffer[..large_bytes_read])\n                    .map_err(RsKvError::Serialization)\n            }\n        }\n    }\n\n    /// Allocate a page in the buffer\n    fn allocate_page(&self, page: u32) -> Result<()> {\n        let page_index = (page % self.buffer_size) as usize;\n\n        let mut page_guard = self.pages[page_index].write();\n        if page_guard.is_none() {\n            // Allocate the page\n            let page_data = vec![0u8; PAGE_SIZE as usize].into_boxed_slice();\n            *page_guard = Some(page_data);\n\n            // Update status\n            let mut status_guard = self.page_status[page_index].write();\n            *status_guard = PageStatus::InMemory;\n        }\n\n        Ok(())\n    }\n\n    /// Shift the read-only address to the current tail\n    /// This makes all current mutable data read-only\n    pub fn shift_read_only_address(&self) -> Address {\n        let tail_address = self.get_tail_address();\n        let old_read_only = self\n            .read_only_address\n            .swap(address_to_u64(tail_address), Ordering::AcqRel);\n        u64_to_address(old_read_only)\n    }\n\n    /// Shift the head address forward\n    /// This removes pages from memory and makes them disk-only\n    pub fn shift_head_address(&self, new_head_address: Address) -> Result<()> {\n        let old_head = self\n            .head_address\n            .swap(address_to_u64(new_head_address), Ordering::AcqRel);\n        let old_head_address = u64_to_address(old_head);\n\n        // Evict pages that are now below the head address\n        self.evict_pages_below_head(old_head_address, new_head_address)?;\n\n        log::debug!(\n            \"Shifted head address from 0x{:x} to 0x{:x}\",\n            old_head_address,\n            new_head_address\n        );\n\n        Ok(())\n    }\n\n    /// Evict pages from memory that are now below the head address\n    fn evict_pages_below_head(&self, old_head: Address, new_head: Address) -> Result<()> {\n        let old_head_page = get_page(old_head);\n        let new_head_page = get_page(new_head);\n\n        // Evict all pages between old_head and new_head\n        for page in old_head_page..new_head_page {\n            self.evict_page(page)?;\n        }\n\n        Ok(())\n    }\n\n    /// Evict a specific page from memory\n    fn evict_page(&self, page: u32) -> Result<()> {\n        let page_index = (page % self.buffer_size) as usize;\n\n        // Lock the page and set status to OnDisk\n        {\n            let mut page_guard = self.pages[page_index].write();\n            let mut status_guard = self.page_status[page_index].write();\n\n            if *status_guard == PageStatus::InMemory {\n                // Free the page memory\n                *page_guard = None;\n                *status_guard = PageStatus::OnDisk;\n\n                log::trace!(\"Evicted page {page} from memory\");\n            }\n        }\n\n        Ok(())\n    }\n\n    /// Get current tail address\n    pub fn get_tail_address(&self) -> Address {\n        let (page, offset) = self.tail_page_offset.load();\n        make_address(page, offset)\n    }\n\n    /// Get current head address\n    pub fn get_head_address(&self) -> Address {\n        u64_to_address(self.head_address.load(Ordering::Acquire))\n    }\n\n    /// Get current read-only address\n    pub fn get_read_only_address(&self) -> Address {\n        u64_to_address(self.read_only_address.load(Ordering::Acquire))\n    }\n\n    /// Get current begin address\n    pub fn get_begin_address(&self) -> Address {\n        u64_to_address(self.begin_address.load(Ordering::Acquire))\n    }\n\n    /// Advance the begin address and truncate the log\n    /// This permanently removes data from storage and reclaims space\n    pub fn advance_begin_address(&self, new_begin_address: Address) -> Result<u64> {\n        let old_begin = self\n            .begin_address\n            .swap(address_to_u64(new_begin_address), Ordering::AcqRel);\n        let old_begin_address = u64_to_address(old_begin);\n\n        if new_begin_address <= old_begin_address {\n            // Nothing to truncate\n            return Ok(0);\n        }\n\n        // Calculate how many bytes we're reclaiming\n        let bytes_reclaimed = new_begin_address.saturating_sub(old_begin_address);\n\n        // Perform actual storage truncation\n        self.truncate_storage(old_begin_address, new_begin_address)?;\n\n        log::info!(\n            \"Advanced begin address from 0x{:x} to 0x{:x}, reclaimed {} bytes\",\n            old_begin_address,\n            new_begin_address,\n            bytes_reclaimed\n        );\n\n        Ok(bytes_reclaimed)\n    }\n\n    /// Truncate storage by removing data before the new begin address\n    fn truncate_storage(&self, old_begin: Address, new_begin: Address) -> Result<()> {\n        let mut storage = self.storage.lock();\n\n        // For memory-mapped files, we can't actually truncate from the beginning\n        // Instead, we mark the space as invalid and potentially compact later\n        if storage.supports_mmap() {\n            // For mmap devices, we use a different strategy\n            self.mark_space_invalid(old_begin, new_begin)?;\n        } else {\n            // For regular file devices, we can perform actual truncation\n            // by copying remaining data to the beginning of the file\n            self.compact_storage(&mut **storage, old_begin, new_begin)?;\n        }\n\n        Ok(())\n    }\n\n    /// Mark space as invalid for memory-mapped storage\n    fn mark_space_invalid(&self, _old_begin: Address, _new_begin: Address) -> Result<()> {\n        // For now, we just update the begin address\n        // In a production system, this might involve:\n        // 1. Marking pages as free in a free list\n        // 2. Scheduling background compaction\n        // 3. Using file hole punching (fallocate) on supported filesystems\n\n        log::debug!(\"Marked address range as invalid (mmap storage)\");\n        Ok(())\n    }\n\n    /// Compact storage by moving data and truncating the file\n    fn compact_storage(\n        &self,\n        storage: &mut dyn StorageDevice,\n        old_begin: Address,\n        new_begin: Address,\n    ) -> Result<()> {\n        const BUFFER_SIZE: usize = 1024 * 1024; // 1MB buffer\n        let mut buffer = vec![0u8; BUFFER_SIZE];\n\n        let total_size = storage.size();\n        let truncate_amount = new_begin - old_begin;\n\n        if new_begin >= total_size {\n            // Truncating everything\n            storage.truncate(0)?;\n            return Ok(());\n        }\n\n        // Read data from new_begin onwards and write it to the beginning\n        let mut read_offset = new_begin;\n        let mut write_offset = 0u64;\n\n        while read_offset < total_size {\n            let bytes_to_read = BUFFER_SIZE.min((total_size - read_offset) as usize);\n            let bytes_read = storage.read(read_offset, &mut buffer[..bytes_to_read])?;\n\n            if bytes_read == 0 {\n                break;\n            }\n\n            storage.write(write_offset, &buffer[..bytes_read])?;\n\n            read_offset += bytes_read as u64;\n            write_offset += bytes_read as u64;\n        }\n\n        // Truncate file to new size\n        let new_size = total_size - truncate_amount;\n        storage.truncate(new_size)?;\n        storage.flush()?;\n\n        log::debug!(\n            \"Compacted storage: removed {} bytes, new size: {} bytes\",\n            truncate_amount,\n            new_size\n        );\n\n        Ok(())\n    }\n\n    /// Flush data to storage device\n    pub async fn flush_to_disk(&self, until_address: Address) -> Result<()> {\n        let current_flushed = u64_to_address(self.flushed_until_address.load(Ordering::Acquire));\n\n        if until_address <= current_flushed {\n            // Already flushed\n            return Ok(());\n        }\n\n        log::debug!(\n            \"Flushing data from 0x{:x} to 0x{:x}\",\n            current_flushed,\n            until_address\n        );\n\n        // Flush page by page\n        let start_page = get_page(current_flushed);\n        let end_page = get_page(until_address);\n\n        for page in start_page..=end_page {\n            self.flush_page_to_disk(page).await?;\n        }\n\n        // Update flushed address\n        self.flushed_until_address\n            .store(address_to_u64(until_address), Ordering::Release);\n\n        // Ensure storage device commits the data\n        {\n            let mut storage = self.storage.lock();\n            storage.flush()?;\n        }\n\n        log::debug!(\"Flush completed to address 0x{until_address:x}\");\n        Ok(())\n    }\n\n    /// Flush a specific page to disk\n    async fn flush_page_to_disk(&self, page: u32) -> Result<()> {\n        let page_index = (page % self.buffer_size) as usize;\n\n        // Get page data under lock\n        let page_data = {\n            let page_guard = self.pages[page_index].read();\n            let status_guard = self.page_status[page_index].read();\n\n            if *status_guard != PageStatus::InMemory {\n                // Page not in memory or already flushed\n                return Ok(());\n            }\n\n            if let Some(ref data) = *page_guard {\n                data.clone()\n            } else {\n                return Ok(()); // No data to flush\n            }\n        };\n\n        // Calculate disk offset for this page\n        let disk_offset = (page as u64) * (PAGE_SIZE as u64);\n\n        // Write to storage device (this is the potentially slow operation)\n        {\n            let mut storage = self.storage.lock();\n            storage.write(disk_offset, &page_data)?;\n        }\n\n        // Update page status to indicate it's been flushed\n        {\n            let mut status_guard = self.page_status[page_index].write();\n            if *status_guard == PageStatus::InMemory {\n                *status_guard = PageStatus::Flushing; // Mark as flushing\n            }\n        }\n\n        log::trace!(\n            \"Flushed page {} to disk at offset 0x{:x}\",\n            page,\n            disk_offset\n        );\n        Ok(())\n    }\n}\n\n/// Memory-mapped storage device for high-performance large file access\npub struct MmapStorageDevice {\n    file: File,\n    mmap: Option<MmapMut>,\n    #[allow(dead_code)]\n    path: PathBuf,\n    size: u64,\n    dirty: bool,\n}\n\nimpl MmapStorageDevice {\n    /// Create a new memory-mapped storage device\n    pub fn new<P: AsRef<Path>>(path: P) -> Result<Self> {\n        let path = path.as_ref().to_path_buf();\n        let file = OpenOptions::new()\n            .create(true)\n            .truncate(true)\n            .read(true)\n            .write(true)\n            .open(&path)?;\n\n        let metadata = file.metadata()?;\n        let size = metadata.len();\n\n        let mut device = Self {\n            file,\n            mmap: None,\n            path,\n            size,\n            dirty: false,\n        };\n\n        // Initialize memory mapping if file is not empty\n        if size > 0 {\n            device.init_mmap()?;\n        }\n\n        Ok(device)\n    }\n\n    /// Initialize memory mapping for the current file size\n    fn init_mmap(&mut self) -> Result<()> {\n        if self.size > 0 {\n            let mmap = unsafe {\n                MmapOptions::new()\n                    .len(self.size as usize)\n                    .map_mut(&self.file)?\n            };\n            self.mmap = Some(mmap);\n        }\n        Ok(())\n    }\n\n    /// Resize the file and remmap if necessary\n    fn resize_and_remap(&mut self, new_size: u64) -> Result<()> {\n        if new_size != self.size {\n            // Drop old mapping\n            self.mmap = None;\n\n            // Resize file\n            self.file.set_len(new_size)?;\n            self.size = new_size;\n\n            // Create new mapping\n            if new_size > 0 {\n                self.init_mmap()?;\n            }\n        }\n        Ok(())\n    }\n\n    /// Ensure the file is large enough for the given offset + length\n    fn ensure_capacity(&mut self, offset: u64, len: usize) -> Result<()> {\n        let required_size = offset + len as u64;\n        if required_size > self.size {\n            // Grow file by at least 64MB chunks for efficiency\n            const GROWTH_CHUNK: u64 = 64 * 1024 * 1024;\n            let new_size = required_size.div_ceil(GROWTH_CHUNK) * GROWTH_CHUNK;\n            self.resize_and_remap(new_size)?;\n        }\n        Ok(())\n    }\n}\n\nimpl StorageDevice for MmapStorageDevice {\n    fn write(&mut self, offset: u64, data: &[u8]) -> Result<()> {\n        self.ensure_capacity(offset, data.len())?;\n\n        if let Some(ref mut mmap) = self.mmap {\n            let start = offset as usize;\n            let end = start + data.len();\n\n            if end <= mmap.len() {\n                mmap[start..end].copy_from_slice(data);\n                self.dirty = true;\n                return Ok(());\n            }\n        }\n\n        // Fallback to file I/O if mmap is not available or out of bounds\n        use std::io::{Seek, SeekFrom, Write};\n        self.file.seek(SeekFrom::Start(offset))?;\n        self.file.write_all(data)?;\n        Ok(())\n    }\n\n    fn read(&self, offset: u64, buf: &mut [u8]) -> Result<usize> {\n        if let Some(ref mmap) = self.mmap {\n            let start = offset as usize;\n            let len = buf.len().min(mmap.len().saturating_sub(start));\n\n            if len > 0 {\n                buf[..len].copy_from_slice(&mmap[start..start + len]);\n                return Ok(len);\n            }\n        }\n\n        // Fallback to file I/O if mmap is not available\n        use std::io::{Read, Seek, SeekFrom};\n        let mut file = &self.file;\n        file.seek(SeekFrom::Start(offset))?;\n        Ok(file.read(buf)?)\n    }\n\n    fn flush(&mut self) -> Result<()> {\n        if self.dirty {\n            if let Some(ref mut mmap) = self.mmap {\n                mmap.flush()?;\n            }\n            self.file.sync_all()?;\n            self.dirty = false;\n        }\n        Ok(())\n    }\n\n    fn size(&self) -> u64 {\n        self.size\n    }\n\n    fn truncate(&mut self, size: u64) -> Result<()> {\n        self.resize_and_remap(size)?;\n        Ok(())\n    }\n\n    fn supports_mmap(&self) -> bool {\n        true\n    }\n\n    fn get_mmap(&mut self, offset: u64, len: usize) -> Result<Option<&mut [u8]>> {\n        self.ensure_capacity(offset, len)?;\n\n        if let Some(ref mut mmap) = self.mmap {\n            let start = offset as usize;\n            let end = start + len;\n\n            if end <= mmap.len() {\n                return Ok(Some(&mut mmap[start..end]));\n            }\n        }\n\n        Ok(None)\n    }\n}\n\nimpl Drop for MmapStorageDevice {\n    fn drop(&mut self) {\n        let _ = self.flush();\n    }\n}\n\n// Address conversion utilities\n/// Convert Address to u64\n#[inline]\npub fn address_to_u64(addr: Address) -> u64 {\n    addr\n}\n\n/// Convert u64 to Address\n#[inline]\npub fn u64_to_address(val: u64) -> Address {\n    val\n}\n\n#[cfg(test)]\nmod tests {\n    use tempfile::tempdir;\n\n    use super::*;\n\n    /// Mock storage device for testing\n    struct MockStorageDevice {\n        data: Vec<u8>,\n    }\n\n    impl MockStorageDevice {\n        fn new() -> Self {\n            Self { data: Vec::new() }\n        }\n    }\n\n    impl StorageDevice for MockStorageDevice {\n        fn write(&mut self, offset: u64, data: &[u8]) -> Result<()> {\n            let end = offset as usize + data.len();\n            if self.data.len() < end {\n                self.data.resize(end, 0);\n            }\n            self.data[offset as usize..end].copy_from_slice(data);\n            Ok(())\n        }\n\n        fn read(&self, offset: u64, buf: &mut [u8]) -> Result<usize> {\n            let start = offset as usize;\n            let end = std::cmp::min(start + buf.len(), self.data.len());\n            if start < self.data.len() {\n                let copy_len = end - start;\n                buf[..copy_len].copy_from_slice(&self.data[start..end]);\n                Ok(copy_len)\n            } else {\n                Ok(0)\n            }\n        }\n\n        fn flush(&mut self) -> Result<()> {\n            Ok(())\n        }\n\n        fn size(&self) -> u64 {\n            self.data.len() as u64\n        }\n\n        fn truncate(&mut self, size: u64) -> Result<()> {\n            self.data.truncate(size as usize);\n            Ok(())\n        }\n    }\n\n    #[test]\n    fn test_atomic_page_offset() {\n        let offset = AtomicPageOffset::new(0, 100);\n        let (page, offset_val) = offset.load();\n        assert_eq!(page, 0);\n        assert_eq!(offset_val, 100);\n\n        let (old_page, old_offset) = offset.reserve(50);\n        assert_eq!(old_page, 0);\n        assert_eq!(old_offset, 100);\n\n        let (page, offset_val) = offset.load();\n        assert_eq!(page, 0);\n        assert_eq!(offset_val, 150);\n    }\n\n    #[test]\n    fn test_hybrid_log_creation() {\n        let storage = Box::new(MockStorageDevice::new());\n        let epoch = Arc::new(crate::epoch::EpochManager::new());\n        let memory_size = 64 * 1024 * 1024; // 64MB\n\n        let hlog = HybridLog::new(memory_size, storage, epoch).unwrap();\n        assert_eq!(hlog.buffer_size, 2); // 64MB / 32MB = 2 pages\n    }\n\n    #[test]\n    fn test_allocation() {\n        let storage = Box::new(MockStorageDevice::new());\n        let epoch = Arc::new(crate::epoch::EpochManager::new());\n        let memory_size = 64 * 1024 * 1024;\n\n        let hlog = HybridLog::new(memory_size, storage, epoch).unwrap();\n\n        // Allocate some space\n        let addr1 = hlog.allocate(1024).unwrap();\n        let addr2 = hlog.allocate(2048).unwrap();\n\n        assert_ne!(addr1, addr2);\n        assert!(get_offset(addr2) > get_offset(addr1));\n    }\n\n    #[test]\n    fn test_record_operations() {\n        let storage = Box::new(MockStorageDevice::new());\n        let epoch = Arc::new(crate::epoch::EpochManager::new());\n        let memory_size = 64 * 1024 * 1024;\n\n        let hlog = HybridLog::new(memory_size, storage, epoch).unwrap();\n\n        // Create and insert a record\n        let key = b\"test_key\".to_vec();\n        let value = b\"test_value\".to_vec();\n        let record = LogRecord::new(key.clone(), value.clone(), crate::common::INVALID_ADDRESS);\n\n        let address = hlog.insert_record(record).unwrap();\n\n        // Read the record back\n        let read_record = hlog.read_record(address).unwrap();\n        assert_eq!(read_record.key, key);\n        assert_eq!(read_record.value, value);\n    }\n\n    #[test]\n    fn test_file_storage_device() {\n        let temp_dir = tempdir().unwrap();\n        let file_path = temp_dir.path().join(\"test.log\");\n\n        let mut storage = FileStorageDevice::new(&file_path).unwrap();\n\n        let test_data = b\"Hello, World!\";\n        storage.write(0, test_data).unwrap();\n        storage.flush().unwrap();\n\n        let mut read_buffer = vec![0u8; test_data.len()];\n        let bytes_read = storage.read(0, &mut read_buffer).unwrap();\n\n        assert_eq!(bytes_read, test_data.len());\n        assert_eq!(&read_buffer, test_data);\n    }\n}\n","traces":[{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":21}},{"line":60,"address":[],"length":0,"stats":{"Line":63}},{"line":61,"address":[],"length":0,"stats":{"Line":42}},{"line":66,"address":[],"length":0,"stats":{"Line":42}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":2}},{"line":76,"address":[],"length":0,"stats":{"Line":6}},{"line":77,"address":[],"length":0,"stats":{"Line":2}},{"line":78,"address":[],"length":0,"stats":{"Line":2}},{"line":81,"address":[],"length":0,"stats":{"Line":1}},{"line":84,"address":[],"length":0,"stats":{"Line":2}},{"line":85,"address":[],"length":0,"stats":{"Line":3}},{"line":86,"address":[],"length":0,"stats":{"Line":1}},{"line":89,"address":[],"length":0,"stats":{"Line":2}},{"line":91,"address":[],"length":0,"stats":{"Line":4}},{"line":92,"address":[],"length":0,"stats":{"Line":2}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":24}},{"line":113,"address":[],"length":0,"stats":{"Line":96}},{"line":115,"address":[],"length":0,"stats":{"Line":24}},{"line":119,"address":[],"length":0,"stats":{"Line":40}},{"line":120,"address":[],"length":0,"stats":{"Line":160}},{"line":121,"address":[],"length":0,"stats":{"Line":120}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":17}},{"line":132,"address":[],"length":0,"stats":{"Line":85}},{"line":133,"address":[],"length":0,"stats":{"Line":51}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":13}},{"line":185,"address":[],"length":0,"stats":{"Line":39}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":1}},{"line":200,"address":[],"length":0,"stats":{"Line":3}},{"line":202,"address":[],"length":0,"stats":{"Line":1}},{"line":251,"address":[],"length":0,"stats":{"Line":23}},{"line":256,"address":[],"length":0,"stats":{"Line":46}},{"line":257,"address":[],"length":0,"stats":{"Line":23}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":41}},{"line":267,"address":[],"length":0,"stats":{"Line":41}},{"line":268,"address":[],"length":0,"stats":{"Line":41}},{"line":291,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":23}},{"line":298,"address":[],"length":0,"stats":{"Line":16}},{"line":299,"address":[],"length":0,"stats":{"Line":32}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[],"length":0,"stats":{"Line":64}},{"line":305,"address":[],"length":0,"stats":{"Line":32}},{"line":307,"address":[],"length":0,"stats":{"Line":16}},{"line":309,"address":[],"length":0,"stats":{"Line":64}},{"line":312,"address":[],"length":0,"stats":{"Line":48}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}},{"line":344,"address":[],"length":0,"stats":{"Line":16}},{"line":345,"address":[],"length":0,"stats":{"Line":48}},{"line":346,"address":[],"length":0,"stats":{"Line":48}},{"line":348,"address":[],"length":0,"stats":{"Line":32}},{"line":349,"address":[],"length":0,"stats":{"Line":48}},{"line":351,"address":[],"length":0,"stats":{"Line":32}},{"line":352,"address":[],"length":0,"stats":{"Line":32}},{"line":356,"address":[],"length":0,"stats":{"Line":80}},{"line":357,"address":[],"length":0,"stats":{"Line":32}},{"line":358,"address":[],"length":0,"stats":{"Line":32}},{"line":359,"address":[],"length":0,"stats":{"Line":32}},{"line":368,"address":[],"length":0,"stats":{"Line":14}},{"line":369,"address":[],"length":0,"stats":{"Line":42}},{"line":370,"address":[],"length":0,"stats":{"Line":42}},{"line":372,"address":[],"length":0,"stats":{"Line":28}},{"line":373,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":14}},{"line":386,"address":[],"length":0,"stats":{"Line":56}},{"line":387,"address":[],"length":0,"stats":{"Line":14}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":14}},{"line":397,"address":[],"length":0,"stats":{"Line":42}},{"line":401,"address":[],"length":0,"stats":{"Line":14}},{"line":403,"address":[],"length":0,"stats":{"Line":0}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":14}},{"line":412,"address":[],"length":0,"stats":{"Line":16}},{"line":414,"address":[],"length":0,"stats":{"Line":48}},{"line":417,"address":[],"length":0,"stats":{"Line":16}},{"line":418,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":445,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":449,"address":[],"length":0,"stats":{"Line":0}},{"line":452,"address":[],"length":0,"stats":{"Line":0}},{"line":462,"address":[],"length":0,"stats":{"Line":39}},{"line":463,"address":[],"length":0,"stats":{"Line":78}},{"line":465,"address":[],"length":0,"stats":{"Line":117}},{"line":466,"address":[],"length":0,"stats":{"Line":62}},{"line":468,"address":[],"length":0,"stats":{"Line":115}},{"line":469,"address":[],"length":0,"stats":{"Line":69}},{"line":472,"address":[],"length":0,"stats":{"Line":92}},{"line":473,"address":[],"length":0,"stats":{"Line":23}},{"line":476,"address":[],"length":0,"stats":{"Line":39}},{"line":481,"address":[],"length":0,"stats":{"Line":15}},{"line":482,"address":[],"length":0,"stats":{"Line":45}},{"line":483,"address":[],"length":0,"stats":{"Line":30}},{"line":484,"address":[],"length":0,"stats":{"Line":15}},{"line":485,"address":[],"length":0,"stats":{"Line":60}},{"line":486,"address":[],"length":0,"stats":{"Line":30}},{"line":491,"address":[],"length":0,"stats":{"Line":0}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":493,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":495,"address":[],"length":0,"stats":{"Line":0}},{"line":498,"address":[],"length":0,"stats":{"Line":0}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":501,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":511,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":515,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":0}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":529,"address":[],"length":0,"stats":{"Line":0}},{"line":531,"address":[],"length":0,"stats":{"Line":0}},{"line":533,"address":[],"length":0,"stats":{"Line":0}},{"line":534,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":540,"address":[],"length":0,"stats":{"Line":0}},{"line":544,"address":[],"length":0,"stats":{"Line":38}},{"line":545,"address":[],"length":0,"stats":{"Line":114}},{"line":546,"address":[],"length":0,"stats":{"Line":114}},{"line":550,"address":[],"length":0,"stats":{"Line":25}},{"line":551,"address":[],"length":0,"stats":{"Line":100}},{"line":555,"address":[],"length":0,"stats":{"Line":2}},{"line":556,"address":[],"length":0,"stats":{"Line":8}},{"line":560,"address":[],"length":0,"stats":{"Line":25}},{"line":561,"address":[],"length":0,"stats":{"Line":100}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":0}},{"line":568,"address":[],"length":0,"stats":{"Line":0}},{"line":569,"address":[],"length":0,"stats":{"Line":0}},{"line":570,"address":[],"length":0,"stats":{"Line":0}},{"line":572,"address":[],"length":0,"stats":{"Line":0}},{"line":574,"address":[],"length":0,"stats":{"Line":0}},{"line":581,"address":[],"length":0,"stats":{"Line":0}},{"line":583,"address":[],"length":0,"stats":{"Line":0}},{"line":584,"address":[],"length":0,"stats":{"Line":0}},{"line":594,"address":[],"length":0,"stats":{"Line":0}},{"line":595,"address":[],"length":0,"stats":{"Line":0}},{"line":599,"address":[],"length":0,"stats":{"Line":0}},{"line":601,"address":[],"length":0,"stats":{"Line":0}},{"line":605,"address":[],"length":0,"stats":{"Line":0}},{"line":608,"address":[],"length":0,"stats":{"Line":0}},{"line":612,"address":[],"length":0,"stats":{"Line":0}},{"line":619,"address":[],"length":0,"stats":{"Line":0}},{"line":620,"address":[],"length":0,"stats":{"Line":0}},{"line":624,"address":[],"length":0,"stats":{"Line":0}},{"line":631,"address":[],"length":0,"stats":{"Line":0}},{"line":633,"address":[],"length":0,"stats":{"Line":0}},{"line":634,"address":[],"length":0,"stats":{"Line":0}},{"line":636,"address":[],"length":0,"stats":{"Line":0}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":639,"address":[],"length":0,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[],"length":0,"stats":{"Line":0}},{"line":648,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":0}},{"line":654,"address":[],"length":0,"stats":{"Line":0}},{"line":656,"address":[],"length":0,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":666,"address":[],"length":0,"stats":{"Line":0}},{"line":675,"address":[],"length":0,"stats":{"Line":30}},{"line":676,"address":[],"length":0,"stats":{"Line":75}},{"line":678,"address":[],"length":0,"stats":{"Line":15}},{"line":680,"address":[],"length":0,"stats":{"Line":14}},{"line":684,"address":[],"length":0,"stats":{"Line":0}},{"line":693,"address":[],"length":0,"stats":{"Line":1}},{"line":694,"address":[],"length":0,"stats":{"Line":3}},{"line":698,"address":[],"length":0,"stats":{"Line":1}},{"line":704,"address":[],"length":0,"stats":{"Line":0}},{"line":707,"address":[],"length":0,"stats":{"Line":1}},{"line":712,"address":[],"length":0,"stats":{"Line":2}},{"line":713,"address":[],"length":0,"stats":{"Line":2}},{"line":716,"address":[],"length":0,"stats":{"Line":1}},{"line":717,"address":[],"length":0,"stats":{"Line":3}},{"line":718,"address":[],"length":0,"stats":{"Line":3}},{"line":720,"address":[],"length":0,"stats":{"Line":1}},{"line":722,"address":[],"length":0,"stats":{"Line":0}},{"line":728,"address":[],"length":0,"stats":{"Line":0}},{"line":738,"address":[],"length":0,"stats":{"Line":0}},{"line":743,"address":[],"length":0,"stats":{"Line":1}},{"line":744,"address":[],"length":0,"stats":{"Line":1}},{"line":745,"address":[],"length":0,"stats":{"Line":1}},{"line":750,"address":[],"length":0,"stats":{"Line":0}},{"line":770,"address":[],"length":0,"stats":{"Line":0}},{"line":771,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":777,"address":[],"length":0,"stats":{"Line":0}},{"line":779,"address":[],"length":0,"stats":{"Line":0}},{"line":780,"address":[],"length":0,"stats":{"Line":0}},{"line":791,"address":[],"length":0,"stats":{"Line":0}},{"line":792,"address":[],"length":0,"stats":{"Line":0}},{"line":795,"address":[],"length":0,"stats":{"Line":0}},{"line":799,"address":[],"length":0,"stats":{"Line":0}},{"line":800,"address":[],"length":0,"stats":{"Line":0}},{"line":802,"address":[],"length":0,"stats":{"Line":0}},{"line":803,"address":[],"length":0,"stats":{"Line":0}},{"line":804,"address":[],"length":0,"stats":{"Line":0}},{"line":806,"address":[],"length":0,"stats":{"Line":0}},{"line":808,"address":[],"length":0,"stats":{"Line":0}},{"line":812,"address":[],"length":0,"stats":{"Line":0}},{"line":813,"address":[],"length":0,"stats":{"Line":0}},{"line":815,"address":[],"length":0,"stats":{"Line":0}},{"line":818,"address":[],"length":0,"stats":{"Line":0}},{"line":819,"address":[],"length":0,"stats":{"Line":0}},{"line":822,"address":[],"length":0,"stats":{"Line":0}},{"line":823,"address":[],"length":0,"stats":{"Line":0}},{"line":826,"address":[],"length":0,"stats":{"Line":0}},{"line":830,"address":[],"length":0,"stats":{"Line":0}},{"line":831,"address":[],"length":0,"stats":{"Line":0}},{"line":832,"address":[],"length":0,"stats":{"Line":0}},{"line":835,"address":[],"length":0,"stats":{"Line":0}},{"line":836,"address":[],"length":0,"stats":{"Line":0}},{"line":838,"address":[],"length":0,"stats":{"Line":0}},{"line":843,"address":[],"length":0,"stats":{"Line":0}},{"line":844,"address":[],"length":0,"stats":{"Line":0}},{"line":846,"address":[],"length":0,"stats":{"Line":0}},{"line":847,"address":[],"length":0,"stats":{"Line":0}},{"line":848,"address":[],"length":0,"stats":{"Line":0}},{"line":850,"address":[],"length":0,"stats":{"Line":0}},{"line":851,"address":[],"length":0,"stats":{"Line":0}},{"line":852,"address":[],"length":0,"stats":{"Line":0}},{"line":853,"address":[],"length":0,"stats":{"Line":0}},{"line":859,"address":[],"length":0,"stats":{"Line":0}},{"line":860,"address":[],"length":0,"stats":{"Line":0}},{"line":861,"address":[],"length":0,"stats":{"Line":0}},{"line":864,"address":[],"length":0,"stats":{"Line":0}},{"line":865,"address":[],"length":0,"stats":{"Line":0}},{"line":866,"address":[],"length":0,"stats":{"Line":0}},{"line":867,"address":[],"length":0,"stats":{"Line":0}},{"line":869,"address":[],"length":0,"stats":{"Line":0}},{"line":870,"address":[],"length":0,"stats":{"Line":0}},{"line":871,"address":[],"length":0,"stats":{"Line":0}},{"line":877,"address":[],"length":0,"stats":{"Line":0}},{"line":878,"address":[],"length":0,"stats":{"Line":0}},{"line":879,"address":[],"length":0,"stats":{"Line":0}},{"line":882,"address":[],"length":0,"stats":{"Line":0}},{"line":883,"address":[],"length":0,"stats":{"Line":0}},{"line":884,"address":[],"length":0,"stats":{"Line":0}},{"line":885,"address":[],"length":0,"stats":{"Line":0}},{"line":887,"address":[],"length":0,"stats":{"Line":0}},{"line":888,"address":[],"length":0,"stats":{"Line":0}},{"line":890,"address":[],"length":0,"stats":{"Line":0}},{"line":893,"address":[],"length":0,"stats":{"Line":0}},{"line":894,"address":[],"length":0,"stats":{"Line":0}},{"line":897,"address":[],"length":0,"stats":{"Line":0}},{"line":898,"address":[],"length":0,"stats":{"Line":0}},{"line":899,"address":[],"length":0,"stats":{"Line":0}},{"line":902,"address":[],"length":0,"stats":{"Line":0}},{"line":903,"address":[],"length":0,"stats":{"Line":0}},{"line":906,"address":[],"length":0,"stats":{"Line":0}},{"line":907,"address":[],"length":0,"stats":{"Line":0}},{"line":909,"address":[],"length":0,"stats":{"Line":0}},{"line":910,"address":[],"length":0,"stats":{"Line":0}},{"line":911,"address":[],"length":0,"stats":{"Line":0}},{"line":913,"address":[],"length":0,"stats":{"Line":0}},{"line":914,"address":[],"length":0,"stats":{"Line":0}},{"line":918,"address":[],"length":0,"stats":{"Line":0}},{"line":923,"address":[],"length":0,"stats":{"Line":0}},{"line":924,"address":[],"length":0,"stats":{"Line":0}},{"line":931,"address":[],"length":0,"stats":{"Line":108}},{"line":932,"address":[],"length":0,"stats":{"Line":108}},{"line":937,"address":[],"length":0,"stats":{"Line":105}},{"line":938,"address":[],"length":0,"stats":{"Line":105}}],"covered":113,"coverable":304},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","index.rs"],"content":"//! Concurrent hash index implementation for rskv\n//!\n//! This module provides a thread-safe hash index for mapping keys to their\n//! addresses in the hybrid log. It's inspired by FASTER's MemHashIndex design.\n\nuse std::hash::Hasher;\nuse std::sync::Arc;\n\nuse ahash::AHasher;\nuse dashmap::DashMap;\n\nuse crate::common::{Address, Key};\nuse crate::epoch::SharedEpochManager;\n\n/// Custom hasher for better performance with binary keys\npub struct KeyHasher {\n    #[allow(dead_code)]\n    hasher: AHasher,\n}\n\nimpl KeyHasher {\n    pub fn new() -> Self {\n        Self {\n            hasher: AHasher::default(),\n        }\n    }\n\n    pub fn hash_key(key: &[u8]) -> u64 {\n        let mut hasher = AHasher::default();\n        hasher.write(key);\n        hasher.finish()\n    }\n}\n\nimpl Default for KeyHasher {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Hash bucket entry containing the key-address mapping\n#[derive(Debug, Clone)]\npub struct HashBucketEntry {\n    /// The key\n    pub key: Key,\n    /// Address pointing to the latest version of the value in the log\n    pub address: Address,\n    /// Hash of the key for quick comparison\n    pub key_hash: u64,\n}\n\nimpl HashBucketEntry {\n    pub fn new(key: Key, address: Address) -> Self {\n        let key_hash = KeyHasher::hash_key(&key);\n        Self {\n            key,\n            address,\n            key_hash,\n        }\n    }\n\n    /// Check if this entry matches the given key\n    pub fn matches_key(&self, key: &[u8]) -> bool {\n        // First check hash for quick rejection\n        let other_hash = KeyHasher::hash_key(key);\n        if self.key_hash != other_hash {\n            return false;\n        }\n\n        // Then check actual key content\n        self.key == key\n    }\n}\n\n/// Memory-based concurrent hash index\n///\n/// This is the main index structure that maps keys to their latest addresses\n/// in the hybrid log. It uses DashMap for thread-safe concurrent access.\npub struct MemHashIndex {\n    /// Internal hash map using DashMap for lock-free concurrent access\n    map: DashMap<Key, Address, ahash::RandomState>,\n\n    /// Epoch manager for safe memory reclamation (currently unused but kept for future optimization)\n    #[allow(dead_code)]\n    epoch: SharedEpochManager,\n}\n\nimpl MemHashIndex {\n    /// Create a new memory hash index\n    pub fn new(epoch: SharedEpochManager) -> Self {\n        Self {\n            map: DashMap::with_hasher(ahash::RandomState::new()),\n            epoch,\n        }\n    }\n\n    /// Create a new memory hash index with specified capacity\n    pub fn with_capacity(capacity: usize, epoch: SharedEpochManager) -> Self {\n        Self {\n            map: DashMap::with_capacity_and_hasher(capacity, ahash::RandomState::new()),\n            epoch,\n        }\n    }\n\n    /// Find the address for a given key\n    /// Returns None if the key is not found\n    pub fn find(&self, key: &Key) -> Option<Address> {\n        self.map.get(key).map(|entry| *entry.value())\n    }\n\n    /// Insert or update a key-address mapping\n    /// This will overwrite any existing mapping for the key\n    pub fn insert(&self, key: Key, address: Address) {\n        self.map.insert(key, address);\n    }\n\n    /// Insert a key-address mapping only if the key doesn't exist\n    /// Returns true if the insertion was successful, false if key already exists\n    pub fn insert_if_not_exists(&self, key: Key, address: Address) -> bool {\n        // Use entry API to check and insert atomically\n        use dashmap::mapref::entry::Entry;\n\n        match self.map.entry(key) {\n            Entry::Occupied(_) => false, // Key already exists\n            Entry::Vacant(entry) => {\n                entry.insert(address);\n                true // Insertion successful\n            }\n        }\n    }\n\n    /// Update an existing key-address mapping using compare-and-swap\n    /// Returns true if the update was successful\n    pub fn update_if_exists(&self, key: &Key, old_address: Address, new_address: Address) -> bool {\n        if let Some(mut entry) = self.map.get_mut(key)\n            && *entry.value() == old_address\n        {\n            *entry.value_mut() = new_address;\n            return true;\n        }\n        false\n    }\n\n    /// Remove a key from the index\n    /// Returns the old address if the key was found and removed\n    pub fn remove(&self, key: &Key) -> Option<Address> {\n        self.map.remove(key).map(|(_, address)| address)\n    }\n\n    /// Remove a key only if it currently maps to the specified address\n    /// This is useful for conditional removals during garbage collection\n    pub fn remove_if_address(&self, key: &Key, expected_address: Address) -> bool {\n        if let Some(entry) = self.map.get(key) {\n            if *entry.value() == expected_address {\n                drop(entry);\n                self.map.remove(key).is_some()\n            } else {\n                false\n            }\n        } else {\n            false\n        }\n    }\n\n    /// Get the number of entries in the index\n    pub fn len(&self) -> usize {\n        self.map.len()\n    }\n\n    /// Check if the index is empty\n    pub fn is_empty(&self) -> bool {\n        self.map.is_empty()\n    }\n\n    /// Clear all entries from the index\n    pub fn clear(&self) {\n        self.map.clear();\n    }\n\n    /// Iterate over all key-address pairs\n    /// The provided closure will be called for each entry\n    pub fn for_each<F>(&self, mut f: F)\n    where\n        F: FnMut(&Key, Address),\n    {\n        for entry in &self.map {\n            f(entry.key(), *entry.value());\n        }\n    }\n\n    /// Iterate over entries and collect those that match a predicate\n    /// This is useful for operations like garbage collection\n    pub fn collect_matching<F>(&self, predicate: F) -> Vec<(Key, Address)>\n    where\n        F: Fn(&Key, Address) -> bool,\n    {\n        let mut result = Vec::new();\n        for entry in &self.map {\n            let key = entry.key();\n            let address = *entry.value();\n            if predicate(key, address) {\n                result.push((key.clone(), address));\n            }\n        }\n        result\n    }\n\n    /// Remove entries that match a predicate\n    /// Returns the number of entries removed\n    pub fn remove_matching<F>(&self, predicate: F) -> usize\n    where\n        F: Fn(&Key, Address) -> bool,\n    {\n        let mut removed_count = 0;\n\n        // Collect keys to remove first to avoid holding locks during iteration\n        let keys_to_remove: Vec<Key> = self\n            .map\n            .iter()\n            .filter_map(|entry| {\n                let key = entry.key();\n                let address = *entry.value();\n                if predicate(key, address) {\n                    Some(key.clone())\n                } else {\n                    None\n                }\n            })\n            .collect();\n\n        // Remove the collected keys\n        for key in keys_to_remove {\n            if self.map.remove(&key).is_some() {\n                removed_count += 1;\n            }\n        }\n\n        removed_count\n    }\n\n    /// Create a snapshot of the current index state\n    /// This is useful for checkpointing\n    pub fn snapshot(&self) -> Vec<(Key, Address)> {\n        self.map\n            .iter()\n            .map(|entry| (entry.key().clone(), *entry.value()))\n            .collect()\n    }\n\n    /// Restore the index from a snapshot\n    /// This will clear the current index and load the snapshot data\n    pub fn restore_from_snapshot(&self, snapshot: Vec<(Key, Address)>) {\n        self.clear();\n        for (key, address) in snapshot {\n            self.insert(key, address);\n        }\n    }\n\n    /// Get memory usage statistics\n    pub fn memory_usage(&self) -> IndexMemoryStats {\n        let entry_count = self.len();\n\n        // Estimate memory usage\n        // DashMap overhead + (Key + Address + metadata) per entry\n        let dashmap_overhead = std::mem::size_of::<DashMap<Key, Address>>();\n\n        let mut total_key_size = 0;\n        for entry in &self.map {\n            total_key_size += entry.key().capacity();\n        }\n\n        let address_size = entry_count * std::mem::size_of::<Address>();\n        let estimated_overhead = entry_count * 64; // Rough estimate for DashMap overhead per entry\n\n        IndexMemoryStats {\n            entry_count,\n            total_key_size,\n            address_size,\n            estimated_overhead: dashmap_overhead + estimated_overhead,\n            total_estimated_size: dashmap_overhead\n                + total_key_size\n                + address_size\n                + estimated_overhead,\n        }\n    }\n}\n\n/// Memory usage statistics for the hash index\n#[derive(Debug, Clone)]\npub struct IndexMemoryStats {\n    /// Number of entries in the index\n    pub entry_count: usize,\n    /// Total size of all keys in bytes\n    pub total_key_size: usize,\n    /// Total size of all addresses in bytes\n    pub address_size: usize,\n    /// Estimated overhead from the hash map structure\n    pub estimated_overhead: usize,\n    /// Total estimated memory usage in bytes\n    pub total_estimated_size: usize,\n}\n\n/// Shared reference to a memory hash index\npub type SharedMemHashIndex = Arc<MemHashIndex>;\n\n/// Create a new shared memory hash index\npub fn new_shared_mem_hash_index(epoch: SharedEpochManager) -> SharedMemHashIndex {\n    Arc::new(MemHashIndex::new(epoch))\n}\n\n/// Create a new shared memory hash index with specified capacity\npub fn new_shared_mem_hash_index_with_capacity(\n    capacity: usize,\n    epoch: SharedEpochManager,\n) -> SharedMemHashIndex {\n    Arc::new(MemHashIndex::with_capacity(capacity, epoch))\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::epoch::EpochManager;\n\n    #[test]\n    fn test_key_hasher() {\n        let key1 = b\"hello\";\n        let key2 = b\"world\";\n        let key3 = b\"hello\";\n\n        let hash1 = KeyHasher::hash_key(key1);\n        let hash2 = KeyHasher::hash_key(key2);\n        let hash3 = KeyHasher::hash_key(key3);\n\n        assert_eq!(hash1, hash3);\n        assert_ne!(hash1, hash2);\n    }\n\n    #[test]\n    fn test_hash_bucket_entry() {\n        let key = b\"test_key\".to_vec();\n        let address = 12345u64;\n\n        let entry = HashBucketEntry::new(key.clone(), address);\n\n        assert!(entry.matches_key(&key));\n        assert!(!entry.matches_key(b\"other_key\"));\n        assert_eq!(entry.address, address);\n    }\n\n    #[test]\n    fn test_mem_hash_index_basic_operations() {\n        let epoch = Arc::new(EpochManager::new());\n        let index = MemHashIndex::new(epoch);\n\n        let key1 = b\"key1\".to_vec();\n        let key2 = b\"key2\".to_vec();\n        let addr1 = 100u64;\n        let addr2 = 200u64;\n\n        // Test insertion\n        index.insert(key1.clone(), addr1);\n        index.insert(key2.clone(), addr2);\n\n        // Test finding\n        assert_eq!(index.find(&key1), Some(addr1));\n        assert_eq!(index.find(&key2), Some(addr2));\n        assert_eq!(index.find(&b\"nonexistent\".to_vec()), None);\n\n        // Test length\n        assert_eq!(index.len(), 2);\n        assert!(!index.is_empty());\n\n        // Test removal\n        assert_eq!(index.remove(&key1), Some(addr1));\n        assert_eq!(index.find(&key1), None);\n        assert_eq!(index.len(), 1);\n\n        // Test clear\n        index.clear();\n        assert_eq!(index.len(), 0);\n        assert!(index.is_empty());\n    }\n\n    #[test]\n    fn test_mem_hash_index_conditional_operations() {\n        let epoch = Arc::new(EpochManager::new());\n        let index = MemHashIndex::new(epoch);\n\n        let key = b\"test_key\".to_vec();\n        let addr1 = 100u64;\n        let addr2 = 200u64;\n\n        // Test insert_if_not_exists\n        assert!(index.insert_if_not_exists(key.clone(), addr1));\n        assert!(!index.insert_if_not_exists(key.clone(), addr2)); // Should fail\n        assert_eq!(index.find(&key), Some(addr1));\n\n        // Test update_if_exists\n        assert!(index.update_if_exists(&key, addr1, addr2));\n        assert_eq!(index.find(&key), Some(addr2));\n        assert!(!index.update_if_exists(&key, addr1, 300u64)); // Should fail\n\n        // Test remove_if_address\n        assert!(!index.remove_if_address(&key, addr1)); // Should fail\n        assert!(index.remove_if_address(&key, addr2)); // Should succeed\n        assert_eq!(index.find(&key), None);\n    }\n\n    #[test]\n    fn test_mem_hash_index_iteration() {\n        let epoch = Arc::new(EpochManager::new());\n        let index = MemHashIndex::new(epoch);\n\n        let entries = vec![\n            (b\"key1\".to_vec(), 100u64),\n            (b\"key2\".to_vec(), 200u64),\n            (b\"key3\".to_vec(), 300u64),\n        ];\n\n        // Insert test data\n        for (key, addr) in &entries {\n            index.insert(key.clone(), *addr);\n        }\n\n        // Test for_each\n        let mut collected = Vec::new();\n        index.for_each(|key, addr| {\n            collected.push((key.clone(), addr));\n        });\n        assert_eq!(collected.len(), 3);\n\n        // Test collect_matching\n        let filtered = index.collect_matching(|_key, addr| addr > 150u64);\n        assert_eq!(filtered.len(), 2);\n\n        // Test remove_matching\n        let removed_count = index.remove_matching(|_key, addr| addr > 150u64);\n        assert_eq!(removed_count, 2);\n        assert_eq!(index.len(), 1);\n    }\n\n    #[test]\n    fn test_mem_hash_index_snapshot() {\n        let epoch = Arc::new(EpochManager::new());\n        let index = MemHashIndex::new(epoch);\n\n        let entries = vec![(b\"key1\".to_vec(), 100u64), (b\"key2\".to_vec(), 200u64)];\n\n        // Insert test data\n        for (key, addr) in &entries {\n            index.insert(key.clone(), *addr);\n        }\n\n        // Create snapshot\n        let snapshot = index.snapshot();\n        assert_eq!(snapshot.len(), 2);\n\n        // Clear and restore\n        index.clear();\n        assert!(index.is_empty());\n\n        index.restore_from_snapshot(snapshot);\n        assert_eq!(index.len(), 2);\n\n        // Verify data is restored correctly\n        for (key, addr) in &entries {\n            assert_eq!(index.find(key), Some(*addr));\n        }\n    }\n\n    #[test]\n    fn test_memory_stats() {\n        let epoch = Arc::new(EpochManager::new());\n        let index = MemHashIndex::new(epoch);\n\n        // Insert some test data\n        for i in 0..100 {\n            let key = format!(\"key_{}\", i).into_bytes();\n            index.insert(key, i as u64);\n        }\n\n        let stats = index.memory_usage();\n        assert_eq!(stats.entry_count, 100);\n        assert!(stats.total_key_size > 0);\n        assert!(stats.address_size > 0);\n        assert!(stats.total_estimated_size > 0);\n    }\n\n    #[test]\n    fn test_shared_index() {\n        let epoch = Arc::new(EpochManager::new());\n        let index: SharedMemHashIndex = new_shared_mem_hash_index(epoch);\n\n        let key = b\"test\".to_vec();\n        let addr = 42u64;\n\n        index.insert(key.clone(), addr);\n        assert_eq!(index.find(&key), Some(addr));\n        assert_eq!(index.len(), 1);\n\n        index.remove(&key);\n        assert_eq!(index.find(&key), None);\n        assert!(index.is_empty());\n    }\n}\n","traces":[{"line":22,"address":[],"length":0,"stats":{"Line":0}},{"line":24,"address":[],"length":0,"stats":{"Line":0}},{"line":28,"address":[],"length":0,"stats":{"Line":6}},{"line":29,"address":[],"length":0,"stats":{"Line":12}},{"line":30,"address":[],"length":0,"stats":{"Line":18}},{"line":31,"address":[],"length":0,"stats":{"Line":12}},{"line":36,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":1}},{"line":54,"address":[],"length":0,"stats":{"Line":3}},{"line":63,"address":[],"length":0,"stats":{"Line":2}},{"line":65,"address":[],"length":0,"stats":{"Line":6}},{"line":66,"address":[],"length":0,"stats":{"Line":2}},{"line":67,"address":[],"length":0,"stats":{"Line":1}},{"line":90,"address":[],"length":0,"stats":{"Line":20}},{"line":92,"address":[],"length":0,"stats":{"Line":40}},{"line":98,"address":[],"length":0,"stats":{"Line":6}},{"line":100,"address":[],"length":0,"stats":{"Line":18}},{"line":107,"address":[],"length":0,"stats":{"Line":186}},{"line":108,"address":[],"length":0,"stats":{"Line":1086}},{"line":113,"address":[],"length":0,"stats":{"Line":291}},{"line":114,"address":[],"length":0,"stats":{"Line":1164}},{"line":119,"address":[],"length":0,"stats":{"Line":2}},{"line":123,"address":[],"length":0,"stats":{"Line":4}},{"line":124,"address":[],"length":0,"stats":{"Line":1}},{"line":125,"address":[],"length":0,"stats":{"Line":1}},{"line":126,"address":[],"length":0,"stats":{"Line":3}},{"line":127,"address":[],"length":0,"stats":{"Line":1}},{"line":134,"address":[],"length":0,"stats":{"Line":2}},{"line":135,"address":[],"length":0,"stats":{"Line":6}},{"line":136,"address":[],"length":0,"stats":{"Line":2}},{"line":138,"address":[],"length":0,"stats":{"Line":2}},{"line":139,"address":[],"length":0,"stats":{"Line":1}},{"line":146,"address":[],"length":0,"stats":{"Line":2}},{"line":147,"address":[],"length":0,"stats":{"Line":8}},{"line":152,"address":[],"length":0,"stats":{"Line":2}},{"line":153,"address":[],"length":0,"stats":{"Line":6}},{"line":155,"address":[],"length":0,"stats":{"Line":2}},{"line":156,"address":[],"length":0,"stats":{"Line":3}},{"line":158,"address":[],"length":0,"stats":{"Line":1}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":12}},{"line":167,"address":[],"length":0,"stats":{"Line":24}},{"line":171,"address":[],"length":0,"stats":{"Line":5}},{"line":172,"address":[],"length":0,"stats":{"Line":10}},{"line":176,"address":[],"length":0,"stats":{"Line":5}},{"line":177,"address":[],"length":0,"stats":{"Line":10}},{"line":182,"address":[],"length":0,"stats":{"Line":10}},{"line":186,"address":[],"length":0,"stats":{"Line":236}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":1}},{"line":197,"address":[],"length":0,"stats":{"Line":2}},{"line":198,"address":[],"length":0,"stats":{"Line":7}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":2}},{"line":202,"address":[],"length":0,"stats":{"Line":8}},{"line":205,"address":[],"length":0,"stats":{"Line":1}},{"line":210,"address":[],"length":0,"stats":{"Line":1}},{"line":214,"address":[],"length":0,"stats":{"Line":2}},{"line":217,"address":[],"length":0,"stats":{"Line":3}},{"line":218,"address":[],"length":0,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":4}},{"line":221,"address":[],"length":0,"stats":{"Line":9}},{"line":222,"address":[],"length":0,"stats":{"Line":6}},{"line":223,"address":[],"length":0,"stats":{"Line":6}},{"line":224,"address":[],"length":0,"stats":{"Line":2}},{"line":226,"address":[],"length":0,"stats":{"Line":1}},{"line":232,"address":[],"length":0,"stats":{"Line":5}},{"line":233,"address":[],"length":0,"stats":{"Line":2}},{"line":234,"address":[],"length":0,"stats":{"Line":2}},{"line":238,"address":[],"length":0,"stats":{"Line":1}},{"line":243,"address":[],"length":0,"stats":{"Line":17}},{"line":244,"address":[],"length":0,"stats":{"Line":17}},{"line":246,"address":[],"length":0,"stats":{"Line":389}},{"line":252,"address":[],"length":0,"stats":{"Line":2}},{"line":253,"address":[],"length":0,"stats":{"Line":4}},{"line":254,"address":[],"length":0,"stats":{"Line":10}},{"line":260,"address":[],"length":0,"stats":{"Line":1}},{"line":261,"address":[],"length":0,"stats":{"Line":3}},{"line":265,"address":[],"length":0,"stats":{"Line":2}},{"line":267,"address":[],"length":0,"stats":{"Line":2}},{"line":268,"address":[],"length":0,"stats":{"Line":201}},{"line":272,"address":[],"length":0,"stats":{"Line":2}},{"line":273,"address":[],"length":0,"stats":{"Line":2}},{"line":279,"address":[],"length":0,"stats":{"Line":2}},{"line":280,"address":[],"length":0,"stats":{"Line":1}},{"line":307,"address":[],"length":0,"stats":{"Line":15}},{"line":308,"address":[],"length":0,"stats":{"Line":45}},{"line":312,"address":[],"length":0,"stats":{"Line":6}},{"line":316,"address":[],"length":0,"stats":{"Line":24}}],"covered":83,"coverable":91},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","lib.rs"],"content":"//! # rskv: A High-Performance Key-Value Store in Rust\n//!\n//! `rskv` is a high-performance, concurrent, persistent key-value store inspired by\n//! Microsoft's FASTER. It leverages modern Rust features for safety and performance.\n//!\n//! ## Core Features\n//!\n//! - **Hybrid Storage Engine**: Combines in-memory hot data with disk-backed log\n//! - **Concurrent Hash Index**: Lock-free hash index for fast key lookups\n//! - **Non-Blocking Checkpoints**: Consistent snapshots without pausing operations\n//! - **Epoch-Based Garbage Collection**: Safe background space reclamation\n//!\n//! ## Example\n//!\n//! ```rust,ignore\n//! use rskv::{RsKv, Config};\n//!\n//! #[tokio::main]\n//! async fn main() -> Result<(), Box<dyn std::error::Error>> {\n//!     let config = Config::default();\n//!     let kv_store = RsKv::new(config).await?;\n//!     \n//!     let key = b\"hello\".to_vec();\n//!     let value = b\"world\".to_vec();\n//!     \n//!     kv_store.upsert(key.clone(), value).await?;\n//!     let result = kv_store.read(&key).await?;\n//!     \n//!     println!(\"Value: {:?}\", result);\n//!     Ok(())\n//! }\n//! ```\n\n#![allow(clippy::uninlined_format_args)]\n\npub mod background;\npub mod checkpoint;\npub mod common;\npub mod epoch;\npub mod gc;\npub mod hlog;\npub mod index;\npub mod metrics;\npub mod rskv;\n\n// Re-export commonly used types\npub use background::{BackgroundTaskManager, BackgroundTaskStats};\npub use checkpoint::{CheckpointMetadata, CheckpointState, CheckpointStats};\npub use common::{Address, Config, Key, Result, RsKvError, Value};\npub use epoch::{EpochHandle, EpochManager, SharedEpochManager};\npub use gc::{GcConfig, GcEstimate, GcState, GcStats};\npub use metrics::{\n    MetricsCollector, MetricsSnapshot, SharedMetricsCollector, new_shared_metrics_collector,\n};\n// Re-export main types\npub use rskv::{RsKv, RsKvStats};\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","metrics.rs"],"content":"//! Performance metrics collection for rskv\n//!\n//! This module provides comprehensive performance monitoring capabilities\n//! including operation counters, latency tracking, and resource utilization.\n\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};\nuse std::time::{Duration, Instant};\n\nuse parking_lot::RwLock;\nuse serde::{Deserialize, Serialize};\n\n/// Global metrics collector for the rskv system\n#[derive(Debug)]\npub struct MetricsCollector {\n    /// Operation counters\n    operations: OperationMetrics,\n    /// Latency tracking\n    latency: LatencyMetrics,\n    /// Storage metrics\n    storage: StorageMetrics,\n    /// Memory metrics\n    memory: MemoryMetrics,\n    /// Background task metrics\n    background: BackgroundMetrics,\n    /// Error metrics\n    errors: ErrorMetrics,\n    /// Start time for uptime calculation\n    start_time: Instant,\n}\n\n/// Operation-specific metrics\n#[derive(Debug, Default)]\npub struct OperationMetrics {\n    /// Total read operations\n    pub reads_total: AtomicU64,\n    /// Total write operations  \n    pub writes_total: AtomicU64,\n    /// Total delete operations\n    pub deletes_total: AtomicU64,\n    /// Total scan operations\n    pub scans_total: AtomicU64,\n    /// Read cache hits\n    pub read_cache_hits: AtomicU64,\n    /// Read cache misses\n    pub read_cache_misses: AtomicU64,\n    /// Bytes read\n    pub bytes_read: AtomicU64,\n    /// Bytes written\n    pub bytes_written: AtomicU64,\n}\n\n/// Latency tracking metrics\n#[derive(Debug)]\npub struct LatencyMetrics {\n    /// Read operation latencies (in microseconds)\n    pub read_latencies: RwLock<LatencyHistogram>,\n    /// Write operation latencies\n    pub write_latencies: RwLock<LatencyHistogram>,\n    /// Delete operation latencies\n    pub delete_latencies: RwLock<LatencyHistogram>,\n    /// Scan operation latencies\n    pub scan_latencies: RwLock<LatencyHistogram>,\n}\n\n/// Storage-related metrics\n#[derive(Debug, Default)]\npub struct StorageMetrics {\n    /// Disk read operations\n    pub disk_reads: AtomicU64,\n    /// Disk write operations\n    pub disk_writes: AtomicU64,\n    /// Disk bytes read\n    pub disk_bytes_read: AtomicU64,\n    /// Disk bytes written\n    pub disk_bytes_written: AtomicU64,\n    /// Disk flush operations\n    pub disk_flushes: AtomicU64,\n    /// Disk sync operations\n    pub disk_syncs: AtomicU64,\n}\n\n/// Memory-related metrics\n#[derive(Debug, Default)]\npub struct MemoryMetrics {\n    /// Current memory usage in bytes\n    pub current_memory_usage: AtomicU64,\n    /// Peak memory usage in bytes\n    pub peak_memory_usage: AtomicU64,\n    /// Number of pages allocated\n    pub pages_allocated: AtomicUsize,\n    /// Number of pages evicted\n    pub pages_evicted: AtomicUsize,\n    /// Number of memory mappings\n    pub mmap_count: AtomicUsize,\n    /// Total memory mapped size\n    pub mmap_size: AtomicU64,\n}\n\n/// Background task metrics\n#[derive(Debug, Default)]\npub struct BackgroundMetrics {\n    /// Number of checkpoints completed\n    pub checkpoints_completed: AtomicU64,\n    /// Number of checkpoint failures\n    pub checkpoint_failures: AtomicU64,\n    /// Total checkpoint duration (in milliseconds)\n    pub checkpoint_duration_ms: AtomicU64,\n    /// Number of GC cycles completed\n    pub gc_cycles_completed: AtomicU64,\n    /// Number of GC failures\n    pub gc_failures: AtomicU64,\n    /// Total GC duration (in milliseconds)\n    pub gc_duration_ms: AtomicU64,\n    /// Bytes reclaimed by GC\n    pub gc_bytes_reclaimed: AtomicU64,\n}\n\n/// Error tracking metrics\n#[derive(Debug, Default)]\npub struct ErrorMetrics {\n    /// Total number of errors\n    pub total_errors: AtomicU64,\n    /// IO errors\n    pub io_errors: AtomicU64,\n    /// Serialization errors\n    pub serialization_errors: AtomicU64,\n    /// Corruption errors\n    pub corruption_errors: AtomicU64,\n    /// Configuration errors\n    pub config_errors: AtomicU64,\n    /// Timeout errors\n    pub timeout_errors: AtomicU64,\n    /// Resource exhaustion errors\n    pub resource_exhausted_errors: AtomicU64,\n}\n\n/// Latency histogram for tracking operation latencies\n#[derive(Debug)]\npub struct LatencyHistogram {\n    /// Bucket boundaries in microseconds\n    buckets: Vec<u64>,\n    /// Count of operations in each bucket\n    counts: Vec<AtomicU64>,\n    /// Total count of operations\n    total_count: AtomicU64,\n    /// Sum of all latencies for average calculation\n    total_sum: AtomicU64,\n    /// Minimum latency observed\n    min_latency: AtomicU64,\n    /// Maximum latency observed\n    max_latency: AtomicU64,\n}\n\n/// Snapshot of metrics at a point in time\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MetricsSnapshot {\n    /// Timestamp when snapshot was taken\n    pub timestamp: u64,\n    /// System uptime in seconds\n    pub uptime_seconds: u64,\n    /// Operation metrics\n    pub operations: OperationMetricsSnapshot,\n    /// Latency metrics\n    pub latency: LatencyMetricsSnapshot,\n    /// Storage metrics\n    pub storage: StorageMetricsSnapshot,\n    /// Memory metrics\n    pub memory: MemoryMetricsSnapshot,\n    /// Background task metrics\n    pub background: BackgroundMetricsSnapshot,\n    /// Error metrics\n    pub errors: ErrorMetricsSnapshot,\n}\n\n/// Snapshot of operation metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OperationMetricsSnapshot {\n    pub reads_total: u64,\n    pub writes_total: u64,\n    pub deletes_total: u64,\n    pub scans_total: u64,\n    pub read_cache_hits: u64,\n    pub read_cache_misses: u64,\n    pub cache_hit_rate: f64,\n    pub bytes_read: u64,\n    pub bytes_written: u64,\n    pub ops_per_second: f64,\n}\n\n/// Snapshot of latency metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LatencyMetricsSnapshot {\n    pub read_p50_us: f64,\n    pub read_p95_us: f64,\n    pub read_p99_us: f64,\n    pub write_p50_us: f64,\n    pub write_p95_us: f64,\n    pub write_p99_us: f64,\n    pub delete_p50_us: f64,\n    pub delete_p95_us: f64,\n    pub delete_p99_us: f64,\n}\n\n/// Snapshot of storage metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct StorageMetricsSnapshot {\n    pub disk_reads: u64,\n    pub disk_writes: u64,\n    pub disk_bytes_read: u64,\n    pub disk_bytes_written: u64,\n    pub disk_flushes: u64,\n    pub disk_syncs: u64,\n    pub disk_read_bandwidth_mbps: f64,\n    pub disk_write_bandwidth_mbps: f64,\n}\n\n/// Snapshot of memory metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct MemoryMetricsSnapshot {\n    pub current_memory_usage: u64,\n    pub peak_memory_usage: u64,\n    pub pages_allocated: usize,\n    pub pages_evicted: usize,\n    pub mmap_count: usize,\n    pub mmap_size: u64,\n    pub memory_utilization: f64,\n}\n\n/// Snapshot of background metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct BackgroundMetricsSnapshot {\n    pub checkpoints_completed: u64,\n    pub checkpoint_failures: u64,\n    pub avg_checkpoint_duration_ms: f64,\n    pub gc_cycles_completed: u64,\n    pub gc_failures: u64,\n    pub avg_gc_duration_ms: f64,\n    pub gc_bytes_reclaimed: u64,\n}\n\n/// Snapshot of error metrics\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ErrorMetricsSnapshot {\n    pub total_errors: u64,\n    pub io_errors: u64,\n    pub serialization_errors: u64,\n    pub corruption_errors: u64,\n    pub config_errors: u64,\n    pub timeout_errors: u64,\n    pub resource_exhausted_errors: u64,\n    pub error_rate: f64,\n}\n\nimpl MetricsCollector {\n    /// Create a new metrics collector\n    pub fn new() -> Self {\n        Self {\n            operations: OperationMetrics::default(),\n            latency: LatencyMetrics::new(),\n            storage: StorageMetrics::default(),\n            memory: MemoryMetrics::default(),\n            background: BackgroundMetrics::default(),\n            errors: ErrorMetrics::default(),\n            start_time: Instant::now(),\n        }\n    }\n\n    /// Record a read operation\n    pub fn record_read(&self, latency: Duration, bytes: u64, cache_hit: bool) {\n        self.operations.reads_total.fetch_add(1, Ordering::Relaxed);\n        self.operations\n            .bytes_read\n            .fetch_add(bytes, Ordering::Relaxed);\n\n        if cache_hit {\n            self.operations\n                .read_cache_hits\n                .fetch_add(1, Ordering::Relaxed);\n        } else {\n            self.operations\n                .read_cache_misses\n                .fetch_add(1, Ordering::Relaxed);\n        }\n\n        self.latency.read_latencies.write().record(latency);\n    }\n\n    /// Record a write operation\n    pub fn record_write(&self, latency: Duration, bytes: u64) {\n        self.operations.writes_total.fetch_add(1, Ordering::Relaxed);\n        self.operations\n            .bytes_written\n            .fetch_add(bytes, Ordering::Relaxed);\n        self.latency.write_latencies.write().record(latency);\n    }\n\n    /// Record a delete operation\n    pub fn record_delete(&self, latency: Duration) {\n        self.operations\n            .deletes_total\n            .fetch_add(1, Ordering::Relaxed);\n        self.latency.delete_latencies.write().record(latency);\n    }\n\n    /// Record a scan operation\n    pub fn record_scan(&self, latency: Duration) {\n        self.operations.scans_total.fetch_add(1, Ordering::Relaxed);\n        self.latency.scan_latencies.write().record(latency);\n    }\n\n    /// Record storage operation\n    pub fn record_storage_op(&self, is_read: bool, bytes: u64) {\n        if is_read {\n            self.storage.disk_reads.fetch_add(1, Ordering::Relaxed);\n            self.storage\n                .disk_bytes_read\n                .fetch_add(bytes, Ordering::Relaxed);\n        } else {\n            self.storage.disk_writes.fetch_add(1, Ordering::Relaxed);\n            self.storage\n                .disk_bytes_written\n                .fetch_add(bytes, Ordering::Relaxed);\n        }\n    }\n\n    /// Record memory usage\n    pub fn record_memory_usage(&self, current: u64) {\n        self.memory\n            .current_memory_usage\n            .store(current, Ordering::Relaxed);\n\n        // Update peak if necessary\n        let mut peak = self.memory.peak_memory_usage.load(Ordering::Relaxed);\n        while current > peak {\n            match self.memory.peak_memory_usage.compare_exchange_weak(\n                peak,\n                current,\n                Ordering::Relaxed,\n                Ordering::Relaxed,\n            ) {\n                Ok(_) => break,\n                Err(new_peak) => peak = new_peak,\n            }\n        }\n    }\n\n    /// Record an error\n    pub fn record_error(&self, error_category: &str) {\n        self.errors.total_errors.fetch_add(1, Ordering::Relaxed);\n\n        match error_category {\n            \"io\" => {\n                self.errors.io_errors.fetch_add(1, Ordering::Relaxed);\n            }\n            \"serialization\" => {\n                self.errors\n                    .serialization_errors\n                    .fetch_add(1, Ordering::Relaxed);\n            }\n            \"corruption\" => {\n                self.errors\n                    .corruption_errors\n                    .fetch_add(1, Ordering::Relaxed);\n            }\n            \"configuration\" => {\n                self.errors.config_errors.fetch_add(1, Ordering::Relaxed);\n            }\n            \"timeout\" => {\n                self.errors.timeout_errors.fetch_add(1, Ordering::Relaxed);\n            }\n            \"resource_exhausted\" => {\n                self.errors\n                    .resource_exhausted_errors\n                    .fetch_add(1, Ordering::Relaxed);\n            }\n            _ => {} // Unknown error category\n        }\n    }\n\n    /// Get a snapshot of current metrics\n    pub fn snapshot(&self) -> MetricsSnapshot {\n        let uptime = self.start_time.elapsed();\n        let uptime_seconds = uptime.as_secs();\n\n        // Operation metrics\n        let reads = self.operations.reads_total.load(Ordering::Relaxed);\n        let writes = self.operations.writes_total.load(Ordering::Relaxed);\n        let deletes = self.operations.deletes_total.load(Ordering::Relaxed);\n        let scans = self.operations.scans_total.load(Ordering::Relaxed);\n        let cache_hits = self.operations.read_cache_hits.load(Ordering::Relaxed);\n        let cache_misses = self.operations.read_cache_misses.load(Ordering::Relaxed);\n\n        let total_ops = reads + writes + deletes + scans;\n        let ops_per_second = if uptime_seconds > 0 {\n            total_ops as f64 / uptime_seconds as f64\n        } else {\n            0.0\n        };\n\n        let cache_hit_rate = if cache_hits + cache_misses > 0 {\n            cache_hits as f64 / (cache_hits + cache_misses) as f64\n        } else {\n            0.0\n        };\n\n        MetricsSnapshot {\n            timestamp: std::time::SystemTime::now()\n                .duration_since(std::time::UNIX_EPOCH)\n                .unwrap_or_default()\n                .as_secs(),\n            uptime_seconds,\n            operations: OperationMetricsSnapshot {\n                reads_total: reads,\n                writes_total: writes,\n                deletes_total: deletes,\n                scans_total: scans,\n                read_cache_hits: cache_hits,\n                read_cache_misses: cache_misses,\n                cache_hit_rate,\n                bytes_read: self.operations.bytes_read.load(Ordering::Relaxed),\n                bytes_written: self.operations.bytes_written.load(Ordering::Relaxed),\n                ops_per_second,\n            },\n            latency: LatencyMetricsSnapshot {\n                read_p50_us: self.latency.read_latencies.read().percentile(50.0),\n                read_p95_us: self.latency.read_latencies.read().percentile(95.0),\n                read_p99_us: self.latency.read_latencies.read().percentile(99.0),\n                write_p50_us: self.latency.write_latencies.read().percentile(50.0),\n                write_p95_us: self.latency.write_latencies.read().percentile(95.0),\n                write_p99_us: self.latency.write_latencies.read().percentile(99.0),\n                delete_p50_us: self.latency.delete_latencies.read().percentile(50.0),\n                delete_p95_us: self.latency.delete_latencies.read().percentile(95.0),\n                delete_p99_us: self.latency.delete_latencies.read().percentile(99.0),\n            },\n            storage: StorageMetricsSnapshot {\n                disk_reads: self.storage.disk_reads.load(Ordering::Relaxed),\n                disk_writes: self.storage.disk_writes.load(Ordering::Relaxed),\n                disk_bytes_read: self.storage.disk_bytes_read.load(Ordering::Relaxed),\n                disk_bytes_written: self.storage.disk_bytes_written.load(Ordering::Relaxed),\n                disk_flushes: self.storage.disk_flushes.load(Ordering::Relaxed),\n                disk_syncs: self.storage.disk_syncs.load(Ordering::Relaxed),\n                disk_read_bandwidth_mbps: if uptime_seconds > 0 {\n                    (self.storage.disk_bytes_read.load(Ordering::Relaxed) as f64)\n                        / (uptime_seconds as f64 * 1024.0 * 1024.0)\n                } else {\n                    0.0\n                },\n                disk_write_bandwidth_mbps: if uptime_seconds > 0 {\n                    (self.storage.disk_bytes_written.load(Ordering::Relaxed) as f64)\n                        / (uptime_seconds as f64 * 1024.0 * 1024.0)\n                } else {\n                    0.0\n                },\n            },\n            memory: MemoryMetricsSnapshot {\n                current_memory_usage: self.memory.current_memory_usage.load(Ordering::Relaxed),\n                peak_memory_usage: self.memory.peak_memory_usage.load(Ordering::Relaxed),\n                pages_allocated: self.memory.pages_allocated.load(Ordering::Relaxed),\n                pages_evicted: self.memory.pages_evicted.load(Ordering::Relaxed),\n                mmap_count: self.memory.mmap_count.load(Ordering::Relaxed),\n                mmap_size: self.memory.mmap_size.load(Ordering::Relaxed),\n                memory_utilization: 0.0, // TODO: Calculate based on system memory\n            },\n            background: BackgroundMetricsSnapshot {\n                checkpoints_completed: self\n                    .background\n                    .checkpoints_completed\n                    .load(Ordering::Relaxed),\n                checkpoint_failures: self.background.checkpoint_failures.load(Ordering::Relaxed),\n                avg_checkpoint_duration_ms: {\n                    let completed = self\n                        .background\n                        .checkpoints_completed\n                        .load(Ordering::Relaxed);\n                    if completed > 0 {\n                        self.background\n                            .checkpoint_duration_ms\n                            .load(Ordering::Relaxed) as f64\n                            / completed as f64\n                    } else {\n                        0.0\n                    }\n                },\n                gc_cycles_completed: self.background.gc_cycles_completed.load(Ordering::Relaxed),\n                gc_failures: self.background.gc_failures.load(Ordering::Relaxed),\n                avg_gc_duration_ms: {\n                    let completed = self.background.gc_cycles_completed.load(Ordering::Relaxed);\n                    if completed > 0 {\n                        self.background.gc_duration_ms.load(Ordering::Relaxed) as f64\n                            / completed as f64\n                    } else {\n                        0.0\n                    }\n                },\n                gc_bytes_reclaimed: self.background.gc_bytes_reclaimed.load(Ordering::Relaxed),\n            },\n            errors: ErrorMetricsSnapshot {\n                total_errors: self.errors.total_errors.load(Ordering::Relaxed),\n                io_errors: self.errors.io_errors.load(Ordering::Relaxed),\n                serialization_errors: self.errors.serialization_errors.load(Ordering::Relaxed),\n                corruption_errors: self.errors.corruption_errors.load(Ordering::Relaxed),\n                config_errors: self.errors.config_errors.load(Ordering::Relaxed),\n                timeout_errors: self.errors.timeout_errors.load(Ordering::Relaxed),\n                resource_exhausted_errors: self\n                    .errors\n                    .resource_exhausted_errors\n                    .load(Ordering::Relaxed),\n                error_rate: if total_ops > 0 {\n                    self.errors.total_errors.load(Ordering::Relaxed) as f64 / total_ops as f64\n                } else {\n                    0.0\n                },\n            },\n        }\n    }\n\n    /// Reset all metrics (useful for testing)\n    pub fn reset(&self) {\n        // Reset operation metrics\n        self.operations.reads_total.store(0, Ordering::Relaxed);\n        self.operations.writes_total.store(0, Ordering::Relaxed);\n        self.operations.deletes_total.store(0, Ordering::Relaxed);\n        self.operations.scans_total.store(0, Ordering::Relaxed);\n        self.operations.read_cache_hits.store(0, Ordering::Relaxed);\n        self.operations\n            .read_cache_misses\n            .store(0, Ordering::Relaxed);\n        self.operations.bytes_read.store(0, Ordering::Relaxed);\n        self.operations.bytes_written.store(0, Ordering::Relaxed);\n\n        // Reset latency histograms\n        self.latency.read_latencies.write().reset();\n        self.latency.write_latencies.write().reset();\n        self.latency.delete_latencies.write().reset();\n        self.latency.scan_latencies.write().reset();\n\n        // Reset other metrics...\n        // (Implementation truncated for brevity)\n    }\n}\n\nimpl LatencyMetrics {\n    fn new() -> Self {\n        Self {\n            read_latencies: RwLock::new(LatencyHistogram::new()),\n            write_latencies: RwLock::new(LatencyHistogram::new()),\n            delete_latencies: RwLock::new(LatencyHistogram::new()),\n            scan_latencies: RwLock::new(LatencyHistogram::new()),\n        }\n    }\n}\n\nimpl LatencyHistogram {\n    fn new() -> Self {\n        // Bucket boundaries: 10us, 50us, 100us, 500us, 1ms, 5ms, 10ms, 50ms, 100ms, 500ms, 1s, 5s\n        let buckets = vec![\n            10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000,\n        ];\n        let counts = buckets.iter().map(|_| AtomicU64::new(0)).collect();\n\n        Self {\n            buckets,\n            counts,\n            total_count: AtomicU64::new(0),\n            total_sum: AtomicU64::new(0),\n            min_latency: AtomicU64::new(u64::MAX),\n            max_latency: AtomicU64::new(0),\n        }\n    }\n\n    fn record(&self, latency: Duration) {\n        let latency_us = latency.as_micros() as u64;\n\n        // Update min/max\n        let mut current_min = self.min_latency.load(Ordering::Relaxed);\n        while latency_us < current_min {\n            match self.min_latency.compare_exchange_weak(\n                current_min,\n                latency_us,\n                Ordering::Relaxed,\n                Ordering::Relaxed,\n            ) {\n                Ok(_) => break,\n                Err(new_min) => current_min = new_min,\n            }\n        }\n\n        let mut current_max = self.max_latency.load(Ordering::Relaxed);\n        while latency_us > current_max {\n            match self.max_latency.compare_exchange_weak(\n                current_max,\n                latency_us,\n                Ordering::Relaxed,\n                Ordering::Relaxed,\n            ) {\n                Ok(_) => break,\n                Err(new_max) => current_max = new_max,\n            }\n        }\n\n        // Find appropriate bucket and increment\n        for (i, &bucket_limit) in self.buckets.iter().enumerate() {\n            if latency_us <= bucket_limit {\n                self.counts[i].fetch_add(1, Ordering::Relaxed);\n                break;\n            }\n        }\n\n        // Update totals\n        self.total_count.fetch_add(1, Ordering::Relaxed);\n        self.total_sum.fetch_add(latency_us, Ordering::Relaxed);\n    }\n\n    fn percentile(&self, p: f64) -> f64 {\n        let total = self.total_count.load(Ordering::Relaxed);\n        if total == 0 {\n            return 0.0;\n        }\n\n        let target_count = (total as f64 * p / 100.0) as u64;\n        let mut cumulative = 0;\n\n        for (i, count) in self.counts.iter().enumerate() {\n            cumulative += count.load(Ordering::Relaxed);\n            if cumulative >= target_count {\n                return self.buckets[i] as f64;\n            }\n        }\n\n        *self.buckets.last().unwrap_or(&0) as f64\n    }\n\n    fn reset(&self) {\n        for count in &self.counts {\n            count.store(0, Ordering::Relaxed);\n        }\n        self.total_count.store(0, Ordering::Relaxed);\n        self.total_sum.store(0, Ordering::Relaxed);\n        self.min_latency.store(u64::MAX, Ordering::Relaxed);\n        self.max_latency.store(0, Ordering::Relaxed);\n    }\n}\n\nimpl Default for MetricsCollector {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Shared metrics collector type\npub type SharedMetricsCollector = Arc<MetricsCollector>;\n\n/// Create a new shared metrics collector\npub fn new_shared_metrics_collector() -> SharedMetricsCollector {\n    Arc::new(MetricsCollector::new())\n}\n\n#[cfg(test)]\nmod tests {\n    use std::time::Duration;\n\n    use super::*;\n\n    #[test]\n    fn test_metrics_collection() {\n        let metrics = MetricsCollector::new();\n\n        // Record some operations\n        metrics.record_read(Duration::from_micros(100), 1024, true);\n        metrics.record_write(Duration::from_micros(200), 2048);\n        metrics.record_delete(Duration::from_micros(50));\n\n        // Get snapshot\n        let snapshot = metrics.snapshot();\n\n        assert_eq!(snapshot.operations.reads_total, 1);\n        assert_eq!(snapshot.operations.writes_total, 1);\n        assert_eq!(snapshot.operations.deletes_total, 1);\n        assert_eq!(snapshot.operations.bytes_read, 1024);\n        assert_eq!(snapshot.operations.bytes_written, 2048);\n        assert_eq!(snapshot.operations.cache_hit_rate, 1.0);\n\n        // Test latency percentiles\n        assert!(snapshot.latency.read_p50_us > 0.0);\n        assert!(snapshot.latency.write_p50_us > 0.0);\n        assert!(snapshot.latency.delete_p50_us > 0.0);\n    }\n\n    #[test]\n    fn test_latency_histogram() {\n        let histogram = LatencyHistogram::new();\n\n        // Record some latencies\n        histogram.record(Duration::from_micros(25)); // Should go to 50us bucket\n        histogram.record(Duration::from_micros(75)); // Should go to 100us bucket\n        histogram.record(Duration::from_micros(150)); // Should go to 500us bucket\n\n        assert_eq!(histogram.total_count.load(Ordering::Relaxed), 3);\n        assert!(histogram.percentile(50.0) > 0.0);\n    }\n}\n","traces":[{"line":257,"address":[],"length":0,"stats":{"Line":1}},{"line":259,"address":[],"length":0,"stats":{"Line":2}},{"line":260,"address":[],"length":0,"stats":{"Line":2}},{"line":261,"address":[],"length":0,"stats":{"Line":2}},{"line":262,"address":[],"length":0,"stats":{"Line":2}},{"line":263,"address":[],"length":0,"stats":{"Line":2}},{"line":264,"address":[],"length":0,"stats":{"Line":1}},{"line":265,"address":[],"length":0,"stats":{"Line":1}},{"line":270,"address":[],"length":0,"stats":{"Line":1}},{"line":271,"address":[],"length":0,"stats":{"Line":3}},{"line":272,"address":[],"length":0,"stats":{"Line":1}},{"line":273,"address":[],"length":0,"stats":{"Line":1}},{"line":274,"address":[],"length":0,"stats":{"Line":3}},{"line":276,"address":[],"length":0,"stats":{"Line":2}},{"line":277,"address":[],"length":0,"stats":{"Line":2}},{"line":278,"address":[],"length":0,"stats":{"Line":2}},{"line":279,"address":[],"length":0,"stats":{"Line":1}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":286,"address":[],"length":0,"stats":{"Line":2}},{"line":290,"address":[],"length":0,"stats":{"Line":1}},{"line":291,"address":[],"length":0,"stats":{"Line":3}},{"line":292,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":1}},{"line":294,"address":[],"length":0,"stats":{"Line":3}},{"line":295,"address":[],"length":0,"stats":{"Line":2}},{"line":299,"address":[],"length":0,"stats":{"Line":1}},{"line":300,"address":[],"length":0,"stats":{"Line":1}},{"line":301,"address":[],"length":0,"stats":{"Line":1}},{"line":302,"address":[],"length":0,"stats":{"Line":2}},{"line":303,"address":[],"length":0,"stats":{"Line":2}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":321,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":0}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":338,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":343,"address":[],"length":0,"stats":{"Line":0}},{"line":349,"address":[],"length":0,"stats":{"Line":0}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":352,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":0}},{"line":354,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":0}},{"line":357,"address":[],"length":0,"stats":{"Line":0}},{"line":358,"address":[],"length":0,"stats":{"Line":0}},{"line":359,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":362,"address":[],"length":0,"stats":{"Line":0}},{"line":363,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":373,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":377,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":1}},{"line":383,"address":[],"length":0,"stats":{"Line":3}},{"line":384,"address":[],"length":0,"stats":{"Line":3}},{"line":387,"address":[],"length":0,"stats":{"Line":4}},{"line":388,"address":[],"length":0,"stats":{"Line":4}},{"line":389,"address":[],"length":0,"stats":{"Line":4}},{"line":390,"address":[],"length":0,"stats":{"Line":4}},{"line":391,"address":[],"length":0,"stats":{"Line":4}},{"line":392,"address":[],"length":0,"stats":{"Line":4}},{"line":394,"address":[],"length":0,"stats":{"Line":2}},{"line":395,"address":[],"length":0,"stats":{"Line":2}},{"line":396,"address":[],"length":0,"stats":{"Line":0}},{"line":398,"address":[],"length":0,"stats":{"Line":1}},{"line":401,"address":[],"length":0,"stats":{"Line":2}},{"line":402,"address":[],"length":0,"stats":{"Line":2}},{"line":404,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":2}},{"line":413,"address":[],"length":0,"stats":{"Line":1}},{"line":425,"address":[],"length":0,"stats":{"Line":1}},{"line":456,"address":[],"length":0,"stats":{"Line":1}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":525,"address":[],"length":0,"stats":{"Line":0}},{"line":526,"address":[],"length":0,"stats":{"Line":0}},{"line":527,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":529,"address":[],"length":0,"stats":{"Line":0}},{"line":530,"address":[],"length":0,"stats":{"Line":0}},{"line":533,"address":[],"length":0,"stats":{"Line":0}},{"line":534,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":544,"address":[],"length":0,"stats":{"Line":1}},{"line":546,"address":[],"length":0,"stats":{"Line":3}},{"line":547,"address":[],"length":0,"stats":{"Line":3}},{"line":548,"address":[],"length":0,"stats":{"Line":3}},{"line":549,"address":[],"length":0,"stats":{"Line":1}},{"line":555,"address":[],"length":0,"stats":{"Line":5}},{"line":557,"address":[],"length":0,"stats":{"Line":10}},{"line":560,"address":[],"length":0,"stats":{"Line":80}},{"line":565,"address":[],"length":0,"stats":{"Line":10}},{"line":566,"address":[],"length":0,"stats":{"Line":10}},{"line":567,"address":[],"length":0,"stats":{"Line":5}},{"line":568,"address":[],"length":0,"stats":{"Line":5}},{"line":572,"address":[],"length":0,"stats":{"Line":6}},{"line":573,"address":[],"length":0,"stats":{"Line":12}},{"line":576,"address":[],"length":0,"stats":{"Line":24}},{"line":577,"address":[],"length":0,"stats":{"Line":6}},{"line":578,"address":[],"length":0,"stats":{"Line":12}},{"line":579,"address":[],"length":0,"stats":{"Line":8}},{"line":580,"address":[],"length":0,"stats":{"Line":8}},{"line":581,"address":[],"length":0,"stats":{"Line":4}},{"line":582,"address":[],"length":0,"stats":{"Line":4}},{"line":584,"address":[],"length":0,"stats":{"Line":4}},{"line":585,"address":[],"length":0,"stats":{"Line":0}},{"line":589,"address":[],"length":0,"stats":{"Line":24}},{"line":590,"address":[],"length":0,"stats":{"Line":6}},{"line":591,"address":[],"length":0,"stats":{"Line":18}},{"line":592,"address":[],"length":0,"stats":{"Line":12}},{"line":593,"address":[],"length":0,"stats":{"Line":12}},{"line":594,"address":[],"length":0,"stats":{"Line":6}},{"line":595,"address":[],"length":0,"stats":{"Line":6}},{"line":597,"address":[],"length":0,"stats":{"Line":6}},{"line":598,"address":[],"length":0,"stats":{"Line":0}},{"line":603,"address":[],"length":0,"stats":{"Line":48}},{"line":604,"address":[],"length":0,"stats":{"Line":18}},{"line":605,"address":[],"length":0,"stats":{"Line":6}},{"line":611,"address":[],"length":0,"stats":{"Line":18}},{"line":612,"address":[],"length":0,"stats":{"Line":24}},{"line":615,"address":[],"length":0,"stats":{"Line":10}},{"line":616,"address":[],"length":0,"stats":{"Line":40}},{"line":617,"address":[],"length":0,"stats":{"Line":10}},{"line":618,"address":[],"length":0,"stats":{"Line":0}},{"line":624,"address":[],"length":0,"stats":{"Line":22}},{"line":625,"address":[],"length":0,"stats":{"Line":22}},{"line":626,"address":[],"length":0,"stats":{"Line":11}},{"line":627,"address":[],"length":0,"stats":{"Line":10}},{"line":631,"address":[],"length":0,"stats":{"Line":0}},{"line":634,"address":[],"length":0,"stats":{"Line":0}},{"line":635,"address":[],"length":0,"stats":{"Line":0}},{"line":636,"address":[],"length":0,"stats":{"Line":0}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":639,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":0}},{"line":641,"address":[],"length":0,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[],"length":0,"stats":{"Line":0}},{"line":655,"address":[],"length":0,"stats":{"Line":0}},{"line":656,"address":[],"length":0,"stats":{"Line":0}}],"covered":89,"coverable":172},{"path":["/","Users","xuesong.zhao","repo","rust","rskv","src","rskv.rs"],"content":"//! Main RsKv key-value store implementation\n//!\n//! This module contains the top-level RsKv struct that orchestrates all other\n//! components including the hybrid log, hash index, and background tasks.\n\nuse std::path::Path;\nuse std::sync::Arc;\n\nuse tokio::sync::RwLock as AsyncRwLock;\n\nuse crate::background::{BackgroundTaskManager, BackgroundTaskStats};\nuse crate::checkpoint::{CheckpointState, CheckpointStats};\nuse crate::common::{Address, Config, INVALID_ADDRESS, Key, Result, RsKvError, Value};\nuse crate::epoch::{EpochManager, SharedEpochManager};\nuse crate::gc::{GcConfig, GcState, GcStats};\nuse crate::hlog::{FileStorageDevice, HybridLog, LogRecord};\nuse crate::index::{SharedMemHashIndex, new_shared_mem_hash_index_with_capacity};\n\n/// The main RsKv key-value store\n///\n/// This is the primary interface for interacting with the rskv system.\n/// It orchestrates the hybrid log, hash index, and background operations.\npub struct RsKv {\n    /// Hybrid log for persistent storage\n    hlog: Arc<HybridLog>,\n\n    /// Hash index for fast key lookups\n    index: SharedMemHashIndex,\n\n    /// Epoch manager for safe memory reclamation\n    #[allow(dead_code)]\n    epoch: SharedEpochManager,\n\n    /// Configuration\n    config: Config,\n\n    /// Lock to coordinate checkpoint and recovery operations\n    checkpoint_lock: Arc<AsyncRwLock<()>>,\n\n    /// Checkpoint state manager\n    checkpoint_state: Arc<CheckpointState>,\n\n    /// Garbage collection state manager\n    gc_state: Arc<GcState>,\n\n    /// Background task manager\n    background_manager: Arc<BackgroundTaskManager>,\n}\n\nimpl RsKv {\n    /// Create a new RsKv instance with the given configuration\n    pub async fn new(config: Config) -> Result<Self> {\n        // Validate configuration first\n        config.validate()?;\n\n        log::info!(\"Initializing RsKv with validated configuration\");\n\n        // Ensure storage directory exists\n        let storage_path = Path::new(&config.storage_dir);\n        if !storage_path.exists() {\n            std::fs::create_dir_all(storage_path)?;\n        }\n\n        // Create epoch manager\n        let epoch = Arc::new(EpochManager::new());\n\n        // Create storage device\n        let log_file_path = storage_path.join(\"rskv.log\");\n        let storage_device = Box::new(FileStorageDevice::new(log_file_path)?);\n\n        // Create hybrid log\n        let hlog = Arc::new(HybridLog::new(\n            config.memory_size,\n            storage_device,\n            epoch.clone(),\n        )?);\n\n        // Create hash index with estimated capacity\n        let estimated_capacity = (config.memory_size / 1024) as usize; // Rough estimate\n        let index = new_shared_mem_hash_index_with_capacity(estimated_capacity, epoch.clone());\n\n        // Create checkpoint state manager\n        let checkpoint_dir = storage_path.join(\"checkpoints\");\n        let checkpoint_state = Arc::new(CheckpointState::new(\n            checkpoint_dir,\n            hlog.clone(),\n            index.clone(),\n        )?);\n\n        // Create garbage collection state manager\n        let gc_state = Arc::new(GcState::new(hlog.clone(), index.clone()));\n\n        // Create operation lock for coordinating with background tasks\n        let checkpoint_lock = Arc::new(AsyncRwLock::new(()));\n\n        // Create background task manager\n        let background_manager = Arc::new(BackgroundTaskManager::new(\n            config.clone(),\n            checkpoint_state.clone(),\n            gc_state.clone(),\n            hlog.clone(),\n            checkpoint_lock.clone(),\n        ));\n\n        // Try to recover from the latest checkpoint if it exists\n        if let Some(_metadata) = checkpoint_state.recover_from_latest_checkpoint().await? {\n            log::info!(\"Recovered from checkpoint\");\n        }\n\n        let rskv = Self {\n            hlog,\n            index,\n            epoch,\n            config: config.clone(),\n            checkpoint_lock,\n            checkpoint_state,\n            gc_state,\n            background_manager,\n        };\n\n        // Start background tasks\n        if config.enable_checkpointing || config.enable_gc {\n            rskv.background_manager.start()?;\n            log::info!(\"Background tasks started\");\n        }\n\n        Ok(rskv)\n    }\n\n    /// Insert or update a key-value pair\n    ///\n    /// This operation writes the record to the log and updates the index.\n    /// If the key already exists, it creates a new version in the log.\n    pub async fn upsert(&self, key: Key, value: Value) -> Result<()> {\n        // Get the current address for this key (if it exists)\n        let previous_address = self.index.find(&key).unwrap_or(INVALID_ADDRESS);\n\n        // Create a new log record\n        let record = LogRecord::new(key.clone(), value, previous_address);\n\n        // Insert the record into the log\n        let new_address = self.hlog.insert_record(record)?;\n\n        // Update the index to point to the new address\n        self.index.insert(key, new_address);\n\n        Ok(())\n    }\n\n    /// Read a value for the given key\n    ///\n    /// This operation first checks the index to find the latest address,\n    /// then retrieves the value from the log.\n    pub async fn read(&self, key: &Key) -> Result<Option<Value>> {\n        // Find the address in the index\n        let address = match self.index.find(key) {\n            Some(addr) => addr,\n            None => return Ok(None), // Key not found\n        };\n\n        // Read the record from the log\n        let record = self.hlog.read_record(address)?;\n\n        // Check if this is a tombstone (deleted record)\n        if record.header.tombstone {\n            return Ok(None);\n        }\n\n        // Verify the key matches (protection against hash collisions)\n        if record.key != *key {\n            return Err(RsKvError::Internal {\n                message: \"Key mismatch in log record\".to_string(),\n            });\n        }\n\n        Ok(Some(record.value))\n    }\n\n    /// Delete a key\n    ///\n    /// This operation creates a tombstone record in the log and updates the index.\n    pub async fn delete(&self, key: &Key) -> Result<()> {\n        // Get the current address for this key (if it exists)\n        let previous_address = self.index.find(key).unwrap_or(INVALID_ADDRESS);\n\n        // Create a tombstone record\n        let tombstone = LogRecord::tombstone(key.clone(), previous_address);\n\n        // Insert the tombstone into the log\n        let new_address = self.hlog.insert_record(tombstone)?;\n\n        // Update the index to point to the tombstone\n        self.index.insert(key.clone(), new_address);\n\n        Ok(())\n    }\n\n    /// Check if a key exists in the store\n    pub async fn contains_key(&self, key: &Key) -> Result<bool> {\n        match self.read(key).await? {\n            Some(_) => Ok(true),\n            None => Ok(false),\n        }\n    }\n\n    /// Get the number of entries in the index\n    /// Note: This may include deleted entries (tombstones)\n    pub fn len(&self) -> usize {\n        self.index.len()\n    }\n\n    /// Check if the store appears to be empty\n    /// Note: This only checks the index, not whether all entries are tombstones\n    pub fn is_empty(&self) -> bool {\n        self.index.is_empty()\n    }\n\n    /// Get current statistics about the store\n    pub fn stats(&self) -> RsKvStats {\n        let index_len = self.index.len();\n        let tail_address = self.hlog.get_tail_address();\n        let head_address = self.hlog.get_head_address();\n        let read_only_address = self.hlog.get_read_only_address();\n        let begin_address = self.hlog.get_begin_address();\n\n        RsKvStats {\n            index_entries: index_len,\n            log_tail_address: tail_address,\n            log_head_address: head_address,\n            log_read_only_address: read_only_address,\n            log_begin_address: begin_address,\n            mutable_region_size: tail_address.saturating_sub(read_only_address),\n            read_only_region_size: read_only_address.saturating_sub(head_address),\n            disk_region_size: head_address.saturating_sub(begin_address),\n        }\n    }\n\n    /// Manually trigger a checkpoint operation\n    /// This will flush the current state to persistent storage\n    pub async fn checkpoint(&self) -> Result<()> {\n        let _lock = self.checkpoint_lock.write().await;\n\n        log::info!(\"Starting checkpoint operation\");\n\n        // Delegate to checkpoint state manager\n        let _metadata = self.checkpoint_state.initiate_checkpoint().await?;\n\n        log::info!(\"Checkpoint completed successfully\");\n        Ok(())\n    }\n\n    /// Get checkpoint statistics\n    pub async fn checkpoint_stats(&self) -> Result<CheckpointStats> {\n        self.checkpoint_state.get_checkpoint_stats().await\n    }\n\n    /// List all available checkpoints\n    pub async fn list_checkpoints(&self) -> Result<Vec<u64>> {\n        self.checkpoint_state.list_checkpoints().await\n    }\n\n    /// Clean up old checkpoints, keeping only the specified number\n    pub async fn cleanup_checkpoints(&self, keep_count: usize) -> Result<()> {\n        self.checkpoint_state\n            .cleanup_old_checkpoints(keep_count)\n            .await\n    }\n\n    /// Manually trigger garbage collection\n    /// This will reclaim space from old log entries\n    pub async fn garbage_collect(&self) -> Result<GcStats> {\n        self.garbage_collect_with_config(GcConfig::default()).await\n    }\n\n    /// Trigger garbage collection with custom configuration\n    pub async fn garbage_collect_with_config(&self, config: GcConfig) -> Result<GcStats> {\n        let _lock = self.checkpoint_lock.read().await;\n\n        log::info!(\"Starting garbage collection\");\n\n        // Delegate to GC state manager\n        let stats = self.gc_state.initiate_gc(config).await?;\n\n        log::info!(\n            \"Garbage collection completed, reclaimed {} bytes\",\n            stats.bytes_reclaimed\n        );\n        Ok(stats)\n    }\n\n    /// Check if garbage collection is recommended\n    pub fn should_run_gc(&self) -> Result<bool> {\n        self.gc_state.should_run_gc(&GcConfig::default())\n    }\n\n    /// Get an estimate of reclaimable space\n    pub fn gc_estimate(&self) -> Result<crate::gc::GcEstimate> {\n        self.gc_state.estimate_reclaimable_space()\n    }\n\n    /// Get the current configuration\n    pub fn config(&self) -> &Config {\n        &self.config\n    }\n\n    /// Iterate over all key-value pairs\n    /// Note: This is an expensive operation that reads from the log\n    pub async fn scan_all(&self) -> Result<Vec<(Key, Value)>> {\n        let mut results = Vec::new();\n\n        // Iterate through the index and read each record\n        self.index.for_each(|key, address| {\n            if let Ok(record) = self.hlog.read_record(address) {\n                // Skip tombstones\n                if !record.header.tombstone {\n                    results.push((key.clone(), record.value));\n                }\n            }\n        });\n\n        Ok(results)\n    }\n\n    /// Perform a prefix scan (find all keys with a given prefix)\n    pub async fn scan_prefix(&self, prefix: &[u8]) -> Result<Vec<(Key, Value)>> {\n        let mut results = Vec::new();\n\n        self.index.for_each(|key, address| {\n            if key.starts_with(prefix)\n                && let Ok(record) = self.hlog.read_record(address)\n                && !record.header.tombstone\n            {\n                results.push((key.clone(), record.value));\n            }\n        });\n\n        Ok(results)\n    }\n\n    /// Get background task statistics\n    pub fn background_stats(&self) -> BackgroundTaskStats {\n        self.background_manager.get_stats()\n    }\n\n    /// Stop background tasks (useful for testing or manual control)\n    pub async fn stop_background_tasks(&self) -> Result<()> {\n        self.background_manager.stop().await\n    }\n\n    /// Start background tasks (useful after stopping them manually)\n    pub fn start_background_tasks(&self) -> Result<()> {\n        self.background_manager.start()\n    }\n\n    /// Close the store and ensure all data is persisted\n    pub async fn close(&self) -> Result<()> {\n        log::info!(\"Closing rskv store\");\n\n        // Stop background tasks first\n        self.background_manager.stop().await?;\n\n        // Wait a moment for any ongoing background operations to complete\n        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n\n        // Perform a final checkpoint to ensure all data is persisted\n        // Use a separate checkpoint call that bypasses the ongoing check\n        match self.checkpoint_state.initiate_checkpoint().await {\n            Ok(_) => {\n                log::info!(\"Final checkpoint completed successfully\");\n            }\n            Err(e) if e.to_string().contains(\"already in progress\") => {\n                log::info!(\"Skipping final checkpoint - one already in progress\");\n            }\n            Err(e) => return Err(e),\n        }\n\n        // Run garbage collection to clean up space\n        if self.should_run_gc()? {\n            let _gc_stats = self.garbage_collect().await?;\n        }\n\n        // Clean up old checkpoints, keeping only the last 3\n        self.cleanup_checkpoints(3).await?;\n\n        log::info!(\"Store closed successfully\");\n        Ok(())\n    }\n}\n\n/// Statistics about the RsKv store\n#[derive(Debug, Clone)]\npub struct RsKvStats {\n    /// Number of entries in the hash index\n    pub index_entries: usize,\n    /// Current tail address of the log\n    pub log_tail_address: Address,\n    /// Current head address of the log\n    pub log_head_address: Address,\n    /// Current read-only address of the log\n    pub log_read_only_address: Address,\n    /// Current begin address of the log\n    pub log_begin_address: Address,\n    /// Size of the mutable region in bytes\n    pub mutable_region_size: u64,\n    /// Size of the read-only region in bytes\n    pub read_only_region_size: u64,\n    /// Size of the disk-only region in bytes\n    pub disk_region_size: u64,\n}\n\n// GcStats moved to gc.rs module\n\n#[cfg(test)]\nmod tests {\n    use tempfile::tempdir;\n\n    use super::*;\n\n    async fn create_test_rskv() -> RsKv {\n        let temp_dir = tempdir().unwrap();\n        let config = Config {\n            storage_dir: temp_dir.path().to_string_lossy().to_string(),\n            memory_size: 64 * 1024 * 1024, // 64MB\n            enable_checkpointing: false,   // Disable for testing to avoid background tasks\n            enable_gc: false,              // Disable for testing to avoid background tasks\n            ..Default::default()\n        };\n\n        RsKv::new(config).await.unwrap()\n    }\n\n    #[tokio::test]\n    async fn test_basic_operations() {\n        let store = create_test_rskv().await;\n\n        let key = b\"test_key\".to_vec();\n        let value = b\"test_value\".to_vec();\n\n        // Test upsert\n        store.upsert(key.clone(), value.clone()).await.unwrap();\n\n        // Test read\n        let result = store.read(&key).await.unwrap();\n        assert_eq!(result, Some(value.clone()));\n\n        // Test contains_key\n        assert!(store.contains_key(&key).await.unwrap());\n\n        // Test delete\n        store.delete(&key).await.unwrap();\n        let result = store.read(&key).await.unwrap();\n        assert_eq!(result, None);\n\n        assert!(!store.contains_key(&key).await.unwrap());\n    }\n\n    #[tokio::test]\n    async fn test_upsert_overwrites() {\n        let store = create_test_rskv().await;\n\n        let key = b\"test_key\".to_vec();\n        let value1 = b\"value1\".to_vec();\n        let value2 = b\"value2\".to_vec();\n\n        // Insert first value\n        store.upsert(key.clone(), value1.clone()).await.unwrap();\n        let result = store.read(&key).await.unwrap();\n        assert_eq!(result, Some(value1));\n\n        // Overwrite with second value\n        store.upsert(key.clone(), value2.clone()).await.unwrap();\n        let result = store.read(&key).await.unwrap();\n        assert_eq!(result, Some(value2));\n    }\n\n    #[tokio::test]\n    async fn test_multiple_keys() {\n        let store = create_test_rskv().await;\n\n        let entries = vec![\n            (b\"key1\".to_vec(), b\"value1\".to_vec()),\n            (b\"key2\".to_vec(), b\"value2\".to_vec()),\n            (b\"key3\".to_vec(), b\"value3\".to_vec()),\n        ];\n\n        // Insert all entries\n        for (key, value) in &entries {\n            store.upsert(key.clone(), value.clone()).await.unwrap();\n        }\n\n        // Verify all entries\n        for (key, value) in &entries {\n            let result = store.read(key).await.unwrap();\n            assert_eq!(result, Some(value.clone()));\n        }\n\n        assert_eq!(store.len(), 3);\n        assert!(!store.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_scan_operations() {\n        let store = create_test_rskv().await;\n\n        let entries = vec![\n            (b\"prefix_key1\".to_vec(), b\"value1\".to_vec()),\n            (b\"prefix_key2\".to_vec(), b\"value2\".to_vec()),\n            (b\"other_key\".to_vec(), b\"value3\".to_vec()),\n        ];\n\n        // Insert all entries\n        for (key, value) in &entries {\n            store.upsert(key.clone(), value.clone()).await.unwrap();\n        }\n\n        // Test scan_all\n        let all_results = store.scan_all().await.unwrap();\n        assert_eq!(all_results.len(), 3);\n\n        // Test scan_prefix\n        let prefix_results = store.scan_prefix(b\"prefix_\").await.unwrap();\n        assert_eq!(prefix_results.len(), 2);\n\n        // Verify prefix results contain the right keys\n        for (key, _) in &prefix_results {\n            assert!(key.starts_with(b\"prefix_\"));\n        }\n    }\n\n    #[tokio::test]\n    async fn test_stats() {\n        let store = create_test_rskv().await;\n\n        let initial_stats = store.stats();\n        assert_eq!(initial_stats.index_entries, 0);\n\n        // Insert some data\n        store\n            .upsert(b\"key1\".to_vec(), b\"value1\".to_vec())\n            .await\n            .unwrap();\n        store\n            .upsert(b\"key2\".to_vec(), b\"value2\".to_vec())\n            .await\n            .unwrap();\n\n        let stats = store.stats();\n        assert_eq!(stats.index_entries, 2);\n        assert!(stats.log_tail_address > stats.log_head_address);\n    }\n\n    #[tokio::test]\n    async fn test_checkpoint() {\n        let temp_dir = tempdir().unwrap();\n        let config = Config {\n            storage_dir: temp_dir.path().to_string_lossy().to_string(),\n            memory_size: 64 * 1024 * 1024, // 64MB\n            enable_checkpointing: true,    // Enable for this test\n            enable_gc: false,              // Disable to avoid conflicts\n            ..Default::default()\n        };\n\n        let store = RsKv::new(config).await.unwrap();\n\n        // Stop background tasks to avoid conflicts\n        store.stop_background_tasks().await.unwrap();\n\n        // Insert some data\n        store\n            .upsert(b\"key1\".to_vec(), b\"value1\".to_vec())\n            .await\n            .unwrap();\n\n        // Perform checkpoint\n        match store.checkpoint().await {\n            Ok(_) => {\n                // Verify data is still accessible\n                let result = store.read(&b\"key1\".to_vec()).await.unwrap();\n                assert_eq!(result, Some(b\"value1\".to_vec()));\n            }\n            Err(e) => {\n                // For now, just log the error but don't fail the test\n                eprintln!(\"Checkpoint failed (expected in test setup): {}\", e);\n            }\n        }\n\n        // Clean shutdown\n        store.close().await.unwrap();\n    }\n}\n","traces":[{"line":52,"address":[],"length":0,"stats":{"Line":12}},{"line":54,"address":[],"length":0,"stats":{"Line":12}},{"line":56,"address":[],"length":0,"stats":{"Line":6}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":6}},{"line":69,"address":[],"length":0,"stats":{"Line":6}},{"line":72,"address":[],"length":0,"stats":{"Line":6}},{"line":84,"address":[],"length":0,"stats":{"Line":6}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":5}},{"line":123,"address":[],"length":0,"stats":{"Line":1}},{"line":124,"address":[],"length":0,"stats":{"Line":1}},{"line":127,"address":[],"length":0,"stats":{"Line":6}},{"line":134,"address":[],"length":0,"stats":{"Line":24}},{"line":136,"address":[],"length":0,"stats":{"Line":48}},{"line":139,"address":[],"length":0,"stats":{"Line":72}},{"line":142,"address":[],"length":0,"stats":{"Line":36}},{"line":154,"address":[],"length":0,"stats":{"Line":20}},{"line":156,"address":[],"length":0,"stats":{"Line":30}},{"line":157,"address":[],"length":0,"stats":{"Line":20}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":30}},{"line":166,"address":[],"length":0,"stats":{"Line":2}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":2}},{"line":184,"address":[],"length":0,"stats":{"Line":4}},{"line":187,"address":[],"length":0,"stats":{"Line":5}},{"line":190,"address":[],"length":0,"stats":{"Line":3}},{"line":199,"address":[],"length":0,"stats":{"Line":4}},{"line":200,"address":[],"length":0,"stats":{"Line":6}},{"line":201,"address":[],"length":0,"stats":{"Line":1}},{"line":202,"address":[],"length":0,"stats":{"Line":1}},{"line":208,"address":[],"length":0,"stats":{"Line":1}},{"line":209,"address":[],"length":0,"stats":{"Line":1}},{"line":214,"address":[],"length":0,"stats":{"Line":1}},{"line":215,"address":[],"length":0,"stats":{"Line":1}},{"line":219,"address":[],"length":0,"stats":{"Line":2}},{"line":220,"address":[],"length":0,"stats":{"Line":4}},{"line":221,"address":[],"length":0,"stats":{"Line":4}},{"line":222,"address":[],"length":0,"stats":{"Line":4}},{"line":223,"address":[],"length":0,"stats":{"Line":4}},{"line":224,"address":[],"length":0,"stats":{"Line":4}},{"line":232,"address":[],"length":0,"stats":{"Line":8}},{"line":233,"address":[],"length":0,"stats":{"Line":8}},{"line":234,"address":[],"length":0,"stats":{"Line":4}},{"line":240,"address":[],"length":0,"stats":{"Line":2}},{"line":241,"address":[],"length":0,"stats":{"Line":2}},{"line":243,"address":[],"length":0,"stats":{"Line":1}},{"line":246,"address":[],"length":0,"stats":{"Line":2}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":2}},{"line":264,"address":[],"length":0,"stats":{"Line":2}},{"line":265,"address":[],"length":0,"stats":{"Line":1}},{"line":266,"address":[],"length":0,"stats":{"Line":1}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":2}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":2}},{"line":309,"address":[],"length":0,"stats":{"Line":2}},{"line":312,"address":[],"length":0,"stats":{"Line":5}},{"line":313,"address":[],"length":0,"stats":{"Line":9}},{"line":315,"address":[],"length":0,"stats":{"Line":3}},{"line":316,"address":[],"length":0,"stats":{"Line":3}},{"line":321,"address":[],"length":0,"stats":{"Line":1}},{"line":325,"address":[],"length":0,"stats":{"Line":2}},{"line":326,"address":[],"length":0,"stats":{"Line":2}},{"line":328,"address":[],"length":0,"stats":{"Line":5}},{"line":329,"address":[],"length":0,"stats":{"Line":6}},{"line":330,"address":[],"length":0,"stats":{"Line":6}},{"line":331,"address":[],"length":0,"stats":{"Line":2}},{"line":333,"address":[],"length":0,"stats":{"Line":2}},{"line":337,"address":[],"length":0,"stats":{"Line":1}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":2}},{"line":347,"address":[],"length":0,"stats":{"Line":1}},{"line":351,"address":[],"length":0,"stats":{"Line":0}},{"line":352,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":2}},{"line":357,"address":[],"length":0,"stats":{"Line":1}},{"line":360,"address":[],"length":0,"stats":{"Line":1}},{"line":363,"address":[],"length":0,"stats":{"Line":1}},{"line":367,"address":[],"length":0,"stats":{"Line":2}},{"line":369,"address":[],"length":0,"stats":{"Line":1}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":378,"address":[],"length":0,"stats":{"Line":1}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":383,"address":[],"length":0,"stats":{"Line":1}},{"line":385,"address":[],"length":0,"stats":{"Line":1}}],"covered":77,"coverable":107}],"coverage":59.6252129471891,"covered":700,"coverable":1174}