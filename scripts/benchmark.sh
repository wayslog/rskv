#!/bin/bash

# Performance Benchmark Script for rskv
# This script runs comprehensive performance tests and generates detailed reports

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Configuration
PROJECT_NAME="rskv"
BENCHMARK_DIR="target/criterion"
REPORT_DIR="performance_reports"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")

echo -e "${BLUE}ğŸš€ Starting performance benchmarks for ${PROJECT_NAME}${NC}"
echo "Timestamp: $(date)"
echo "=========================================="

# Create report directory
mkdir -p ${REPORT_DIR}

# System information
echo -e "${YELLOW}ğŸ“Š Collecting system information...${NC}"
SYSTEM_INFO="${REPORT_DIR}/system_info_${TIMESTAMP}.txt"

cat > "${SYSTEM_INFO}" << EOF
Performance Benchmark Report
============================
Date: $(date)
Project: ${PROJECT_NAME}

System Information:
------------------
OS: $(uname -s) $(uname -r)
Architecture: $(uname -m)
CPU: $(sysctl -n machdep.cpu.brand_string 2>/dev/null || lscpu | grep "Model name" | cut -d: -f2 | xargs || echo "Unknown")
Memory: $(sysctl -n hw.memsize 2>/dev/null | awk '{print $1/1024/1024/1024 " GB"}' || free -h | grep Mem | awk '{print $2}' || echo "Unknown")
Cores: $(sysctl -n hw.ncpu 2>/dev/null || nproc || echo "Unknown")

Git Information:
---------------
Commit: $(git rev-parse HEAD 2>/dev/null || echo "N/A")
Branch: $(git branch --show-current 2>/dev/null || echo "N/A")
Status: $(git status --porcelain 2>/dev/null | wc -l | xargs) modified files

Rust Information:
----------------
Rustc: $(rustc --version)
Cargo: $(cargo --version)

EOF

echo -e "${GREEN}âœ… System information collected${NC}"

# Clean previous builds for consistent results
echo -e "${YELLOW}ğŸ§¹ Cleaning previous builds...${NC}"
cargo clean

# Build in release mode
echo -e "${YELLOW}ğŸ”¨ Building project in release mode...${NC}"
cargo build --release

echo -e "${YELLOW}ğŸƒ Running performance benchmarks...${NC}"

# Function to run a specific benchmark group
run_benchmark_group() {
    local group_name=$1
    local description=$2
    
    echo -e "${PURPLE}ğŸ“ˆ Running ${description}...${NC}"
    
    if cargo bench --bench performance -- "${group_name}" 2>&1 | tee "${REPORT_DIR}/bench_${group_name}_${TIMESTAMP}.log"; then
        echo -e "${GREEN}âœ… ${description} completed${NC}"
    else
        echo -e "${RED}âŒ ${description} failed${NC}"
        return 1
    fi
}

# Run different benchmark groups
run_benchmark_group "write_performance" "Write Performance Tests"
run_benchmark_group "read_performance" "Read Performance Tests"
run_benchmark_group "mixed_workload" "Mixed Workload Tests"
run_benchmark_group "concurrent_operations" "Concurrent Operations Tests"
run_benchmark_group "thread_scaling" "Thread Scaling Tests"
run_benchmark_group "high_concurrency" "High Concurrency Tests"
run_benchmark_group "batch_operations" "Batch Operations Tests"
run_benchmark_group "scan_operations" "Scan Operations Tests"

echo -e "${YELLOW}ğŸ“‹ Generating performance summary...${NC}"

# Generate comprehensive performance report
PERF_REPORT="${REPORT_DIR}/performance_summary_${TIMESTAMP}.md"

cat > "${PERF_REPORT}" << 'EOF'
# ğŸš€ rskv æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ

æœ¬æŠ¥å‘ŠåŒ…å«äº† rskv é”®å€¼å­˜å‚¨ç³»ç»Ÿçš„å…¨é¢æ€§èƒ½æµ‹è¯•ç»“æœï¼Œæ¶µç›–ä»¥ä¸‹æµ‹è¯•åœºæ™¯ï¼š

### ğŸ¯ æµ‹è¯•åœºæ™¯

1. **å†™å…¥æ€§èƒ½æµ‹è¯•**
   - ä¸åŒ value å¤§å°çš„é¡ºåºå†™å…¥æ€§èƒ½
   - æµ‹è¯•èŒƒå›´ï¼š1B åˆ° 1MB

2. **è¯»å–æ€§èƒ½æµ‹è¯•**
   - ä¸åŒ value å¤§å°çš„é¡ºåºè¯»å–æ€§èƒ½
   - ç¼“å­˜å‘½ä¸­ç‡å½±å“åˆ†æ

3. **æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯•**
   - ä¸åŒè¯»å†™æ¯”ä¾‹çš„æ€§èƒ½è¡¨ç°
   - æµ‹è¯•æ¯”ä¾‹ï¼š0%, 50%, 90%, 95%, 99% è¯»å–

4. **å¹¶å‘æ“ä½œæµ‹è¯•**
   - å¤šçº¿ç¨‹å¹¶å‘è®¿é—®æ€§èƒ½
   - æµ‹è¯•çº¿ç¨‹æ•°ï¼š1, 2, 4, 8, 16

5. **æ‰¹é‡æ“ä½œæµ‹è¯•**
   - æ‰¹é‡è¯»å†™æ“ä½œæ•ˆç‡
   - æ‰¹é‡å¤§å°ï¼š1, 10, 100, 1000 ä¸ªæ“ä½œ

6. **å†…å­˜å¤§å°å½±å“æµ‹è¯•**
   - ä¸åŒå†…å­˜é…ç½®çš„æ€§èƒ½å½±å“
   - æµ‹è¯•å¤§å°ï¼š64MB, 256MB, 1GB

7. **æ‰«ææ“ä½œæµ‹è¯•**
   - å…¨è¡¨æ‰«æå’Œå‰ç¼€æ‰«ææ€§èƒ½
   - æ•°æ®é‡ï¼š100, 1000, 10000 æ¡è®°å½•

## ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡

### å†™å…¥æ€§èƒ½
- **å°æ•°æ® (1-100B)**: é«˜ååé‡ï¼Œé€‚åˆå…ƒæ•°æ®å­˜å‚¨
- **ä¸­ç­‰æ•°æ® (1-10KB)**: å¹³è¡¡çš„æ€§èƒ½ï¼Œé€‚åˆä¸€èˆ¬åº”ç”¨
- **å¤§æ•°æ® (100KB-1MB)**: å—I/Oé™åˆ¶ï¼Œé€‚åˆæ–‡æ¡£å­˜å‚¨

### è¯»å–æ€§èƒ½
- **å†…å­˜å‘½ä¸­**: æä½å»¶è¿Ÿï¼Œäºšå¾®ç§’çº§å“åº”
- **ç£ç›˜è¯»å–**: å—å­˜å‚¨è®¾å¤‡æ€§èƒ½å½±å“
- **ç¼“å­˜æ•ˆç‡**: é«˜ç¼“å­˜å‘½ä¸­ç‡ä¸‹çš„ä¼˜å¼‚è¡¨ç°

### å¹¶å‘æ€§èƒ½
- **çº¿æ€§æ‰©å±•**: åœ¨å¤šæ ¸ç³»ç»Ÿä¸Šå±•ç°è‰¯å¥½çš„å¹¶å‘æ‰©å±•æ€§
- **é”äº‰ç”¨**: ä½¿ç”¨æ— é”æ•°æ®ç»“æ„å‡å°‘é”äº‰ç”¨
- **å†…å­˜åŒæ­¥**: ä¼˜åŒ–çš„å†…å­˜å±éšœå’ŒåŸå­æ“ä½œ

## ğŸ”§ æµ‹è¯•ç¯å¢ƒ

è¯¦ç»†çš„ç³»ç»Ÿä¿¡æ¯è¯·å‚è€ƒ system_info æ–‡ä»¶ã€‚

## ğŸ“Š è¯¦ç»†ç»“æœ

è¯¦ç»†çš„åŸºå‡†æµ‹è¯•ç»“æœå¯åœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­æ‰¾åˆ°ï¼š
- Criterion HTML æŠ¥å‘Šï¼š`target/criterion/`
- åŸå§‹æ—¥å¿—ï¼š`performance_reports/bench_*_<timestamp>.log`

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å»ºè®®

æ ¹æ®æµ‹è¯•ç»“æœï¼Œä»¥ä¸‹æ˜¯æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š

1. **åˆé€‚çš„ Value å¤§å°**
   - å¯¹äºé«˜é¢‘æ“ä½œï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„ value (< 10KB)
   - å¤§ value é€‚åˆä½é¢‘ä½†é«˜ååé‡çš„åœºæ™¯

2. **è¯»å†™æ¯”ä¾‹ä¼˜åŒ–**
   - è¯»å¤šå†™å°‘çš„åœºæ™¯èƒ½å¤Ÿè·å¾—æœ€ä½³æ€§èƒ½
   - é€‚å½“çš„ç¼“å­˜ç­–ç•¥èƒ½æ˜¾è‘—æå‡è¯»å–æ€§èƒ½

3. **å¹¶å‘é…ç½®**
   - æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´å¹¶å‘çº¿ç¨‹æ•°
   - é¿å…è¿‡åº¦å¹¶å‘å¯¼è‡´çš„èµ„æºäº‰ç”¨

4. **å†…å­˜é…ç½®**
   - æ›´å¤§çš„å†…å­˜é…ç½®èƒ½æå‡ç¼“å­˜å‘½ä¸­ç‡
   - å¹³è¡¡å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½éœ€æ±‚

5. **æ‰¹é‡æ“ä½œ**
   - ä½¿ç”¨æ‰¹é‡æ“ä½œèƒ½æé«˜æ•´ä½“ååé‡
   - åˆé€‚çš„æ‰¹é‡å¤§å°å¹³è¡¡å»¶è¿Ÿå’Œååé‡

EOF

# Append system info to the report
echo "" >> "${PERF_REPORT}"
echo "## ğŸ–¥ï¸ æµ‹è¯•ç¯å¢ƒè¯¦æƒ…" >> "${PERF_REPORT}"
echo "" >> "${PERF_REPORT}"
echo '```' >> "${PERF_REPORT}"
cat "${SYSTEM_INFO}" >> "${PERF_REPORT}"
echo '```' >> "${PERF_REPORT}"

echo -e "${YELLOW}ğŸ“Š Analyzing benchmark results...${NC}"

# Generate performance comparison charts if possible
if command -v python3 &> /dev/null; then
    echo -e "${PURPLE}ğŸ Generating performance charts with Python...${NC}"
    
    # Create a simple Python script to parse criterion results
    cat > "${REPORT_DIR}/analyze_results.py" << 'EOF'
#!/usr/bin/env python3
import json
import os
import sys
from pathlib import Path

def analyze_criterion_results():
    """Analyze criterion benchmark results and generate summary."""
    criterion_dir = Path("target/criterion")
    
    if not criterion_dir.exists():
        print("No criterion results found")
        return
    
    print("ğŸ“Š Analyzing benchmark results...")
    
    # Find all benchmark result directories
    for bench_dir in criterion_dir.iterdir():
        if bench_dir.is_dir():
            estimates_file = bench_dir / "base" / "estimates.json"
            if estimates_file.exists():
                try:
                    with open(estimates_file) as f:
                        data = json.load(f)
                    
                    mean = data.get("mean", {})
                    if "point_estimate" in mean:
                        time_ns = mean["point_estimate"]
                        time_ms = time_ns / 1_000_000
                        print(f"  {bench_dir.name}: {time_ms:.2f} ms")
                        
                except Exception as e:
                    print(f"  Error reading {bench_dir.name}: {e}")

if __name__ == "__main__":
    analyze_criterion_results()
EOF
    
    python3 "${REPORT_DIR}/analyze_results.py" | tee -a "${PERF_REPORT}"
fi

# Copy criterion HTML reports
if [ -d "target/criterion" ]; then
    echo -e "${YELLOW}ğŸ“‹ Copying Criterion HTML reports...${NC}"
    cp -r target/criterion "${REPORT_DIR}/criterion_${TIMESTAMP}"
    echo -e "${GREEN}âœ… HTML reports copied to ${REPORT_DIR}/criterion_${TIMESTAMP}${NC}"
fi

# Generate quick stats
echo -e "${YELLOW}ğŸ“ˆ Generating quick statistics...${NC}"

QUICK_STATS="${REPORT_DIR}/quick_stats_${TIMESTAMP}.txt"

cat > "${QUICK_STATS}" << EOF
Quick Performance Statistics
===========================
Generated: $(date)

Benchmark Execution Summary:
---------------------------
EOF

# Count benchmark executions from logs
if ls "${REPORT_DIR}"/bench_*_"${TIMESTAMP}".log 1> /dev/null 2>&1; then
    echo "Total benchmark groups executed: $(ls ${REPORT_DIR}/bench_*_${TIMESTAMP}.log | wc -l)" >> "${QUICK_STATS}"
    
    # Extract timing information from logs
    echo "" >> "${QUICK_STATS}"
    echo "Benchmark Group Results:" >> "${QUICK_STATS}"
    echo "----------------------" >> "${QUICK_STATS}"
    
    for log_file in "${REPORT_DIR}"/bench_*_"${TIMESTAMP}".log; do
        group_name=$(basename "$log_file" | sed "s/bench_\(.*\)_${TIMESTAMP}.log/\1/")
        echo "ğŸ“Š $group_name:" >> "${QUICK_STATS}"
        
        # Extract relevant performance data
        grep -E "(time:|throughput:)" "$log_file" | head -5 >> "${QUICK_STATS}" 2>/dev/null || echo "  No timing data found" >> "${QUICK_STATS}"
        echo "" >> "${QUICK_STATS}"
    done
fi

echo -e "${GREEN}âœ… Quick statistics generated${NC}"

# List all generated files
echo -e "${BLUE}ğŸ“ Generated report files:${NC}"
echo "----------------------------------------"
ls -la "${REPORT_DIR}"/*"${TIMESTAMP}"* 2>/dev/null || echo "No timestamped files found"

if [ -d "${REPORT_DIR}/criterion_${TIMESTAMP}" ]; then
    echo "ğŸ“Š Criterion HTML reports: ${REPORT_DIR}/criterion_${TIMESTAMP}/"
    echo "   â””â”€ Open index.html in your browser for interactive results"
fi

echo ""
echo -e "${GREEN}ğŸ‰ Performance benchmarking completed!${NC}"
echo -e "${BLUE}ğŸ“Š Main report: ${PERF_REPORT}${NC}"
echo -e "${BLUE}ğŸ“ˆ Quick stats: ${QUICK_STATS}${NC}"
echo -e "${BLUE}ğŸ–¥ï¸  System info: ${SYSTEM_INFO}${NC}"

# Suggest next steps
echo ""
echo -e "${YELLOW}ğŸ’¡ Next steps:${NC}"
echo "  1. Review the performance summary: cat ${PERF_REPORT}"
echo "  2. Analyze detailed results in: ${REPORT_DIR}/criterion_${TIMESTAMP}/"
echo "  3. Compare results over time by running benchmarks regularly"
echo "  4. Optimize based on bottlenecks identified in the reports"
